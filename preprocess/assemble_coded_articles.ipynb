{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble hand-coded articles and prepare for modeling\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: November 2020<br>\n",
    "\n",
    "@description: '''Loads and merges two datasets in preparation for classification model training. We're dealing with three theoretical perspectives in org. science (cultural, demographic, and relational) and two subject areas (sociology & management/OB, not differentiated here). The first dataset is of articles hand-coded by the author and Prof. Haveman, and it comes as a clean .csv file. This first contains lots of false positives (from the previous approach based on cosine measures), so it consists of mainly negative cases. The second dataset is of articles identified by Prof. Haveman as being foundation/definitive for each perspective. This comes as a list of citations, one per perspective, and requires some pretty heavy cleaning to match with articles in the main JSTOR articles dataset.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import imp, importlib # For working with modules\n",
    "import pandas as pd # for working with dataframes\n",
    "import numpy as np # for working with numbers\n",
    "import pickle # For working with .pkl files\n",
    "from tqdm import tqdm # Shows progress over iterations, including in pandas via \"progress_apply\"\n",
    "tqdm.pandas(desc='')\n",
    "import sys # For terminal tricks\n",
    "import _pickle as cPickle # Optimized version of pickle\n",
    "import gc # For managing garbage collector\n",
    "import timeit # For counting time taken for a process\n",
    "import datetime # For working with dates & times\n",
    "import tables\n",
    "import random\n",
    "import os; from os import listdir; from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filepaths\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/preprocess', '')\n",
    "#root = '/home/jovyan/work/' # set root directory\n",
    "\n",
    "# dictionary counts (using core dictionaries) and matched subjects \n",
    "counts_fp = root + 'dictionary_methods/counts_and_subject.csv'\n",
    "\n",
    "# per-article info on cosine scores using each dictionary (core or 100-term dictionaries??)\n",
    "cosines_fp = root + 'models_storage/word_embeddings_data/text_with_cosine_scores_wdg_2020_oct27.csv'\n",
    "\n",
    "# per-article metadata with URLs\n",
    "meta_fp = root + 'dictionary_methods/code/metadata_combined.h5' \n",
    "\n",
    "# Filtered index of research articles\n",
    "articles_list_fp = root + 'dictionary_methods/code/filtered_index.csv'\n",
    "\n",
    "# coded output directory: save files here\n",
    "output_fp = root + 'classification/data/hand_coded/'\n",
    "coded_11620 = output_fp + 'coded_sample_cleaned_111620.csv'\n",
    "coded_cult = output_fp + 'true_positives_cultural.csv'\n",
    "coded_relt = output_fp + 'true_positives_relational.csv'\n",
    "coded_demog = output_fp + 'true_positives_demographic.csv'\n",
    "\n",
    "# for text files\n",
    "ocr_fp = root + 'jstor_data/ocr/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69659/69659 [00:00<00:00, 1436647.60it/s]\n",
      "100%|██████████| 399129/399129 [00:02<00:00, 170673.53it/s]\n",
      " 56%|█████▋    | 225355/399128 [05:54<04:26, 650.93it/s]"
     ]
    }
   ],
   "source": [
    "# collect article file list\n",
    "colnames = ['file_name']\n",
    "articles = pd.read_csv(articles_list_fp, names=colnames, header=None)\n",
    "\n",
    "files_to_be_opened = [ocr_fp + file + '.txt' for file in tqdm(articles.file_name)]\n",
    "all_files = [ocr_fp + f for f in tqdm(listdir(ocr_fp)) if isfile(join(ocr_fp, f))]\n",
    "\n",
    "files = [file for file in tqdm(all_files) if file in files_to_be_opened]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

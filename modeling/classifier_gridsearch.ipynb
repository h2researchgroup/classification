{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using grid search and balanced samples from hand-labeled set of articles\n",
    "\n",
    "@author: Thomas Lu, Jaren Haber PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: September 2021\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.6.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2021.8.28)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, KFold\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "data_folder = 'classification'\n",
    "folder = 'tlu_test'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, f'{folder}/modeling', '')\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + f'{data_folder}/data/'\n",
    "model_fp = root + f'{data_folder}/models/'\n",
    "logs = root + f'{folder}/modeling/logs/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_022621.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_022621.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_022621.pkl'\n",
    "orgs_labeled_fp = data_fp + 'training_orgs_preprocessed_022621.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_MLP_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_MLP_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_MLP_{str(thisday)}.joblib'\n",
    "orgs_model_fp = model_fp + f'classifier_orgs_MLP_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + 'vectorizer_cult_022621.joblib'\n",
    "relt_vec_fp = model_fp + 'vectorizer_relt_022621.joblib'\n",
    "demog_vec_fp = model_fp + 'vectorizer_demog_022621.joblib'\n",
    "orgs_vec_fp = model_fp + 'vectorizer_orgs_022621.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cultural_score</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>article_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[research, note, church_membership, netherlan...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_210179</td>\n",
       "      <td>Where Do Interorganizational Networks Come From?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[polish, io_oo, sociological_review, issn, co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_210317</td>\n",
       "      <td>Civil Rights Law at Work: Sex Discrimination a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[article, jjdlbsj, grapliy, compassionate, eg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_231084</td>\n",
       "      <td>Between Markets and Politics: Organizational R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[reply, allison, more, comparing, regression_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_231174</td>\n",
       "      <td>World Society and the Nation‐State</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[determinants, spousal, interaction, marital,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_382347</td>\n",
       "      <td>Kinship Networks and Entrepreneurs in China’s ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[wsê, ih, ompany, profile, john, porter, musé...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_517899</td>\n",
       "      <td>What Is Organizational Imprinting? Cultural En...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[andrew_christensen, university_california, l...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_588742</td>\n",
       "      <td>Homeward Bound? Interest, Identity, and Invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[lawyers, consumer_protection, laws, stewart_...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_657524</td>\n",
       "      <td>Corporate Unity in American Trade Policy: A Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[establishing, sense, personal, control, tran...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_659639</td>\n",
       "      <td>The Credit Crisis as a Problem in the Sociolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[guess, who, coming, town, white_supremacy, e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1525_irqr.2011.4.3.199</td>\n",
       "      <td>Science, Health, and Nationhood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cultural_score  \\\n",
       "0  [[research, note, church_membership, netherlan...             0.0   \n",
       "1  [[polish, io_oo, sociological_review, issn, co...             1.0   \n",
       "2  [[article, jjdlbsj, grapliy, compassionate, eg...             0.0   \n",
       "3  [[reply, allison, more, comparing, regression_...             1.0   \n",
       "4  [[determinants, spousal, interaction, marital,...             1.0   \n",
       "5  [[wsê, ih, ompany, profile, john, porter, musé...             1.0   \n",
       "6  [[andrew_christensen, university_california, l...             1.0   \n",
       "7  [[lawyers, consumer_protection, laws, stewart_...             0.0   \n",
       "8  [[establishing, sense, personal, control, tran...             1.0   \n",
       "9  [[guess, who, coming, town, white_supremacy, e...             0.0   \n",
       "\n",
       "  primary_subject            edited_filename  \\\n",
       "0       Sociology             10.1086_210179   \n",
       "1       Sociology             10.1086_210317   \n",
       "2       Sociology             10.1086_231084   \n",
       "3       Sociology             10.1086_231174   \n",
       "4       Sociology             10.1086_382347   \n",
       "5       Sociology             10.1086_517899   \n",
       "6       Sociology             10.1086_588742   \n",
       "7       Sociology             10.1086_657524   \n",
       "8       Sociology             10.1086_659639   \n",
       "9       Sociology  10.1525_irqr.2011.4.3.199   \n",
       "\n",
       "                                        article_name  \n",
       "0   Where Do Interorganizational Networks Come From?  \n",
       "1  Civil Rights Law at Work: Sex Discrimination a...  \n",
       "2  Between Markets and Politics: Organizational R...  \n",
       "3                 World Society and the Nation‐State  \n",
       "4  Kinship Networks and Entrepreneurs in China’s ...  \n",
       "5  What Is Organizational Imprinting? Cultural En...  \n",
       "6  Homeward Bound? Interest, Identity, and Invest...  \n",
       "7  Corporate Unity in American Trade Policy: A Ne...  \n",
       "8  The Credit Crisis as a Problem in the Sociolog...  \n",
       "9                    Science, Health, and Nationhood  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "orgs_df = quickpickle_load(orgs_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    475\n",
      "0.5     24\n",
      "1.0    234\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    420\n",
      "0.5     29\n",
      "1.0    287\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    477\n",
      "0.5      7\n",
      "1.0    256\n",
      "dtype: int64\n",
      "\n",
      "orgs_score\n",
      "0.0    303\n",
      "0.5     10\n",
      "1.0    511\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())\n",
    "print()\n",
    "print(orgs_df.groupby('orgs_score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsure cases: where X_score = 0.5\n",
    "drop_unsure = True\n",
    "\n",
    "if drop_unsure:\n",
    "    cult_df_yes = cult_df[cult_df['cultural_score'] == 1.0]\n",
    "    cult_df_no = cult_df[cult_df['cultural_score'] == 0.0]\n",
    "    cult_df = pd.concat([cult_df_yes, cult_df_no])\n",
    "    \n",
    "    relt_df_yes = relt_df[relt_df['relational_score'] == 1.0]\n",
    "    relt_df_no = relt_df[relt_df['relational_score'] == 0.0]\n",
    "    relt_df = pd.concat([relt_df_yes, relt_df_no])\n",
    "    \n",
    "    demog_df_yes = demog_df[demog_df['demographic_score'] == 1.0]\n",
    "    demog_df_no = demog_df[demog_df['demographic_score'] == 0.0]\n",
    "    demog_df = pd.concat([demog_df_yes, demog_df_no])\n",
    "    \n",
    "    orgs_df_yes = orgs_df[orgs_df['orgs_score'] == 1.0]\n",
    "    orgs_df_no = orgs_df[orgs_df['orgs_score'] == 0.0]\n",
    "    orgs_df = pd.concat([orgs_df_yes, orgs_df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_article_tokens(article, return_string=False):\n",
    "    '''\n",
    "    Collects words from already-tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "        return_string: whether to return single, long string representing article\n",
    "    Returns:\n",
    "        tokens: string if return_string, else list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = [] # initialize\n",
    "    \n",
    "    if return_string:\n",
    "        for sent in article:\n",
    "            sent = ' '.join(sent) # make sentence into a string\n",
    "            tokens.append(sent) # add sentence to list of sentences\n",
    "        tokens = ' '.join(tokens) # join sentences into string\n",
    "        return tokens # return string\n",
    "    \n",
    "    else:\n",
    "        for sent in article:\n",
    "            tokens += [word for word in sent] # add each word to list of tokens\n",
    "        return tokens # return list of tokens\n",
    "\n",
    "# For capturing word frequencies, add all words from each article to single, shared list (can't use this to create models)\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "orgs_tokens = []; orgs_df['text'].apply(lambda article: orgs_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 169701\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('from', 89011),\n",
       " ('social', 77638),\n",
       " ('have', 73580),\n",
       " ('we', 70788),\n",
       " ('more', 68737),\n",
       " ('which', 66698),\n",
       " ('were', 65677),\n",
       " ('one', 55317),\n",
       " ('other', 44993),\n",
       " ('than', 43319),\n",
       " ('may', 43175),\n",
       " ('also', 41037),\n",
       " ('all', 40362),\n",
       " ('can', 39250),\n",
       " ('research', 38752),\n",
       " ('between', 38287),\n",
       " ('who', 37469),\n",
       " ('time', 35868),\n",
       " ('has', 33474),\n",
       " ('when', 33441)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = ((cult_tokens + relt_tokens) + demog_tokens) + orgs_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 113692\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('entry', 12932),\n",
       " ('bottom_entry', 8227),\n",
       " ('align', 8045),\n",
       " ('row', 4734),\n",
       " ('align_center', 2573),\n",
       " ('bottom', 2279),\n",
       " ('align_left', 2004),\n",
       " ('colspec_colnum', 774),\n",
       " ('top entry', 542),\n",
       " ('caption', 430),\n",
       " ('label_label', 424),\n",
       " ('tex_math notation_latex', 404),\n",
       " ('documentclass_aastex', 404),\n",
       " ('usepackage_amsbsy', 404),\n",
       " ('usepackage_amsfonts', 404),\n",
       " ('usepackage_amssymb', 404),\n",
       " ('usepackage_bm', 404),\n",
       " ('usepackage_mathrsfs', 404),\n",
       " ('usepackage_pifont', 404),\n",
       " ('usepackage_stmaryrd', 404),\n",
       " ('usepackage_textcomp', 404),\n",
       " ('usepackage_portland xspace', 404),\n",
       " ('usepackage_amsmath amsxtra', 404),\n",
       " ('usepackage_ot ot_fontenc', 404),\n",
       " ('newcommand_cyr', 404),\n",
       " ('renewcommand_rmdefault wncyr', 404),\n",
       " ('renewcommand_sfdefault wncyss', 404),\n",
       " ('renewcommand_encodingdefault ot', 404),\n",
       " ('normalfont', 404),\n",
       " ('selectfont', 404),\n",
       " ('declaretextfontcommand_extcyr cyr', 404),\n",
       " ('pagestyle_empty', 404),\n",
       " ('declaremathsizes', 404),\n",
       " ('begin_document', 404),\n",
       " ('landscape', 404),\n",
       " ('end_document tex_math', 393),\n",
       " ('entry_align entry', 378),\n",
       " ('position_float', 332),\n",
       " ('alt_version', 332),\n",
       " ('mimetype_image', 332),\n",
       " ('xlink:type_simple', 332),\n",
       " ('align right', 287),\n",
       " ('top', 279),\n",
       " ('italic_italic', 259),\n",
       " ('group', 258),\n",
       " ('table_wrap foot', 242),\n",
       " ('top_break entry', 218),\n",
       " ('sub', 213),\n",
       " ('inline_formula', 172),\n",
       " ('entry_align left_top', 171),\n",
       " ('entry_namest', 158),\n",
       " ('nameend', 158),\n",
       " ('table_xmlns:oasis dtd_xml exchange table model en', 137),\n",
       " ('xmlns:oasis_http docs.oasis_open.org ns_exchange table', 137),\n",
       " ('frame_topbot', 137),\n",
       " ('pgwide', 137),\n",
       " ('orient_port', 137),\n",
       " ('tbody', 137),\n",
       " ('tgroup', 137),\n",
       " ('table', 137),\n",
       " ('table_wrap', 137),\n",
       " ('tbody_top', 134),\n",
       " ('label table label', 133),\n",
       " ('tgroup_cols align_left', 133),\n",
       " ('graphic_xlink:href tb.eps', 98),\n",
       " ('list_item', 98),\n",
       " ('disp_formula df', 94),\n",
       " ('disp_formula', 94),\n",
       " ('entry_align left_top entry', 86),\n",
       " ('fig position_float fig_type figure', 82),\n",
       " ('rotation', 82),\n",
       " ('fig', 82),\n",
       " ('fig group', 82),\n",
       " ('graphic_xlink:href df.eps', 80),\n",
       " ('graphic_xlink:href fg.tiff', 64),\n",
       " ('italic_italic entry', 60),\n",
       " ('hat', 56),\n",
       " ('sup', 52),\n",
       " ('label_fig xml:space_preserve', 48),\n",
       " ('label', 48),\n",
       " ('tbfna', 46),\n",
       " ('bottom yes entry', 44),\n",
       " ('bottom model entry', 44),\n",
       " ('title', 41),\n",
       " ('bottom sd entry', 40),\n",
       " ('entry_align left_top χ_sup sup italic df italic_italic italic lt entry',\n",
       "  40),\n",
       " ('morerows', 39),\n",
       " ('graphic_xlink:href tb_eps', 39),\n",
       " ('bottom mean entry', 37),\n",
       " ('bottom mse entry', 36),\n",
       " ('bottom total entry', 36),\n",
       " ('italic_italic italic df italic_italic italic two_tailed entry', 36),\n",
       " ('label_fig label', 34),\n",
       " ('mathrm_mathrm', 32),\n",
       " ('bottom configuration sets entry', 28),\n",
       " ('bottom continuation rate entry', 28),\n",
       " ('bottom continuation rate other configurations entry', 28),\n",
       " ('italic white boys drd risk italic', 28),\n",
       " ('bottom χ_sup sup italic df italic_italic italic entry', 28),\n",
       " ('italic_ij italic', 27)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(\n",
    "    lambda article: cult_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(\n",
    "    lambda article: relt_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(\n",
    "    lambda article: demog_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "orgs_sents = []; orgs_df['text'].apply(\n",
    "    lambda article: orgs_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = ((cult_sents + relt_sents) + demog_sents) + orgs_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and apply text vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect articles: Add each article as single str to list of str:\n",
    "cult_docs = [] # empty list\n",
    "cult_df['text'].apply(\n",
    "    lambda article: cult_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "relt_docs = [] # empty list\n",
    "relt_df['text'].apply(\n",
    "    lambda article: relt_docs.append(\n",
    "       collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "demog_docs = [] # empty list\n",
    "demog_df['text'].apply(\n",
    "    lambda article: demog_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "orgs_docs = [] # empty list\n",
    "orgs_df['text'].apply(\n",
    "    lambda article: orgs_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "print() # skip weird output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 100000\n",
      "Every 1000th word:\n",
      "['_author', 'achivement', 'aerotic', 'alam', 'amazon_basin', 'anjlated', 'appropriated', 'attacks', 'barbara_entwisle', 'berlin_berlin', 'boles', 'brotherhood', 'cantonments', 'cfi_comparative', 'ciu', 'collects', 'conceptually', 'convoluted', 'criticised', 'daunting_task', 'demonized', 'diesel_engines', 'distributed', 'dst', 'efforts', 'enhance_interpretability', 'euroqol', 'fabrigar', 'fictional', 'foreignness', 'furstenberg', 'giddens', 'greenfeld', 'hard_currency', 'het_erogeneity', 'hospital_bed', 'illuminate', 'infants', 'internally_consistent', 'jaffee_david', 'kagono_nonaka', 'kockott', 'latham_wexley', 'liebling', 'loseke_eds', 'manager', 'mazur', 'mexican_immigrants', 'modifier', 'murdie', 'neter_john', 'norfolk', 'oil_spills', 'ot_ot', 'paris_harmattan', 'perks', 'plea_negotiation', 'praeger_publishers', 'processors', 'psych_bull', 'qtj', 'ramadan', 'reappeared', 'referrals', 'relmo', 'residen_tial', 'rffirl', 'romantics', 'running', 'sandra_hanson', 'scholl', 'seers', 'seriously_jeopardize', 'shiriki', 'sina', 'smarquez', 'somatoform', 'spiritualists', 'statics', 'stratified_multistage', 'substantively_methodologically', 'surpassing', 'tackling', 'technology_transfer', 'thampil', 'tietze', 'toxoid', 'tripath', 'tyran', 'undermined', 'unobtrusively', 'ustong', 'venkatraman', 'vistasl', 'walmart', 'werren', 'winner_winner', 'ws', 'youthfulness', 'íocd']\n",
      "\n",
      "Number of features in relational vectorizer: 100000\n",
      "Every 1000th word:\n",
      "['_author', 'adner_levinthal', 'allott', 'anthony_bryk', 'asp_doi', 'back_propagation', 'behave_opportunistically', 'black_householders', 'braverman_labor', 'bx', 'catholic_church', 'child_minding', 'co_cm', 'compensated', 'constrained_equal', 'coup', 'cutoff_criteria', 'deering', 'deterministic_chaos', 'discover', 'djebarni', 'drink', 'edward_bruner', 'enduring', 'eu_eu', 'eye_storm', 'fewer_hours', 'forcomptar', 'functionality', 'getting_caught', 'graves_rinnooy', 'hangs', 'heredity', 'hoop', 'ignagni', 'inductive_deductive', 'interest_intermediation', 'ivc_vi', 'jury_selection', 'kmenta_elements', 'larimo', 'liberated', 'looms_large', 'males_females', 'max_min', 'meted_out', 'modal_stratum', 'multiplication', 'neiqhbours', 'nonsignificant', 'ofpower', 'orzco', 'parasubordinato', 'performance_appraisal', 'platitudes', 'prac_tices', 'prin_cipal', 'pronatalist', 'publicly_available', 'questionnaire_pretested', 'raskin_white', 'recognises', 'regularized', 'rent_arrears', 'rests_heavily', 'rigged', 'romania_albania', 'rumelt_barney', 'sanct', 'schmollers_jahrbuch', 'see_appendix', 'serendipity', 'sheraton', 'siltanen', 'slightly_overestimated', 'software_developers', 'specifity', 'stanched', 'stirring', 'stupendo', 'sunbelt_cities', 'sylos_labini', 'tations', 'tergenerational', 'thirty', 'tky', 'tragedies', 'trude', 'uences', 'undp_undp', 'unresponsiveness', 'vacated', 'versational', 'vodca', 'warner_defleur', 'western_flores', 'winship_christopher', 'wtrenton', 'youthful_atheism', 'économique']\n",
      "\n",
      "Number of features in demographic vectorizer: 100000\n",
      "Every 1000th word:\n",
      "['_author', 'ackerman', 'aequitasaudit', 'al_dïn', 'amana', 'anglia', 'appellate_judge', 'aside', 'babson_college', 'bean_counters', 'berger_morris', 'bleaker', 'brahmanic', 'bryant', 'cacioppo_petty', 'carers', 'ce_ce', 'charvat', 'cia_world', 'co_cm', 'comparing', 'constance_shehan', 'counseling', 'currently_completing', 'decorating', 'deskilling', 'disbelieved', 'dominated', 'early_twentieth', 'elwert', 'equity_joint', 'exhausted', 'fatally_flawed', 'fitton', 'fraught', 'gatt', 'goffé', 'guitars', 'he_insists', 'hisp', 'humphreys', 'impure', 'inroads_male', 'inupiaq', 'jensen_meckling', 'kay_lehman', 'kroeber_kluckhohn', 'league_baseball', 'ling', 'lucius', 'mar_kets', 'mclean', 'mid_victorian', 'monist', 'muñoz', 'newly_merged', 'notation_latex', 'ols_regression', 'outfit', 'partially_compensated', 'person', 'plus_minus', 'precarious', 'professionals', 'píí', 'rdc', 'refutes', 'remembrance', 'resource_extraction', 'rical', 'robustness_check', 'rrr_se', 'salesmen', 'sceptic', 'searles', 'senior_executives', 'shaping_reshaping', 'sidewalk', 'skewness_kurtosis', 'sofer', 'srensen_kalleberg', 'stoclk', 'sub_sahara', 'sunshine', 'syncretic', 'tenured_professors', 'tiróle', 'tri_emissions', 'ube', 'understatement', 'unproctored', 'utoledo', 'verbal_abilities', 'vivante', 'wandered', 'were_reinterviewed', 'wilmoth', 'wreckers', 'yond', 'á_']\n",
      "\n",
      "Number of features in organizational soc vectorizer: 100000\n",
      "Every 1000th word:\n",
      "['_author', 'adjudicate', 'alawis', 'ancestry', 'archeological', 'atki', 'backfires', 'bbc', 'bes', 'blog', 'braconi', 'burdening', 'carlotta', 'chankong', 'cj_cj', 'coleman_hoffer', 'con_cern', 'continuamente', 'coverman_sheley', 'cuts', 'deci', 'denrell_march', 'dia_lectic', 'discotheques', 'doctoral_fellowship', 'dry', 'economia', 'elicits', 'enlarged_eu', 'est', 'executive_compensation', 'fadéla', 'femme', 'flacks', 'forunx', 'functional', 'generalizado', 'globaliza_tion', 'greenbaum', 'haignere', 'hconomic', 'hierarchical_dualisms', 'homosexual_acts', 'husic', 'impatience', 'information', 'internationaljournal_sociology', 'iviay', 'judge', 'kimble', 'ladner_joyce', 'leigh', 'littoral', 'lust', 'march_april', 'mckay', 'michael_messner', 'mod_els', 'multi_nationals', 'nearest_filing', 'non_estonians', 'nutshell', 'onr', 'outlines', 'parochial', 'perishable_goods', 'plants', 'potion', 'priority', 'psy_chologists', 'racial_discrimination', 'received_november', 'relays', 'resulf', 'robust', 'sacralized', 'schooler', 'sensualist', 'shortness', 'skarlicki', 'sociologists_psychologists', 'spheres', 'staw_eds', 'strives', 'sultan_johore', 'synergistic', 'technischer', 'theodore_newcomb', 'tipton_steven', 'trans_ferred', 'tumin', 'und_perspektiven', 'unplanned_pregnancies', 'valencia', 'victorian_edwardian', 'wagner_frick', 'wendy_cadge', 'wisp', 'xor', 'zombie']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Uses TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "cult_vectorizer = joblib.load(cult_vec_fp, \"r+\")\n",
    "X_cult = cult_vectorizer.transform(cult_docs)\n",
    "print('Number of features in cultural vectorizer:', len(cult_vectorizer.get_feature_names()))\n",
    "print('Every 1000th word:\\n{}'.format(cult_vectorizer.get_feature_names()[::1000])) # get every 1000th word\n",
    "print()\n",
    "\n",
    "relt_vectorizer = joblib.load(relt_vec_fp, \"r+\")\n",
    "X_relt = relt_vectorizer.transform(relt_docs)\n",
    "print('Number of features in relational vectorizer:', len(relt_vectorizer.get_feature_names()))\n",
    "print('Every 1000th word:\\n{}'.format(relt_vectorizer.get_feature_names()[::1000])) # get every 1000th word\n",
    "print()\n",
    "\n",
    "demog_vectorizer = joblib.load(demog_vec_fp, \"r+\")\n",
    "X_demog = demog_vectorizer.transform(demog_docs)\n",
    "print('Number of features in demographic vectorizer:', len(demog_vectorizer.get_feature_names()))\n",
    "print('Every 1000th word:\\n{}'.format(demog_vectorizer.get_feature_names()[::1000])) # get every 1000th word\n",
    "print()\n",
    "\n",
    "orgs_vectorizer = joblib.load(orgs_vec_fp, \"r+\")\n",
    "X_orgs = orgs_vectorizer.transform(orgs_docs)\n",
    "print('Number of features in organizational soc vectorizer: {}'.format(len(orgs_vectorizer.get_feature_names())))\n",
    "print('Every 1000th word:\\n{}'.format(orgs_vectorizer.get_feature_names()[::1000])) # get every 1000th word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Generates Balanced Pipelines\n",
    "######################################################\n",
    "\n",
    "def make_model_pipeline(model, undersample=False, sampling_ratio = 1.0, use_SMOTE=False, random_state=None):\n",
    "    \"\"\"\n",
    "    Creates an sklearn pipeline object to handle over/under sampling or SMOTE sampling\n",
    "    This is to avoid data leakage from oversampling before partitioning for cross-validation\n",
    "    Apparently, this will not effect the test data when running grid search, which is desired\n",
    "    \n",
    "    Args:\n",
    "        model: An sklearn classifier model to be packaged with an over/under sampling model\n",
    "        undersample: boolean for over or undersampling\n",
    "        sampling_ratio: ratio of minority to majority class\n",
    "        use_SMOTE: boolean for enabling SMOTE (overrides undersample)\n",
    "        \n",
    "    Returns:\n",
    "        A pipeline that will handle under/over sampling without the prior data leakage errors\n",
    "    \"\"\"\n",
    "    # All sampling methods use the same arguments, so do this to save time\n",
    "    kwargs = {'sampling_strategy':sampling_ratio, 'random_state': random_state}\n",
    "    if use_SMOTE:\n",
    "        sampler = SMOTE(**kwargs)\n",
    "    elif undersample:\n",
    "        sampler = RandomUnderSampler(**kwargs)\n",
    "    else:\n",
    "        sampler = RandomOverSampler(**kwargs)\n",
    "    return Pipeline(steps=[('sampler', sampler), ('model', model)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(text, vectorizer_model, class_model):\n",
    "    '''\n",
    "    Predicts the label for an input text using a given model trained to classify the texts. \n",
    "    Uses vectorizer_model to restrict the vocab of the input text so it's consistent with vocab in class_model (avoids errors).\n",
    "    \n",
    "    Args:\n",
    "        text: preprocessed text in format of list of sentences, each a str or list of tokens\n",
    "        vectorizer_model: fitted text vectorizer\n",
    "        class_model: trained classification model\n",
    "    Returns:\n",
    "        label: label for text predicted by model, false for tie\n",
    "        prob: probability for label\n",
    "    '''\n",
    "    \n",
    "    X = vectorizer_model.transform(text) # create TF-IDF-weighted DTM from text\n",
    "    try:\n",
    "        probabilities = class_model.predict_proba(X)\n",
    "    except: \n",
    "        return\n",
    "    \n",
    "    label = 'no'\n",
    "    prob_no = probabilities[0][0]\n",
    "    prob_yes = probabilities[0][1]\n",
    "    \n",
    "    # predicted label is one with greater probability\n",
    "    if probabilities[0][0] < probabilities[0][1]:\n",
    "        label = 'yes'\n",
    "        \n",
    "    return label, prob_yes, prob_no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define algorithms and hyperparameter grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Create different models with different hyperparameter grids\n",
    "######################################################\n",
    "seed = 42\n",
    "models = []\n",
    "\n",
    "# The first item in the tuple is the name of the algorithm\n",
    "# The second item is the estimator object itself\n",
    "# The third item is the param grid\n",
    "\n",
    "models.append(('DecisionTree', DecisionTreeClassifier(random_state=seed), {\n",
    "                        'criterion': ['gini','entropy'],\n",
    "                        'min_samples_split': range(0, 21, 6),\n",
    "                        'max_depth': range(6, 21, 4)\n",
    "                    }))\n",
    "\n",
    "models.append(('RandomForest', \n",
    "               RandomForestClassifier(random_state=seed),\n",
    "                    {\n",
    "                        'criterion': ['gini','entropy'],\n",
    "                        'n_estimators': range(200, 900, 200),\n",
    "                        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                        'max_depth': list(range(2, 21, 5)) + [None]\n",
    "                    }))\n",
    "\n",
    "# models.append(('LogisticRegression', \n",
    "#                LogisticRegression(random_state=seed), \n",
    "#                {'penalty': ['l2', 'elasticnet', 'none'],'C': np.logspace(-4, 6, num=5)}))\n",
    "\n",
    "# models.append(('SupportVectorMachine', SVC(gamma='auto'), \n",
    "#                {'C': np.logspace(-4, 6, num=10), 'gamma': [0.001, 0.0001], 'kernel': ['rbf', 'linear']}\n",
    "#               ))\n",
    "\n",
    "models.append(('PassiveAggressive', \n",
    "               PassiveAggressiveClassifier(random_state=seed, n_jobs=-1), \n",
    "               {'C': np.logspace(-4, 6, num=10), 'loss': ['hinge', 'squared_hinge']}))\n",
    "\n",
    "\n",
    "models.append(('AdaBoost', AdaBoostClassifier(random_state=seed), \n",
    "               {'n_estimators': range(500, 2501, 500), 'learning_rate':[0.0001, 0.001, 0.01, 0.1]}))\n",
    "\n",
    "models.append(('MultiLayerPerceptron', \n",
    "               MLPClassifier(random_state=seed, max_iter=500),\n",
    "            {'hidden_layer_sizes': [(50,50), (100,50), (100,)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.05],\n",
    "            'learning_rate': ['adaptive', 'constant']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(X, y, cv=10, metrics=[\n",
    "    'mean_test_balanced_accuracy', 'mean_test_f1', \n",
    "    'mean_test_precision', 'mean_test_recall',\n",
    "    'mean_test_accuracy', 'mean_train_accuracy']):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs a grid search over all the models in the models variable for the given X and y dataset.\n",
    "    This produces a cross-validation and gathers some useful metrics.\n",
    "    \n",
    "    Args:\n",
    "        X: the input training data of shape (n_samples, n_features)\n",
    "        y: the vector of labels of shape (n_samples,) or (n_samples, n_targets)\n",
    "        cv: int, cross-validation generator or an iterable, Determines the cross-validation splitting strategy\n",
    "        metrics: list of metrics to gather, from https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "                the first argument of this is used as the refit parameter\n",
    "    Returns:\n",
    "        A dataframe containing the parameters of the best fit model for each model and sampling type\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Make raw_metrics a list of the unique metrics (f1, accuracy, etc) without the \"mean_test_\" prefix\n",
    "    temp_metrics = [re.sub(r'\\w+?_\\w+?_', '', metric) for metric in metrics]\n",
    "    raw_metrics = []\n",
    "    for metric in temp_metrics:\n",
    "        if metric not in raw_metrics:\n",
    "            raw_metrics.append(metric)\n",
    "    \n",
    "    # iterate over every model in the models list\n",
    "    for model_name, model, params in models:\n",
    "        \n",
    "        # modify the param keys because the pipeline changes some names\n",
    "        cv_params = {f'model__{key}': value for key, value in params.items()}\n",
    "        \n",
    "        # also iterate over each sampling type\n",
    "        for sampling_name, under, smote in [\n",
    "            ('under', True, False), ('over', False, False), ('smote', False, True)]:\n",
    "            \n",
    "            # create a resampling pipeline object\n",
    "            pipeline = make_model_pipeline(model, undersample=under, use_SMOTE=smote)\n",
    "            \n",
    "            # run gridsearch \n",
    "            gscv = GridSearchCV(\n",
    "                pipeline, param_grid=cv_params, cv=cv, \n",
    "                scoring=raw_metrics,\n",
    "                return_train_score=True, refit=raw_metrics[0])\n",
    "            gscv.fit(X, y)\n",
    "\n",
    "            # find and store the best performing parameters\n",
    "            row = {'Name':f'{model_name} {sampling_name}',\n",
    "                   'Params':gscv.best_params_}\n",
    "            results, ind = gscv.cv_results_, gscv.best_index_\n",
    "            \n",
    "            for metric in metrics:\n",
    "                row[metric[5:]] = results[metric][ind]\n",
    "                print(model_name, sampling_name, metric[5:], results[metric][ind])\n",
    "            data.append(row)\n",
    "            \n",
    "            print()\n",
    "        print()\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 709\n",
      "Number of codes (should match): 709\n",
      "DecisionTree under test_balanced_accuracy 0.5255502235584335\n",
      "DecisionTree under test_f1 0.40561756678134875\n",
      "DecisionTree under test_precision 0.35156854561204937\n",
      "DecisionTree under test_recall 0.4871376811594203\n",
      "DecisionTree under test_accuracy 0.5387726358148892\n",
      "DecisionTree under train_accuracy 0.7956456257573304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the grid search on the culture data\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "print(\"Number of cases:\", str(X_cult.shape[0]))\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "cult_params = gridsearch(X_cult, Y)\n",
    "cult_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search on the relational data\n",
    "\n",
    "relt_df = relt_df[['text', 'relational_score']]\n",
    "print(\"Number of cases:\", str(X_relt.shape[0]))\n",
    "\n",
    "valueArray = relt_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "relt_params = gridsearch(X_relt, Y)\n",
    "relt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search on the demographic data\n",
    "\n",
    "demog_df = demog_df[['text', 'demographic_score']]\n",
    "print(\"Number of cases:\", str(X_demog.shape[0]))\n",
    "\n",
    "valueArray = demog_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "demog_params = gridsearch(X_demog, Y)\n",
    "demog_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save each dataframe to a csv in the logs folder\n",
    "\n",
    "cult_params.to_csv(path_or_buf=logs + f'cult_grid_{thisday}.csv', index=False)\n",
    "relt_params.to_csv(path_or_buf=logs + f'relt_grid_{thisday}.csv', index=False)\n",
    "demog_params.to_csv(path_or_buf=logs + f'demog_grid_{thisday}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using balanced samples from hand-labeled set of articles\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: December 2020\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, KFold\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/modeling', '')\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + 'classification/data/'\n",
    "model_fp = root + 'classification/models/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_121120.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_121120.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_121120.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + 'vectorizer_cult_121120.joblib'\n",
    "relt_vec_fp = model_fp + 'vectorizer_relt_121120.joblib'\n",
    "demog_vec_fp = model_fp + 'vectorizer_demog_121120.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultural_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, of, managerial, issues, vol], [xxii...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[organization, ht, icna, vol], [no], [may, ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[from, fiefs, to, clans, and, network, capita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[the, collective, strategy, framework, an, ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[manag, int, rev, doi, sl, research, article,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[int], [studies, ofmgt], [amp, org, vol], [no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[linking, organizational, values, to, relatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, of, organizational, behavior, organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[®, academy, oí, management, learning, amp, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[strategie, management, journal, strat], [mgm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultural_score                                               text\n",
       "0             1.0  [[journal, of, managerial, issues, vol], [xxii...\n",
       "1             1.0  [[organization, ht, icna, vol], [no], [may, ju...\n",
       "2             1.0  [[from, fiefs, to, clans, and, network, capita...\n",
       "3             1.0  [[the, collective, strategy, framework, an, ap...\n",
       "4             1.0  [[manag, int, rev, doi, sl, research, article,...\n",
       "5             1.0  [[int], [studies, ofmgt], [amp, org, vol], [no...\n",
       "6             1.0  [[linking, organizational, values, to, relatio...\n",
       "7             1.0  [[journal, of, organizational, behavior, organ...\n",
       "8             1.0  [[®, academy, oí, management, learning, amp, e...\n",
       "9             1.0  [[strategie, management, journal, strat], [mgm..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    209\n",
      "0.5     12\n",
      "1.0    105\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    229\n",
      "0.5      7\n",
      "1.0     92\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    248\n",
      "0.5      5\n",
      "1.0     77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsure cases: where X_score = 0.5\n",
    "drop_unsure = True\n",
    "\n",
    "if drop_unsure:\n",
    "    cult_df_yes = cult_df[cult_df['cultural_score'] == 1.0]\n",
    "    cult_df_no = cult_df[cult_df['cultural_score'] == 0.0]\n",
    "    cult_df = pd.concat([cult_df_yes, cult_df_no])\n",
    "    \n",
    "    relt_df_yes = relt_df[relt_df['relational_score'] == 1.0]\n",
    "    relt_df_no = relt_df[relt_df['relational_score'] == 0.0]\n",
    "    relt_df = pd.concat([relt_df_yes, relt_df_no])\n",
    "    \n",
    "    demog_df_yes = demog_df[demog_df['demographic_score'] == 1.0]\n",
    "    demog_df_no = demog_df[demog_df['demographic_score'] == 0.0]\n",
    "    demog_df = pd.concat([demog_df_yes, demog_df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_article_tokens(article, return_string=False):\n",
    "    '''\n",
    "    Collects words from already-tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "        return_string: whether to return single, long string representing article\n",
    "    Returns:\n",
    "        tokens: string if return_string, else list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = [] # initialize\n",
    "    \n",
    "    if return_string:\n",
    "        for sent in article:\n",
    "            sent = ' '.join(sent) # make sentence into a string\n",
    "            tokens.append(sent) # add sentence to list of sentences\n",
    "        tokens = ' '.join(tokens) # join sentences into string\n",
    "        return tokens # return string\n",
    "    \n",
    "    else:\n",
    "        for sent in article:\n",
    "            tokens += [word for word in sent] # add each word to list of tokens\n",
    "        return tokens # return list of tokens\n",
    "\n",
    "# For capturing word frequencies, add all words from each article to single, shared list (can't use this to create models)\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 75614\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 541746),\n",
       " ('of', 388052),\n",
       " ('and', 300309),\n",
       " ('in', 210253),\n",
       " ('to', 197892),\n",
       " ('that', 106610),\n",
       " ('is', 95025),\n",
       " ('for', 85691),\n",
       " ('as', 69812),\n",
       " ('on', 57671),\n",
       " ('are', 57133),\n",
       " ('with', 53278),\n",
       " ('by', 51497),\n",
       " ('this', 48992),\n",
       " ('be', 42995),\n",
       " ('oasis', 40297),\n",
       " ('or', 38481),\n",
       " ('it', 36482),\n",
       " ('from', 36262),\n",
       " ('an', 35447)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = (cult_tokens + relt_tokens) + demog_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 472263\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('valign bottom oasis entry oasis entry colname colsep rowsep align char char',\n",
       "  3751),\n",
       " ('pp', 1425),\n",
       " ('oasis entry oasis entry colname colsep rowsep align char char', 1271),\n",
       " ('no', 1100),\n",
       " ('oasis entry colname colsep rowsep align char char', 1077),\n",
       " ('american sociological review', 1041),\n",
       " ('american journal of sociology', 1000),\n",
       " ('administrative science quarterly', 941),\n",
       " ('valign bottom oasis entry colname colsep rowsep align char char', 938),\n",
       " ('sci', 491),\n",
       " ('academy of management journal', 444),\n",
       " ('chicago university of chicago press', 443),\n",
       " ('organ', 365),\n",
       " ('colsep rowsep oasis entry align char char', 360),\n",
       " ('academy of management review', 359),\n",
       " ('ed', 322),\n",
       " ('new york oxford university press', 317),\n",
       " ('new york free press', 307),\n",
       " ('oasis entry oasis entry colsep rowsep align char char', 296),\n",
       " ('cambridge ma harvard university press', 291)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(\n",
    "    lambda article: cult_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(\n",
    "    lambda article: relt_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(\n",
    "    lambda article: demog_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = (cult_sents + relt_sents) + demog_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and apply text vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27     None\n",
       "38     None\n",
       "40     None\n",
       "140    None\n",
       "146    None\n",
       "154    None\n",
       "136    None\n",
       "137    None\n",
       "138    None\n",
       "139    None\n",
       "140    None\n",
       "141    None\n",
       "142    None\n",
       "143    None\n",
       "144    None\n",
       "145    None\n",
       "146    None\n",
       "147    None\n",
       "148    None\n",
       "149    None\n",
       "150    None\n",
       "151    None\n",
       "152    None\n",
       "153    None\n",
       "154    None\n",
       "155    None\n",
       "156    None\n",
       "157    None\n",
       "158    None\n",
       "159    None\n",
       "       ... \n",
       "229    None\n",
       "230    None\n",
       "231    None\n",
       "232    None\n",
       "233    None\n",
       "234    None\n",
       "235    None\n",
       "237    None\n",
       "238    None\n",
       "239    None\n",
       "240    None\n",
       "241    None\n",
       "242    None\n",
       "243    None\n",
       "244    None\n",
       "245    None\n",
       "246    None\n",
       "247    None\n",
       "248    None\n",
       "249    None\n",
       "250    None\n",
       "251    None\n",
       "252    None\n",
       "253    None\n",
       "254    None\n",
       "255    None\n",
       "256    None\n",
       "257    None\n",
       "258    None\n",
       "259    None\n",
       "Name: text, Length: 325, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect articles: Add each article as single str to list of str:\n",
    "cult_docs = [] # empty list\n",
    "cult_df['text'].apply(\n",
    "    lambda article: cult_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "relt_docs = [] # empty list\n",
    "relt_df['text'].apply(\n",
    "    lambda article: relt_docs.append(\n",
    "       collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "demog_docs = [] # empty list\n",
    "demog_df['text'].apply(\n",
    "    lambda article: demog_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "print() # skip weird output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 62115\n",
      "['aa', 'afixed', 'andp', 'artes', 'backtocauses', 'beroepsarbeid', 'bottles', 'byzantine', 'cer', 'clemen', 'comports', 'conyon', 'culinary', 'deflation', 'dibia', 'divoky', 'eca', 'endangers', 'ethnonationalism', 'falbe', 'flavors', 'fuelwood', 'giraud', 'guigni', 'hennis', 'hsd', 'impinging', 'inome', 'ipb', 'journalof', 'kohl', 'leaking', 'loans', 'makrellag', 'mcluhan', 'ministrative', 'muisic', 'nerve', 'notable', 'ool', 'owning', 'percap', 'plummer', 'premised', 'ps', 'rationalities', 'reincarnated', 'revamping', 'ruef', 'schumpeterian', 'shaper', 'sleek', 'spouses', 'studentized', 'symbol', 'testimony', 'tongkonan', 'turation', 'unify', 'vaughan', 'wasp', 'workweeks', 'íí']\n",
      "\n",
      "Number of features in relational vectorizer: 62933\n",
      "['aa', 'afred', 'andre', 'artifacts', 'bacteriophage', 'berscheid', 'bottom', 'búngalo', 'certifying', 'clientage', 'compromise', 'copenhagen', 'current', 'delegitimated', 'difficultto', 'dodüü', 'eder', 'enitity', 'everlastingly', 'favorable', 'folow', 'galaxy', 'golding', 'haligarten', 'highbrows', 'hypergeometric', 'incline', 'insurrection', 'issuance', 'kagan', 'koza', 'lehu', 'longerterm', 'mammaw', 'mechanistic', 'miser', 'multidisciplinary', 'neural', 'noticeable', 'onomy', 'originates', 'parekh', 'pgwide', 'popularized', 'prison', 'pvalue', 'receptivity', 'reor', 'ripe', 'salaman', 'seditious', 'showers', 'sociograms', 'stanfotd', 'substan', 'tainability', 'thetraversalis', 'tradiction', 'uddannelserne', 'unrepresentative', 'vida', 'werner', 'xnt']\n",
      "\n",
      "Number of features in demographic vectorizer: 60956\n",
      "['aa', 'ag', 'andt', 'artl', 'bagh', 'betsos', 'bowker', 'calculative', 'champion', 'cmcap', 'concrete', 'corrob', 'cyr', 'demur', 'dirigiste', 'doubled', 'eie', 'environmental', 'exit', 'ffcz', 'foucauldian', 'gence', 'gratton', 'hasten', 'hollingshead', 'ij', 'infallibility', 'intervsew', 'jerszy', 'kilman', 'larkey', 'linux', 'mackie', 'maury', 'mihajlovic', 'mosteller', 'neatly', 'nonrepresentation', 'ojjvcers', 'outwardoriented', 'pe', 'plague', 'preced', 'prospects', 'ramsoedh', 'regularities', 'retrospectively', 'rublic', 'schwarzweller', 'sharkey', 'slovenia', 'squaw', 'subcontracting', 'systemi', 'thechahroan', 'touches', 'tyrants', 'unpredictable', 'vey', 'wenona', 'xo']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Uses TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "cult_vectorizer = joblib.load(cult_vec_fp, \"r+\")\n",
    "X_cult = cult_vectorizer.fit_transform(cult_docs)\n",
    "print('Number of features in cultural vectorizer:', len(cult_vectorizer.get_feature_names()))\n",
    "print(cult_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "relt_vectorizer = joblib.load(relt_vec_fp, \"r+\")\n",
    "X_relt = relt_vectorizer.fit_transform(relt_docs)\n",
    "print('Number of features in relational vectorizer:', len(relt_vectorizer.get_feature_names()))\n",
    "print(relt_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "demog_vectorizer = joblib.load(demog_vec_fp, \"r+\")\n",
    "X_demog = demog_vectorizer.fit_transform(demog_docs)\n",
    "print('Number of features in demographic vectorizer:', len(demog_vectorizer.get_feature_names()))\n",
    "print(demog_vectorizer.get_feature_names()[::1000]) # get every 1000th word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Balance x_train, y_train\n",
    "######################################################\n",
    "\n",
    "def resample_data(X_train, Y_train, undersample = False, sampling_ratio = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train: X training data\n",
    "        Y_train: Y training data\n",
    "        undersample: boolean for over or undersampling\n",
    "        sampling_ratio: ratio of minority to majority class\n",
    "        \n",
    "        archived/not used:\n",
    "        sampling_strategy: strategy for resampled distribution\n",
    "            if oversample: 'majority' makes minority = to majority\n",
    "            if undersample: 'minority' makes majority = to minority\n",
    "            \n",
    "    Returns:\n",
    "        X_balanced: predictors at balanced ratio\n",
    "        Y_balanced: outcomes at balanced ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = undersample.fit_resample(X_train, Y_train)\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = oversample.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    print(f'Y_train: {Counter(Y_train)}\\nY_resample: {Counter(Y_balanced)}')\n",
    "    \n",
    "    return X_balanced, Y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# k-fold cross validation for model evaluation\n",
    "######################################################\n",
    "\n",
    "# Define test options for k-fold CV\n",
    "num_folds = 10 \n",
    "seed = 3\n",
    "scoring='f1_weighted' # set scoring metric (not used here)\n",
    "\n",
    "def show_kfold_output(models, \n",
    "                      X, \n",
    "                      Y, \n",
    "                      num_folds = num_folds, \n",
    "                      random_state = seed, \n",
    "                      shuffle = True):\n",
    "    '''\n",
    "    Estimates the accuracy of different model algorithms, adds results to a results array and returns.\n",
    "    Prints the accuracy results: averages and std.\n",
    "    Uses cross_val_predict, which unlike cross_val_score cannot define scoring option/evaluation metric.\n",
    "    \n",
    "    Args:\n",
    "        models: list of (name, model) tuples\n",
    "        X: predictors\n",
    "        Y: outcomes\n",
    "        num_folds: Split data randomly into num_folds parts: (num_folds-1) for training, 1 for scoring\n",
    "        random_state: seed\n",
    "        shuffle: \n",
    "    \n",
    "    Returns:\n",
    "        results: list of model results\n",
    "        names: list of model names (matches results)\n",
    "        \n",
    "    Source: \n",
    "        https://stackoverflow.com/questions/40057049/using-confusion-matrix-as-scoring-metric-in-cross-validation-in-scikit-learn\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Setup model options\n",
    "        kfold = KFold(\n",
    "            n_splits=num_folds, \n",
    "            random_state=seed, \n",
    "            shuffle=True)\n",
    "        \n",
    "        # Get kfold results\n",
    "        cv_results = cross_val_predict(\n",
    "            model, \n",
    "            X, \n",
    "            Y, \n",
    "            cv=kfold, \n",
    "            #scoring=scoring, \n",
    "            n_jobs=-1) # use all cores = faster\n",
    "        \n",
    "        # Add results and name of each algorithm to the model array\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'{name}:')\n",
    "        print()\n",
    "        print(f'Mean (std):\\t {round(cv_results.mean(),4)} ({round(cv_results.std(),4)})')\n",
    "        print(f'Accuracy:\\t', {round(accuracy_score(Y_balanced, cv_results)), 4})\n",
    "        print()\n",
    "        print('Confusion matrix:\\n', confusion_matrix(Y_balanced, cv_results))\n",
    "        print()\n",
    "        print('Report:\\n', classification_report(Y_balanced, cv_results))\n",
    "        print()\n",
    "        \n",
    "    # Return arrays\n",
    "    return results, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 314\n",
      "Number of codes (should match): 314\n",
      "Y_train Distribution: [(0.0, 170), (1.0, 81)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "print(\"Number of cases:\", str(X_cult.shape[0]))\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    test_size=test_size, \n",
    "    random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 209, 1.0: 105})\n",
      "Y_resample: Counter({1.0: 209, 0.0: 209})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "# Use these settings here and below\n",
    "sampling_ratio = 1.0 # ratio of minority to majority cases\n",
    "undersample = False # whether to undersample or oversample\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.622 (0.4849)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[138  71]\n",
      " [ 20 189]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.66      0.75       209\n",
      "         1.0       0.73      0.90      0.81       209\n",
      "\n",
      "    accuracy                           0.78       418\n",
      "   macro avg       0.80      0.78      0.78       418\n",
      "weighted avg       0.80      0.78      0.78       418\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.4856 (0.4998)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[201   8]\n",
      " [ 14 195]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.96      0.95       209\n",
      "         1.0       0.96      0.93      0.95       209\n",
      "\n",
      "    accuracy                           0.95       418\n",
      "   macro avg       0.95      0.95      0.95       418\n",
      "weighted avg       0.95      0.95      0.95       418\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5813 (0.4933)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[160  49]\n",
      " [ 15 194]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.77      0.83       209\n",
      "         1.0       0.80      0.93      0.86       209\n",
      "\n",
      "    accuracy                           0.85       418\n",
      "   macro avg       0.86      0.85      0.85       418\n",
      "weighted avg       0.86      0.85      0.85       418\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.7392 (0.4391)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[108 101]\n",
      " [  1 208]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.52      0.68       209\n",
      "         1.0       0.67      1.00      0.80       209\n",
      "\n",
      "    accuracy                           0.76       418\n",
      "   macro avg       0.83      0.76      0.74       418\n",
      "weighted avg       0.83      0.76      0.74       418\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.4617 (0.4985)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[196  13]\n",
      " [ 29 180]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.94      0.90       209\n",
      "         1.0       0.93      0.86      0.90       209\n",
      "\n",
      "    accuracy                           0.90       418\n",
      "   macro avg       0.90      0.90      0.90       418\n",
      "weighted avg       0.90      0.90      0.90       418\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.5 (0.5)\n",
      "Accuracy:\t {0.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 93 116]\n",
      " [116  93]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.44      0.44       209\n",
      "         1.0       0.44      0.44      0.44       209\n",
      "\n",
      "    accuracy                           0.44       418\n",
      "   macro avg       0.44      0.44      0.44       418\n",
      "weighted avg       0.44      0.44      0.44       418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_cult_121420.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "rf_cult = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(rf_cult, cult_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 321\n",
      "Number of codes (should match): 321\n",
      "Y_train Distribution: [(0.0, 185), (1.0, 71)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "relt_df = relt_df[['text', 'relational_score']]\n",
    "print(\"Number of cases:\", str(X_relt.shape[0]))\n",
    "\n",
    "valueArray = relt_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_relt, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 229, 1.0: 92})\n",
      "Y_resample: Counter({1.0: 229, 0.0: 229})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_relt, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6114 (0.4874)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[168  61]\n",
      " [ 10 219]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.73      0.83       229\n",
      "         1.0       0.78      0.96      0.86       229\n",
      "\n",
      "    accuracy                           0.84       458\n",
      "   macro avg       0.86      0.84      0.84       458\n",
      "weighted avg       0.86      0.84      0.84       458\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.4934 (0.5)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[225   4]\n",
      " [  7 222]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98       229\n",
      "         1.0       0.98      0.97      0.98       229\n",
      "\n",
      "    accuracy                           0.98       458\n",
      "   macro avg       0.98      0.98      0.98       458\n",
      "weighted avg       0.98      0.98      0.98       458\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5349 (0.4988)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[207  22]\n",
      " [  6 223]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.90      0.94       229\n",
      "         1.0       0.91      0.97      0.94       229\n",
      "\n",
      "    accuracy                           0.94       458\n",
      "   macro avg       0.94      0.94      0.94       458\n",
      "weighted avg       0.94      0.94      0.94       458\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.6812 (0.466)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[144  85]\n",
      " [  2 227]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.63      0.77       229\n",
      "         1.0       0.73      0.99      0.84       229\n",
      "\n",
      "    accuracy                           0.81       458\n",
      "   macro avg       0.86      0.81      0.80       458\n",
      "weighted avg       0.86      0.81      0.80       458\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.476 (0.4994)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[223   6]\n",
      " [ 17 212]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95       229\n",
      "         1.0       0.97      0.93      0.95       229\n",
      "\n",
      "    accuracy                           0.95       458\n",
      "   macro avg       0.95      0.95      0.95       458\n",
      "weighted avg       0.95      0.95      0.95       458\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.6288 (0.4831)\n",
      "Accuracy:\t {0.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 79 150]\n",
      " [ 91 138]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.34      0.40       229\n",
      "         1.0       0.48      0.60      0.53       229\n",
      "\n",
      "    accuracy                           0.47       458\n",
      "   macro avg       0.47      0.47      0.46       458\n",
      "weighted avg       0.47      0.47      0.46       458\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_relt_121420.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "rf_relt = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(rf_relt, relt_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 325\n",
      "Number of codes (should match): 325\n",
      "Y_train Distribution: [(0.0, 200), (1.0, 60)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First reltove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "demog_df = demog_df[['text', 'demographic_score']]\n",
    "print(\"Number of cases:\", str(X_demog.shape[0]))\n",
    "\n",
    "valueArray = demog_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_demog, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 248, 1.0: 77})\n",
      "Y_resample: Counter({1.0: 248, 0.0: 248})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_demog, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6512 (0.4766)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[171  77]\n",
      " [  2 246]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.69      0.81       248\n",
      "         1.0       0.76      0.99      0.86       248\n",
      "\n",
      "    accuracy                           0.84       496\n",
      "   macro avg       0.88      0.84      0.84       496\n",
      "weighted avg       0.88      0.84      0.84       496\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.502 (0.5)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[244   4]\n",
      " [  3 245]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99       248\n",
      "         1.0       0.98      0.99      0.99       248\n",
      "\n",
      "    accuracy                           0.99       496\n",
      "   macro avg       0.99      0.99      0.99       496\n",
      "weighted avg       0.99      0.99      0.99       496\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5625 (0.4961)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[215  33]\n",
      " [  2 246]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.87      0.92       248\n",
      "         1.0       0.88      0.99      0.93       248\n",
      "\n",
      "    accuracy                           0.93       496\n",
      "   macro avg       0.94      0.93      0.93       496\n",
      "weighted avg       0.94      0.93      0.93       496\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.6472 (0.4778)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[175  73]\n",
      " [  0 248]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.71      0.83       248\n",
      "         1.0       0.77      1.00      0.87       248\n",
      "\n",
      "    accuracy                           0.85       496\n",
      "   macro avg       0.89      0.85      0.85       496\n",
      "weighted avg       0.89      0.85      0.85       496\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.4899 (0.4999)\n",
      "Accuracy:\t {1.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[245   3]\n",
      " [  8 240]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       248\n",
      "         1.0       0.99      0.97      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.5 (0.5)\n",
      "Accuracy:\t {0.0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[115 133]\n",
      " [133 115]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.46      0.46      0.46       248\n",
      "         1.0       0.46      0.46      0.46       248\n",
      "\n",
      "    accuracy                           0.46       496\n",
      "   macro avg       0.46      0.46      0.46       496\n",
      "weighted avg       0.46      0.46      0.46       496\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_demog_121420.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "rf_demog = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(rf_demog, demog_model_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

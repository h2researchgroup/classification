{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using balanced samples from hand-coding\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: December 2020\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/modeling', '')\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + 'classification/data/'\n",
    "model_fp = root + 'classification/models/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_112420.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_112420.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_112420.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + f'vectorizer_cult_{str(thisday)}.joblib'\n",
    "relt_vec_fp = model_fp + f'vectorizer_relt_{str(thisday)}.joblib'\n",
    "demog_vec_fp = model_fp + f'vectorizer_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vocab of vectorizers (for verification purposes)\n",
    "cult_vec_feat_fp = model_fp + f'vectorizer_features_cult_{str(thisday)}.csv'\n",
    "relt_vec_feat_fp = model_fp + f'vectorizer_features_relt_{str(thisday)}.csv'\n",
    "demog_vec_feat_fp = model_fp + f'vectorizer_features_demog_{str(thisday)}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultural_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, managerial, issues, vol], [xxiii, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[organization, ht, icna, vol], [may, june, pp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[from, fiefs, clans, network, capitalism, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[collective, strategy, framework, application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[manag, int, rev, doi, sl, research, article,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[int], [studies, ofmgt], [amp, org, vol], [pp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[linking, organizational, values, relationshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, organizational, behavior, organiz],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[®, academy, oí, management, learning, amp, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[strategie, management, journal, strat], [mgm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultural_score                                               text\n",
       "0             1.0  [[journal, managerial, issues, vol], [xxiii, n...\n",
       "1             1.0  [[organization, ht, icna, vol], [may, june, pp...\n",
       "2             1.0  [[from, fiefs, clans, network, capitalism, exp...\n",
       "3             1.0  [[collective, strategy, framework, application...\n",
       "4             1.0  [[manag, int, rev, doi, sl, research, article,...\n",
       "5             1.0  [[int], [studies, ofmgt], [amp, org, vol], [pp...\n",
       "6             1.0  [[linking, organizational, values, relationshi...\n",
       "7             1.0  [[journal, organizational, behavior, organiz],...\n",
       "8             1.0  [[®, academy, oí, management, learning, amp, e...\n",
       "9             1.0  [[strategie, management, journal, strat], [mgm..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    209\n",
      "0.5     12\n",
      "1.0    105\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    229\n",
      "0.5      7\n",
      "1.0     92\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    248\n",
      "0.5      5\n",
      "1.0     77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect article tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_article_tokens(article):\n",
    "    '''\n",
    "    Collects words from tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "    Returns:\n",
    "        list: single list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for sent in article:\n",
    "        tokens += [word for word in sent]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "# Add words from each article to empty list:\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 83116\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('oasis', 40297),\n",
       " ('from', 36982),\n",
       " ('entry', 34945),\n",
       " ('we', 32514),\n",
       " ('social', 30979),\n",
       " ('have', 27674),\n",
       " ('which', 27441),\n",
       " ('more', 26129),\n",
       " ('were', 21858),\n",
       " ('char', 20565),\n",
       " ('new', 20537),\n",
       " ('one', 20489),\n",
       " ('other', 19192),\n",
       " ('rowsep', 18993),\n",
       " ('colsep', 18958),\n",
       " ('between', 18184),\n",
       " ('than', 18113),\n",
       " ('can', 17941),\n",
       " ('has', 17494),\n",
       " ('organizational', 16978)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = (cult_tokens + relt_tokens) + demog_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 482124\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('valign bottom oasis entry oasis entry colname colsep rowsep align char char',\n",
       "  3751),\n",
       " ('pp', 1427),\n",
       " ('oasis entry oasis entry colname colsep rowsep align char char', 1271),\n",
       " ('oasis entry colname colsep rowsep align char char', 1077),\n",
       " ('american sociological review', 1046),\n",
       " ('american journal sociology', 1015),\n",
       " ('administrative science quarterly', 951),\n",
       " ('valign bottom oasis entry colname colsep rowsep align char char', 938),\n",
       " ('sci', 556),\n",
       " ('chicago university chicago press', 469),\n",
       " ('academy management journal', 461),\n",
       " ('organ', 407),\n",
       " ('academy management review', 394),\n",
       " ('new york free press', 360),\n",
       " ('colsep rowsep oasis entry align char char', 360),\n",
       " ('new york oxford university press', 322),\n",
       " ('cambridge ma harvard university press', 296),\n",
       " ('oasis entry oasis entry colsep rowsep align char char', 296),\n",
       " ('admin', 292),\n",
       " ('ed', 288)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(lambda article: cult_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(lambda article: relt_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(lambda article: demog_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = (cult_sents + relt_sents) + demog_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 66962\n",
      "\n",
      "Number of features in relational vectorizer: 67936\n",
      "\n",
      "Number of features in demographic vectorizer: 65796\n",
      "\n",
      "['000000000000o', '2r', '9p', 'afag', 'anachronistic', 'arised', 'avowed', 'belie', 'bogran', 'bur', 'cating', 'cinlar', 'commensurable', 'consumptive', 'cristoph', 'debug', 'destructive', 'dispersed', 'dulcimer', 'emancipation', 'erupted', 'externals', 'finalists', 'frank', 'gen', 'graphs', 'harmonious', 'hlend', 'idealization', 'indexing', 'interdependent', 'ix', 'kanov', 'kuczyiski', 'lentils', 'loose', 'maniha', 'megarry', 'misuse', 'multitudinous', 'nevitt', 'novel', 'onoe', 'overseen', 'pedulla1', 'plantación', 'preceed', 'prosharing', 'railroads', 'regain', 'restructurings', 'routley', 'schary', 'setabout', 'sisyphus', 'specialist', 'straighten', 'surowiecki', 'technocrats', 'timie', 'tribution', 'undefined', 'ushered', 'voir', 'wildlife', 'yi']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Use TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "#vectorizer = CountVectorizer(max_features=100000, min_df=1, max_df=0.8, stop_words=jstor_stopwords) # DTM\n",
    "vectorizer = TfidfVectorizer(max_features=100000, min_df=1, max_df=0.8, stop_words=jstor_stopwords) # TFIDF\n",
    "\n",
    "X_cult = vectorizer.fit_transform(cult_tokens)\n",
    "joblib.dump(vectorizer, open(cult_vec_fp, \"wb\"))\n",
    "with open(cult_vec_feat_fp,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([vectorizer.get_feature_names()])\n",
    "    \n",
    "print('Number of features in cultural vectorizer:', len(vectorizer.get_feature_names()))\n",
    "print()\n",
    "\n",
    "X_relt = vectorizer.fit_transform(relt_tokens)\n",
    "joblib.dump(vectorizer, open(relt_vec_fp, \"wb\"))\n",
    "with open(relt_vec_feat_fp,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([vectorizer.get_feature_names()])\n",
    "    \n",
    "print('Number of features in relational vectorizer:', len(vectorizer.get_feature_names()))\n",
    "print()\n",
    "\n",
    "X_demog = vectorizer.fit_transform(demog_tokens)\n",
    "joblib.dump(vectorizer, open(demog_vec_fp, \"wb\"))\n",
    "with open(demog_vec_feat_fp,'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([vectorizer.get_feature_names()])\n",
    "\n",
    "print('Number of features in demographic vectorizer:', len(vectorizer.get_feature_names()))\n",
    "print()\n",
    "\n",
    "print(vectorizer.get_feature_names()[::1000]) # get every 1000th word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Balance x_train, y_train\n",
    "######################################################\n",
    "\n",
    "def resample_data(X_train, Y_train, undersample, sampling_strategy):\n",
    "    \"\"\"\n",
    "    args\n",
    "        X_train: X training data\n",
    "        Y_train: Y training data\n",
    "        undersmample: boolean for over or undersampling\n",
    "        sampling_strategy: strategy for resampled distribution\n",
    "            if oversample: 'majority' makes minority = to majority\n",
    "            if undersample: 'minority' makes majority = to minority\n",
    "    \"\"\"\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "        X_balanced, Y_balanced = undersample.fit_resample(X_train, Y_train)\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_strategy)\n",
    "        X_balanced, Y_balanced = oversample.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    print(f'Y_train: {Counter(Y_train)}\\nY_resample: {Counter(Y_balanced)}')\n",
    "    \n",
    "    return X_balanced, Y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060700"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cult.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2060700, 326]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-848cf8550565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_cult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Y_train Distribution: {Counter(Y_train).most_common()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2060700, 326]"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_cult, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')\n",
    "\n",
    "# Setup 10-fold cross validation to estimate the accuracy of different models\n",
    "# Split data into 10 parts\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "# num_instances = len(X_train)\n",
    "seed = 7\n",
    "scoring='f1_weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 331, 1.0: 21})\n",
      "Y_resample: Counter({0.0: 331, 1.0: 165})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Undersample to minority size\n",
    "######################################################\n",
    "sampling_strategy = .7\n",
    "undersample = False\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(X_train, Y_train, undersample=undersample, sampling_strategy=sampling_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Disinfectants myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.9643, (0.0291)\n",
      "RF: 1.0, (0.0)\n",
      "DT: 0.9879, (0.0161)\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "# Add each algorithm and its name to the model array\n",
    "models = []\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "# Evaluate each model, add results to a results array,\n",
    "# Print the accuracy results (remember these are averages and std)\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_balanced, Y_balanced, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {round(cv_results.mean(),4)}, ({round(cv_results.std(),4)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Disinfectants myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 331), (1.0, 21)]\n",
      "0.9659090909090909\n",
      "[[82  0]\n",
      " [ 3  3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      1.00      0.98        82\n",
      "         1.0       1.00      0.50      0.67         6\n",
      "\n",
      "    accuracy                           0.97        88\n",
      "   macro avg       0.98      0.75      0.82        88\n",
      "weighted avg       0.97      0.97      0.96        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 331), (1.0, 165)]\n",
      "0.9318181818181818\n",
      "[[77  5]\n",
      " [ 1  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.94      0.96        82\n",
      "         1.0       0.50      0.83      0.62         6\n",
      "\n",
      "    accuracy                           0.93        88\n",
      "   macro avg       0.74      0.89      0.79        88\n",
      "weighted avg       0.95      0.93      0.94        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_dis = KNeighborsClassifier()\n",
    "knn_dis.fit(X_train, Y_train)\n",
    "knn_predictions = knn_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_dis = KNeighborsClassifier()\n",
    "knn_dis.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Disinfectants myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 331), (1.0, 21)]\n",
      "0.9545454545454546\n",
      "[[82  0]\n",
      " [ 4  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.98        82\n",
      "         1.0       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.95        88\n",
      "   macro avg       0.98      0.67      0.74        88\n",
      "weighted avg       0.96      0.95      0.94        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 331), (1.0, 165)]\n",
      "0.9772727272727273\n",
      "[[82  0]\n",
      " [ 2  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99        82\n",
      "         1.0       1.00      0.67      0.80         6\n",
      "\n",
      "    accuracy                           0.98        88\n",
      "   macro avg       0.99      0.83      0.89        88\n",
      "weighted avg       0.98      0.98      0.98        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_dis = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_dis.fit(X_train, Y_train) \n",
    "rf_predictions = rf_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "rf_dis = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_dis.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Disinfectants myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 331), (1.0, 21)]\n",
      "0.9886363636363636\n",
      "[[82  0]\n",
      " [ 1  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99        82\n",
      "         1.0       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.99        88\n",
      "   macro avg       0.99      0.92      0.95        88\n",
      "weighted avg       0.99      0.99      0.99        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 331), (1.0, 165)]\n",
      "0.9886363636363636\n",
      "[[82  0]\n",
      " [ 1  5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99        82\n",
      "         1.0       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.99        88\n",
      "   macro avg       0.99      0.92      0.95        88\n",
      "weighted avg       0.99      0.99      0.99        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_dis = DecisionTreeClassifier()\n",
    "dt_dis.fit(X_train, Y_train)\n",
    "dt_predictions = dt_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "dt_dis = DecisionTreeClassifier()\n",
    "dt_dis.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_dis.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "# joblib.dump(rf_dis, dis_mod_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train Distribution: [(0.0, 324), (1.0, 28)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "valueArray = rem_df_is_myth.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_rem, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')\n",
    "\n",
    "# Setup 10-fold cross validation to estimate the accuracy of different models\n",
    "# Split data into 10 parts\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "# num_instances = len(X_train)\n",
    "seed = 7\n",
    "scoring = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 324, 1.0: 28})\n",
      "Y_resample: Counter({0.0: 324, 1.0: 162})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Undersample to minority size\n",
    "######################################################\n",
    "sampling_strategy = .5\n",
    "undersample = False\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(X_train, Y_train, undersample=undersample, sampling_strategy=sampling_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Home remedies myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.9241, (0.0531)\n",
      "RF: 0.9979, (0.0062)\n",
      "DT: 0.9636, (0.0216)\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "# Add each algorithm and its name to the model array\n",
    "models = []\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "# Evaluate each model, add results to a results array,\n",
    "# Print the accuracy results (remember these are averages and std)\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_balanced, Y_balanced, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {round(cv_results.mean(),4)}, ({round(cv_results.std(),4)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Home remedies myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 324), (1.0, 28)]\n",
      "0.9318181818181818\n",
      "[[81  1]\n",
      " [ 5  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.99      0.96        82\n",
      "         1.0       0.50      0.17      0.25         6\n",
      "\n",
      "    accuracy                           0.93        88\n",
      "   macro avg       0.72      0.58      0.61        88\n",
      "weighted avg       0.91      0.93      0.92        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 324), (1.0, 162)]\n",
      "0.8522727272727273\n",
      "[[71 11]\n",
      " [ 2  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.87      0.92        82\n",
      "         1.0       0.27      0.67      0.38         6\n",
      "\n",
      "    accuracy                           0.85        88\n",
      "   macro avg       0.62      0.77      0.65        88\n",
      "weighted avg       0.92      0.85      0.88        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_rem = KNeighborsClassifier()\n",
    "knn_rem.fit(X_train, Y_train)\n",
    "knn_predictions = knn_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "# Make predictions on validation dataset\n",
    "knn_rem = KNeighborsClassifier()\n",
    "knn_rem.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Home remedies myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 324), (1.0, 28)]\n",
      "0.9431818181818182\n",
      "[[82  0]\n",
      " [ 5  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97        82\n",
      "         1.0       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.94        88\n",
      "   macro avg       0.97      0.58      0.63        88\n",
      "weighted avg       0.95      0.94      0.92        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 324), (1.0, 162)]\n",
      "0.9431818181818182\n",
      "[[82  0]\n",
      " [ 5  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      1.00      0.97        82\n",
      "         1.0       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.94        88\n",
      "   macro avg       0.97      0.58      0.63        88\n",
      "weighted avg       0.95      0.94      0.92        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_rem = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_rem.fit(X_train, Y_train) \n",
    "rf_predictions = rf_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_rem = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_rem.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Home remedies myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 324), (1.0, 28)]\n",
      "0.8977272727272727\n",
      "[[78  4]\n",
      " [ 5  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.95      0.95        82\n",
      "         1.0       0.20      0.17      0.18         6\n",
      "\n",
      "    accuracy                           0.90        88\n",
      "   macro avg       0.57      0.56      0.56        88\n",
      "weighted avg       0.89      0.90      0.89        88\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 324), (1.0, 162)]\n",
      "0.8863636363636364\n",
      "[[77  5]\n",
      " [ 5  1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94        82\n",
      "         1.0       0.17      0.17      0.17         6\n",
      "\n",
      "    accuracy                           0.89        88\n",
      "   macro avg       0.55      0.55      0.55        88\n",
      "weighted avg       0.89      0.89      0.89        88\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_rem = DecisionTreeClassifier()\n",
    "dt_rem.fit(X_train, Y_train)\n",
    "dt_predictions = dt_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_rem = DecisionTreeClassifier()\n",
    "dt_rem.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_rem.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "# joblib.dump(rf_rem, rem_mod_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train Distribution: [(1.0, 207), (0.0, 13)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "valueArray = wth_df_is_myth.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "test_size = 0.5\n",
    "seed = 15\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_wth, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')\n",
    "\n",
    "# Setup 10-fold cross validation to estimate the accuracy of different models\n",
    "# Split data into 10 parts\n",
    "# Test options and evaluation metric\n",
    "num_folds = 10\n",
    "# num_instances = len(X_train)\n",
    "seed = 7\n",
    "scoring = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Weather myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.9241, (0.0531)\n",
      "RF: 0.9979, (0.0062)\n",
      "DT: 0.9656, (0.0201)\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "# Add each algorithm and its name to the model array\n",
    "models = []\n",
    "models.append(('KNN',KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "# Evaluate each model, add results to a results array,\n",
    "# Print the accuracy results (remember these are averages and std)\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "    cv_results = cross_val_score(model, X_balanced, Y_balanced, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(f'{name}: {round(cv_results.mean(),4)}, ({round(cv_results.std(),4)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({1.0: 207, 0.0: 13})\n",
      "Y_resample: Counter({1.0: 207, 0.0: 103})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Undersample to minority size\n",
    "######################################################\n",
    "sampling_strategy = .5\n",
    "undersample = False\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(X_train, Y_train, undersample=undersample, sampling_strategy=sampling_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Weather myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(1.0, 207), (0.0, 13)]\n",
      "0.9636363636363636\n",
      "[[  0   8]\n",
      " [  0 212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         8\n",
      "         1.0       0.96      1.00      0.98       212\n",
      "\n",
      "    accuracy                           0.96       220\n",
      "   macro avg       0.48      0.50      0.49       220\n",
      "weighted avg       0.93      0.96      0.95       220\n",
      "\n",
      "\n",
      "Balanced Classifier [(1.0, 207), (0.0, 103)]\n",
      "0.9181818181818182\n",
      "[[  1   7]\n",
      " [ 11 201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.08      0.12      0.10         8\n",
      "         1.0       0.97      0.95      0.96       212\n",
      "\n",
      "    accuracy                           0.92       220\n",
      "   macro avg       0.52      0.54      0.53       220\n",
      "weighted avg       0.93      0.92      0.93       220\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac1975/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "# from sklearn.ensemble import BaggingClassifier # improves estimates but hard with so little data\n",
    "#knn_wth = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "knn_wth = KNeighborsClassifier()\n",
    "knn_wth.fit(X_train, Y_train)\n",
    "knn_predictions = knn_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "# from sklearn.ensemble import BaggingClassifier # improves estimates but hard with so little data\n",
    "#knn_wth = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "knn_wth = KNeighborsClassifier()\n",
    "knn_wth.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Weather myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(1.0, 207), (0.0, 13)]\n",
      "0.9636363636363636\n",
      "[[  0   8]\n",
      " [  0 212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         8\n",
      "         1.0       0.96      1.00      0.98       212\n",
      "\n",
      "    accuracy                           0.96       220\n",
      "   macro avg       0.48      0.50      0.49       220\n",
      "weighted avg       0.93      0.96      0.95       220\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac1975/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced Classifier [(1.0, 207), (0.0, 103)]\n",
      "0.9636363636363636\n",
      "[[  0   8]\n",
      " [  0 212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         8\n",
      "         1.0       0.96      1.00      0.98       212\n",
      "\n",
      "    accuracy                           0.96       220\n",
      "   macro avg       0.48      0.50      0.49       220\n",
      "weighted avg       0.93      0.96      0.95       220\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ac1975/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "#rf_wth = BaggingClassifier(RandomForestClassifier(n_estimators=1000, random_state=0), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "rf_wth = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_wth.fit(X_train, Y_train) \n",
    "rf_predictions = rf_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "#rf_wth = BaggingClassifier(RandomForestClassifier(n_estimators=1000, random_state=0), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "rf_wth = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_wth.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Weather myths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(1.0, 207), (0.0, 13)]\n",
      "0.9363636363636364\n",
      "[[  0   8]\n",
      " [  6 206]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         8\n",
      "         1.0       0.96      0.97      0.97       212\n",
      "\n",
      "    accuracy                           0.94       220\n",
      "   macro avg       0.48      0.49      0.48       220\n",
      "weighted avg       0.93      0.94      0.93       220\n",
      "\n",
      "\n",
      "Balanced Classifier [(1.0, 207), (0.0, 103)]\n",
      "0.9090909090909091\n",
      "[[  4   4]\n",
      " [ 16 196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.20      0.50      0.29         8\n",
      "         1.0       0.98      0.92      0.95       212\n",
      "\n",
      "    accuracy                           0.91       220\n",
      "   macro avg       0.59      0.71      0.62       220\n",
      "weighted avg       0.95      0.91      0.93       220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "#dt_wth = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "dt_wth = DecisionTreeClassifier()\n",
    "dt_wth.fit(X_train, Y_train)\n",
    "dt_predictions = dt_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "#dt_wth = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "dt_wth = DecisionTreeClassifier()\n",
    "dt_wth.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_wth.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "# joblib.dump(rf_wth, wth_mod_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

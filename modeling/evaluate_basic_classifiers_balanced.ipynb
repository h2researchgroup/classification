{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using balanced samples from hand-coding\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: December 2020\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, KFold\n",
    "\n",
    "# !pip install imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/modeling', '')\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + 'classification/data/'\n",
    "model_fp = root + 'classification/models/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_112420.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_112420.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_112420.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + 'vectorizer_cult_113020.joblib'\n",
    "relt_vec_fp = model_fp + 'vectorizer_relt_113020.joblib'\n",
    "demog_vec_fp = model_fp + 'vectorizer_demog_113020.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cultural_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, managerial, issues, vol], [xxiii, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[organization, ht, icna, vol], [may, june, pp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[from, fiefs, clans, network, capitalism, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[collective, strategy, framework, application...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[manag, int, rev, doi, sl, research, article,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[int], [studies, ofmgt], [amp, org, vol], [pp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[linking, organizational, values, relationshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[journal, organizational, behavior, organiz],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[®, academy, oí, management, learning, amp, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[[strategie, management, journal, strat], [mgm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cultural_score                                               text\n",
       "0             1.0  [[journal, managerial, issues, vol], [xxiii, n...\n",
       "1             1.0  [[organization, ht, icna, vol], [may, june, pp...\n",
       "2             1.0  [[from, fiefs, clans, network, capitalism, exp...\n",
       "3             1.0  [[collective, strategy, framework, application...\n",
       "4             1.0  [[manag, int, rev, doi, sl, research, article,...\n",
       "5             1.0  [[int], [studies, ofmgt], [amp, org, vol], [pp...\n",
       "6             1.0  [[linking, organizational, values, relationshi...\n",
       "7             1.0  [[journal, organizational, behavior, organiz],...\n",
       "8             1.0  [[®, academy, oí, management, learning, amp, e...\n",
       "9             1.0  [[strategie, management, journal, strat], [mgm..."
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    209\n",
      "0.5     12\n",
      "1.0    105\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    229\n",
      "0.5      7\n",
      "1.0     92\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    248\n",
      "0.5      5\n",
      "1.0     77\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsure cases: where X_score = 0.5\n",
    "drop_unsure = True\n",
    "\n",
    "if drop_unsure:\n",
    "    cult_df_yes = cult_df[cult_df['cultural_score'] == 1.0]\n",
    "    cult_df_no = cult_df[cult_df['cultural_score'] == 0.0]\n",
    "    cult_df = pd.concat([cult_df_yes, cult_df_no])\n",
    "    \n",
    "    relt_df_yes = relt_df[relt_df['relational_score'] == 1.0]\n",
    "    relt_df_no = relt_df[relt_df['relational_score'] == 0.0]\n",
    "    relt_df = pd.concat([relt_df_yes, relt_df_no])\n",
    "    \n",
    "    demog_df_yes = demog_df[demog_df['demographic_score'] == 1.0]\n",
    "    demog_df_no = demog_df[demog_df['demographic_score'] == 0.0]\n",
    "    demog_df = pd.concat([demog_df_yes, demog_df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_article_tokens(article, return_string=False):\n",
    "    '''\n",
    "    Collects words from already-tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "        return_string: whether to return single, long string representing article\n",
    "    Returns:\n",
    "        tokens: string if return_string, else list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = [] # initialize\n",
    "    \n",
    "    if return_string:\n",
    "        for sent in article:\n",
    "            sent = ' '.join(sent) # make sentence into a string\n",
    "            tokens.append(sent) # add sentence to list of sentences\n",
    "        tokens = ' '.join(tokens) # join sentences into string\n",
    "        return tokens # return string\n",
    "    \n",
    "    else:\n",
    "        for sent in article:\n",
    "            tokens += [word for word in sent] # add each word to list of tokens\n",
    "        return tokens # return list of tokens\n",
    "\n",
    "# For capturing word frequencies, add all words from each article to single, shared list (can't use this to create models)\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 83116\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('oasis', 40297),\n",
       " ('from', 36256),\n",
       " ('entry', 34917),\n",
       " ('we', 31897),\n",
       " ('social', 30067),\n",
       " ('have', 27000),\n",
       " ('which', 26867),\n",
       " ('more', 25646),\n",
       " ('were', 21277),\n",
       " ('char', 20564),\n",
       " ('new', 20161),\n",
       " ('one', 19987),\n",
       " ('rowsep', 18993),\n",
       " ('colsep', 18958),\n",
       " ('other', 18714),\n",
       " ('between', 17821),\n",
       " ('than', 17793),\n",
       " ('can', 17436),\n",
       " ('has', 17067),\n",
       " ('organizational', 16700)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = (cult_tokens + relt_tokens) + demog_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 471679\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('valign bottom oasis entry oasis entry colname colsep rowsep align char char',\n",
       "  3751),\n",
       " ('pp', 1419),\n",
       " ('oasis entry oasis entry colname colsep rowsep align char char', 1271),\n",
       " ('oasis entry colname colsep rowsep align char char', 1077),\n",
       " ('american sociological review', 1041),\n",
       " ('american journal sociology', 1006),\n",
       " ('administrative science quarterly', 947),\n",
       " ('valign bottom oasis entry colname colsep rowsep align char char', 938),\n",
       " ('sci', 491),\n",
       " ('chicago university chicago press', 463),\n",
       " ('academy management journal', 458),\n",
       " ('academy management review', 387),\n",
       " ('organ', 366),\n",
       " ('colsep rowsep oasis entry align char char', 360),\n",
       " ('new york free press', 356),\n",
       " ('new york oxford university press', 317),\n",
       " ('oasis entry oasis entry colsep rowsep align char char', 296),\n",
       " ('cambridge ma harvard university press', 291),\n",
       " ('ed', 285),\n",
       " ('cambridge cambridge university press', 271)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(lambda article: cult_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(lambda article: relt_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(lambda article: demog_sents.extend([' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = (cult_sents + relt_sents) + demog_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and apply text vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect article tokens into a list of strings, each string an article\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.append(collect_article_tokens(article, return_string = True)))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.append(collect_article_tokens(article, return_string = True)))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.append(collect_article_tokens(article, return_string = True)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 65619\n",
      "['0000779e2340', '37i', 'abortive', 'agnes', 'anne', 'asean', 'bagdad', 'besier', 'bouma', 'c06', 'centrating', 'classt', 'competidiversity', 'contrived', 'crush', 'decouple', 'develple', 'dissolves', 'dwarfs', 'emigre', 'eschewing', 'extertion', 'finalised', 'franchised', 'gegenstandstheorie', 'grande', 'harden', 'historiques', 'ice', 'incredibly', 'intentioned', 'itching', 'kalra', 'kryukov', 'lenders', 'looping', 'manitoba', 'meetproactive', 'misunderstood', 'munford', 'newl', 'nr170', 'opener', 'oç', 'peptide', 'plo', 'preferable', 'provinces', 'ranged', 'regime', 'resulting', 'routed', 'schachter', 'serves', 'sist', 'specialists', 'straatweg', 'surmised', 'ted', 'tinbergen', 'tribesmen', 'underagain', 'usefully', 'vociferously', 'wiki', 'yoshida']\n",
      "\n",
      "Number of features in relational vectorizer: 67106\n",
      "['00000000000000000000000000000000z000', '1979c', '64cic', 'acrylic', 'alitomatic', 'apopriate', 'atrrmon', 'barton', 'biostatistics', 'bristolian', 'capitalisms', 'cheerful', 'coercing', 'confidante', 'costner', 'd2f2', 'demonstr', 'dipartimento', 'donesky', 'eere', 'enshrined', 'ewe', 'favourably', 'foils', 'g122', 'glickman', 'guy', 'herculaneum', 'hubei', 'immigrate', 'ini', 'inventiveness', 'jjj', 'kilmann', 'langer', 'lineaments', 'lystra', 'master', 'mff', 'monin', 'nanay', 'noinreferrals', 'observant', 'oooooooooooooooolololooooooooxoooooo', 'overlordship', 'pdsf36', 'pittance', 'pppspssspsp', 'promotions', 'qxd', 'redolent', 'researches', 'rogan', 'santrock', 'selfregulatory', 'siellawa', 'solidly', 'stds', 'sudberry', 'tami', 'thinkability', 'trailing', 'udel', 'unremarked', 'vibe', 'weldon', 'x3', 'îii']\n",
      "\n",
      "Number of features in demographic vectorizer: 64859\n",
      "['000000000000o', '2ro', '9s', 'affectedfrom', 'analytical', 'armen', 'axt', 'bendix', 'bonham', 'bushway', 'cdpit', 'cjttcnnomnor', 'companies', 'contradict', 'cruder', 'ded', 'devising', 'distress', 'débat', 'empts', 'eth', 'fade', 'flags', 'fruitful', 'gestión', 'growth', 'heavier', 'hopwood', 'ilms', 'informa', 'intrinsically', 'jg', 'kinase', 'lapsing', 'lineback', 'm8', 'mating', 'microrationalist', 'moratorium', 'native', 'nonfulfilment', 'oeconomic', 'oscillating', 'partiality', 'philosophia', 'positive', 'proclamation', 'quadrant', 'reconsolidation', 'reproach', 'robertson', 'sanfey', 'seligson', 'significance', 'sonnel', 'sternly', 'sungbaik', 'tattered', 'thursby', 'transtemporal', 'unanswered', 'upperclass', 'vison', 'wicca1', 'yat']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Uses TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "cult_vectorizer = joblib.load(cult_vec_fp, \"r+\")\n",
    "X_cult = cult_vectorizer.fit_transform(cult_tokens)\n",
    "print('Number of features in cultural vectorizer:', len(cult_vectorizer.get_feature_names()))\n",
    "print(cult_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "relt_vectorizer = joblib.load(relt_vec_fp, \"r+\")\n",
    "X_relt = relt_vectorizer.fit_transform(relt_tokens)\n",
    "print('Number of features in relational vectorizer:', len(relt_vectorizer.get_feature_names()))\n",
    "print(relt_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "demog_vectorizer = joblib.load(demog_vec_fp, \"r+\")\n",
    "X_demog = demog_vectorizer.fit_transform(demog_tokens)\n",
    "print('Number of features in demographic vectorizer:', len(demog_vectorizer.get_feature_names()))\n",
    "print(demog_vectorizer.get_feature_names()[::1000]) # get every 1000th word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Balance x_train, y_train\n",
    "######################################################\n",
    "\n",
    "def resample_data(X_train, Y_train, undersample = False, sampling_ratio = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train: X training data\n",
    "        Y_train: Y training data\n",
    "        undersample: boolean for over or undersampling\n",
    "        sampling_ratio: ratio of minority to majority class\n",
    "        \n",
    "        archived/not used:\n",
    "        sampling_strategy: strategy for resampled distribution\n",
    "            if oversample: 'majority' makes minority = to majority\n",
    "            if undersample: 'minority' makes majority = to minority\n",
    "            \n",
    "    Returns:\n",
    "        X_balanced: predictors at balanced ratio\n",
    "        Y_balanced: outcomes at balanced ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = undersample.fit_resample(X_train, Y_train)\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = oversample.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    print(f'Y_train: {Counter(Y_train)}\\nY_resample: {Counter(Y_balanced)}')\n",
    "    \n",
    "    return X_balanced, Y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# k-fold cross validation for model evaluation\n",
    "######################################################\n",
    "\n",
    "# Define test options for k-fold CV\n",
    "num_folds = 10 \n",
    "seed = 3\n",
    "scoring='f1_weighted' # set scoring metric (not used here)\n",
    "\n",
    "def show_kfold_output(models, \n",
    "                      X, \n",
    "                      Y, \n",
    "                      num_folds = num_folds, \n",
    "                      random_state = seed, \n",
    "                      shuffle = True):\n",
    "    '''\n",
    "    Estimates the accuracy of different model algorithms, adds results to a results array and returns.\n",
    "    Prints the accuracy results: averages and std.\n",
    "    Uses cross_val_predict, which unlike cross_val_score cannot define scoring option/evaluation metric.\n",
    "    \n",
    "    Args:\n",
    "        models: list of (name, model) tuples\n",
    "        X: predictors\n",
    "        Y: outcomes\n",
    "        num_folds: Split data randomly into num_folds parts: (num_folds-1) for training, 1 for scoring\n",
    "        random_state: seed\n",
    "        shuffle: \n",
    "    \n",
    "    Returns:\n",
    "        results: list of model results\n",
    "        names: list of model names (matches results)\n",
    "        \n",
    "    Source: \n",
    "        https://stackoverflow.com/questions/40057049/using-confusion-matrix-as-scoring-metric-in-cross-validation-in-scikit-learn\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Setup model options\n",
    "        kfold = KFold(\n",
    "            n_splits=num_folds, \n",
    "            random_state=seed, \n",
    "            shuffle=True)\n",
    "        \n",
    "        # Get kfold results\n",
    "        cv_results = cross_val_predict(\n",
    "            model, \n",
    "            X, \n",
    "            Y, \n",
    "            cv=kfold, \n",
    "            #scoring=scoring, \n",
    "            n_jobs=-1) # use all cores = faster\n",
    "        \n",
    "        # Add results and name of each algorithm to the model array\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'{name}:\\nMean (std):\\t {round(cv_results.mean(),4)} ({round(cv_results.std(),4)})')\n",
    "        print('Accuracy:\\t', accuracy_score(Y_balanced, cv_results))\n",
    "        print()\n",
    "        print('Confusion matrix:\\n', confusion_matrix(Y_balanced, cv_results))\n",
    "        print()\n",
    "        print('Report:\\n', classification_report(Y_balanced, cv_results))\n",
    "        print()\n",
    "        \n",
    "    # Return arrays\n",
    "    return results, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 314\n",
      "Number of codes (should match): 314\n",
      "Y_train Distribution: [(0.0, 170), (1.0, 81)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "print(\"Number of cases:\", str(X_cult.shape[0]))\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    test_size=test_size, \n",
    "    random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 170, 1.0: 81})\n",
      "Y_resample: Counter({0.0: 170, 1.0: 170})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "# Use these settings here and below\n",
    "sampling_ratio = 1.0 # ratio of minority to majority cases\n",
    "undersample = False # whether to undersample or oversample\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Mean (std):\t 0.6176 (0.486)\n",
      "Accuracy:\t 0.8176470588235294\n",
      "\n",
      "Confusion matrix:\n",
      " [[119  51]\n",
      " [ 11 159]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.70      0.79       170\n",
      "         1.0       0.76      0.94      0.84       170\n",
      "\n",
      "    accuracy                           0.82       340\n",
      "   macro avg       0.84      0.82      0.82       340\n",
      "weighted avg       0.84      0.82      0.82       340\n",
      "\n",
      "\n",
      "RF:\n",
      "Mean (std):\t 0.4912 (0.4999)\n",
      "Accuracy:\t 0.9382352941176471\n",
      "\n",
      "Confusion matrix:\n",
      " [[161   9]\n",
      " [ 12 158]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.95      0.94       170\n",
      "         1.0       0.95      0.93      0.94       170\n",
      "\n",
      "    accuracy                           0.94       340\n",
      "   macro avg       0.94      0.94      0.94       340\n",
      "weighted avg       0.94      0.94      0.94       340\n",
      "\n",
      "\n",
      "DT:\n",
      "Mean (std):\t 0.5471 (0.4978)\n",
      "Accuracy:\t 0.8941176470588236\n",
      "\n",
      "Confusion matrix:\n",
      " [[144  26]\n",
      " [ 10 160]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.85      0.89       170\n",
      "         1.0       0.86      0.94      0.90       170\n",
      "\n",
      "    accuracy                           0.89       340\n",
      "   macro avg       0.90      0.89      0.89       340\n",
      "weighted avg       0.90      0.89      0.89       340\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 170), (1.0, 81)]\n",
      "0.7619047619047619\n",
      "[[29 10]\n",
      " [ 5 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.74      0.79        39\n",
      "         1.0       0.66      0.79      0.72        24\n",
      "\n",
      "    accuracy                           0.76        63\n",
      "   macro avg       0.75      0.77      0.76        63\n",
      "weighted avg       0.78      0.76      0.76        63\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 170), (1.0, 170)]\n",
      "0.6190476190476191\n",
      "[[18 21]\n",
      " [ 3 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.46      0.60        39\n",
      "         1.0       0.50      0.88      0.64        24\n",
      "\n",
      "    accuracy                           0.62        63\n",
      "   macro avg       0.68      0.67      0.62        63\n",
      "weighted avg       0.72      0.62      0.61        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_cult = KNeighborsClassifier()\n",
    "knn_cult.fit(X_train, Y_train)\n",
    "knn_predictions = knn_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_cult = KNeighborsClassifier()\n",
    "knn_cult.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 170), (1.0, 81)]\n",
      "0.7777777777777778\n",
      "[[38  1]\n",
      " [13 11]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.97      0.84        39\n",
      "         1.0       0.92      0.46      0.61        24\n",
      "\n",
      "    accuracy                           0.78        63\n",
      "   macro avg       0.83      0.72      0.73        63\n",
      "weighted avg       0.81      0.78      0.76        63\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 170), (1.0, 170)]\n",
      "0.7619047619047619\n",
      "[[35  4]\n",
      " [11 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.90      0.82        39\n",
      "         1.0       0.76      0.54      0.63        24\n",
      "\n",
      "    accuracy                           0.76        63\n",
      "   macro avg       0.76      0.72      0.73        63\n",
      "weighted avg       0.76      0.76      0.75        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_cult = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_cult.fit(X_train, Y_train) \n",
    "rf_predictions = rf_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "rf_cult = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_cult.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 170), (1.0, 81)]\n",
      "0.7142857142857143\n",
      "[[28 11]\n",
      " [ 7 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.72      0.76        39\n",
      "         1.0       0.61      0.71      0.65        24\n",
      "\n",
      "    accuracy                           0.71        63\n",
      "   macro avg       0.70      0.71      0.71        63\n",
      "weighted avg       0.73      0.71      0.72        63\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 170), (1.0, 170)]\n",
      "0.6666666666666666\n",
      "[[26 13]\n",
      " [ 8 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.67      0.71        39\n",
      "         1.0       0.55      0.67      0.60        24\n",
      "\n",
      "    accuracy                           0.67        63\n",
      "   macro avg       0.66      0.67      0.66        63\n",
      "weighted avg       0.68      0.67      0.67        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_cult = DecisionTreeClassifier()\n",
    "dt_cult.fit(X_train, Y_train)\n",
    "dt_predictions = dt_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "dt_cult = DecisionTreeClassifier()\n",
    "dt_cult.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_cult.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_cult_120220.joblib']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#joblib.dump(rf_cult, cult_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 321\n",
      "Number of codes (should match): 321\n",
      "Y_train Distribution: [(0.0, 185), (1.0, 71)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "relt_df = relt_df[['text', 'relational_score']]\n",
    "print(\"Number of cases:\", str(X_relt.shape[0]))\n",
    "\n",
    "valueArray = relt_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_relt, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 185, 1.0: 71})\n",
      "Y_resample: Counter({0.0: 185, 1.0: 185})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_train, Y_train, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Mean (std):\t 0.5811 (0.4934)\n",
      "Accuracy:\t 0.8756756756756757\n",
      "\n",
      "Confusion matrix:\n",
      " [[147  38]\n",
      " [  8 177]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.79      0.86       185\n",
      "         1.0       0.82      0.96      0.89       185\n",
      "\n",
      "    accuracy                           0.88       370\n",
      "   macro avg       0.89      0.88      0.87       370\n",
      "weighted avg       0.89      0.88      0.87       370\n",
      "\n",
      "\n",
      "RF:\n",
      "Mean (std):\t 0.4919 (0.4999)\n",
      "Accuracy:\t 0.981081081081081\n",
      "\n",
      "Confusion matrix:\n",
      " [[183   2]\n",
      " [  5 180]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       185\n",
      "         1.0       0.99      0.97      0.98       185\n",
      "\n",
      "    accuracy                           0.98       370\n",
      "   macro avg       0.98      0.98      0.98       370\n",
      "weighted avg       0.98      0.98      0.98       370\n",
      "\n",
      "\n",
      "DT:\n",
      "Mean (std):\t 0.5703 (0.495)\n",
      "Accuracy:\t 0.9243243243243243\n",
      "\n",
      "Confusion matrix:\n",
      " [[158  27]\n",
      " [  1 184]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.85      0.92       185\n",
      "         1.0       0.87      0.99      0.93       185\n",
      "\n",
      "    accuracy                           0.92       370\n",
      "   macro avg       0.93      0.92      0.92       370\n",
      "weighted avg       0.93      0.92      0.92       370\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 185), (1.0, 71)]\n",
      "0.8461538461538461\n",
      "[[38  6]\n",
      " [ 4 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.86      0.88        44\n",
      "         1.0       0.74      0.81      0.77        21\n",
      "\n",
      "    accuracy                           0.85        65\n",
      "   macro avg       0.82      0.84      0.83        65\n",
      "weighted avg       0.85      0.85      0.85        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 185), (1.0, 185)]\n",
      "0.7692307692307693\n",
      "[[31 13]\n",
      " [ 2 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.70      0.81        44\n",
      "         1.0       0.59      0.90      0.72        21\n",
      "\n",
      "    accuracy                           0.77        65\n",
      "   macro avg       0.77      0.80      0.76        65\n",
      "weighted avg       0.83      0.77      0.78        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "knn_relt = KNeighborsClassifier()\n",
    "knn_relt.fit(X_train, Y_train)\n",
    "knn_predictions = knn_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "# Make predictions on validation dataset\n",
    "knn_relt = KNeighborsClassifier()\n",
    "knn_relt.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 185), (1.0, 71)]\n",
      "0.8923076923076924\n",
      "[[44  0]\n",
      " [ 7 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.93        44\n",
      "         1.0       1.00      0.67      0.80        21\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.93      0.83      0.86        65\n",
      "weighted avg       0.91      0.89      0.89        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 185), (1.0, 185)]\n",
      "0.8923076923076924\n",
      "[[44  0]\n",
      " [ 7 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.93        44\n",
      "         1.0       1.00      0.67      0.80        21\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.93      0.83      0.86        65\n",
      "weighted avg       0.91      0.89      0.89        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_relt = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_relt.fit(X_train, Y_train) \n",
    "rf_predictions = rf_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "rf_relt = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_relt.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 185), (1.0, 71)]\n",
      "0.8923076923076924\n",
      "[[42  2]\n",
      " [ 5 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.95      0.92        44\n",
      "         1.0       0.89      0.76      0.82        21\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.89      0.86      0.87        65\n",
      "weighted avg       0.89      0.89      0.89        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(0.0, 185), (1.0, 185)]\n",
      "0.8307692307692308\n",
      "[[40  4]\n",
      " [ 7 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.91      0.88        44\n",
      "         1.0       0.78      0.67      0.72        21\n",
      "\n",
      "    accuracy                           0.83        65\n",
      "   macro avg       0.81      0.79      0.80        65\n",
      "weighted avg       0.83      0.83      0.83        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_relt = DecisionTreeClassifier()\n",
    "dt_relt.fit(X_train, Y_train)\n",
    "dt_predictions = dt_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "\n",
    "dt_relt = DecisionTreeClassifier()\n",
    "dt_relt.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_relt.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_relt_120220.joblib']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#joblib.dump(rf_relt, relt_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 325\n",
      "Number of codes (should match): 325\n",
      "Y_train Distribution: [(0.0, 200), (1.0, 60)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First reltove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "demog_df = demog_df[['text', 'demographic_score']]\n",
    "print(\"Number of cases:\", str(X_demog.shape[0]))\n",
    "\n",
    "valueArray = demog_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_demog, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 200, 1.0: 60})\n",
      "Y_resample: Counter({1.0: 200, 0.0: 200})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_train, Y_train, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross Validation: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Mean (std):\t 0.665 (0.472)\n",
      "Accuracy:\t 0.835\n",
      "\n",
      "Confusion matrix:\n",
      " [[134  66]\n",
      " [  0 200]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       200\n",
      "         1.0       0.75      1.00      0.86       200\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.88      0.83      0.83       400\n",
      "weighted avg       0.88      0.83      0.83       400\n",
      "\n",
      "\n",
      "RF:\n",
      "Mean (std):\t 0.505 (0.5)\n",
      "Accuracy:\t 0.995\n",
      "\n",
      "Confusion matrix:\n",
      " [[198   2]\n",
      " [  0 200]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       200\n",
      "         1.0       0.99      1.00      1.00       200\n",
      "\n",
      "    accuracy                           0.99       400\n",
      "   macro avg       1.00      0.99      0.99       400\n",
      "weighted avg       1.00      0.99      0.99       400\n",
      "\n",
      "\n",
      "DT:\n",
      "Mean (std):\t 0.555 (0.497)\n",
      "Accuracy:\t 0.94\n",
      "\n",
      "Confusion matrix:\n",
      " [[177  23]\n",
      " [  1 199]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94       200\n",
      "         1.0       0.90      0.99      0.94       200\n",
      "\n",
      "    accuracy                           0.94       400\n",
      "   macro avg       0.95      0.94      0.94       400\n",
      "weighted avg       0.95      0.94      0.94       400\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('RF', RandomForestClassifier(n_estimators=1000, random_state=0)))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 200), (1.0, 60)]\n",
      "0.9076923076923077\n",
      "[[43  5]\n",
      " [ 1 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.90      0.93        48\n",
      "         1.0       0.76      0.94      0.84        17\n",
      "\n",
      "    accuracy                           0.91        65\n",
      "   macro avg       0.87      0.92      0.89        65\n",
      "weighted avg       0.92      0.91      0.91        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(1.0, 200), (0.0, 200)]\n",
      "0.7076923076923077\n",
      "[[30 18]\n",
      " [ 1 16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.62      0.76        48\n",
      "         1.0       0.47      0.94      0.63        17\n",
      "\n",
      "    accuracy                           0.71        65\n",
      "   macro avg       0.72      0.78      0.69        65\n",
      "weighted avg       0.84      0.71      0.72        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "# from sklearn.ensemble import BaggingClassifier # improves estimates but hard with so little data\n",
    "#knn_demog = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "knn_demog = KNeighborsClassifier()\n",
    "knn_demog.fit(X_train, Y_train)\n",
    "knn_predictions = knn_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: KNN\n",
    "######################################################\n",
    "\n",
    "# Make predictions on validation dataset\n",
    "# from sklearn.ensemble import BaggingClassifier # improves estimates but hard with so little data\n",
    "#knn_demog = BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "knn_demog = KNeighborsClassifier()\n",
    "knn_demog.fit(X_balanced, Y_balanced)\n",
    "knn_predictions = knn_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, knn_predictions))\n",
    "print(confusion_matrix(Y_validate, knn_predictions))\n",
    "print(classification_report(Y_validate, knn_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 200), (1.0, 60)]\n",
      "0.8923076923076924\n",
      "[[48  0]\n",
      " [ 7 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        48\n",
      "         1.0       1.00      0.59      0.74        17\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.94      0.79      0.84        65\n",
      "weighted avg       0.91      0.89      0.88        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(1.0, 200), (0.0, 200)]\n",
      "0.9230769230769231\n",
      "[[47  1]\n",
      " [ 4 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.98      0.95        48\n",
      "         1.0       0.93      0.76      0.84        17\n",
      "\n",
      "    accuracy                           0.92        65\n",
      "   macro avg       0.93      0.87      0.89        65\n",
      "weighted avg       0.92      0.92      0.92        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "#rf_demog = BaggingClassifier(RandomForestClassifier(n_estimators=1000, random_state=0), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "rf_demog = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_demog.fit(X_train, Y_train) \n",
    "rf_predictions = rf_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Random Forest\n",
    "######################################################\n",
    "\n",
    "#rf_demog = BaggingClassifier(RandomForestClassifier(n_estimators=1000, random_state=0), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "rf_demog = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_demog.fit(X_balanced, Y_balanced) \n",
    "rf_predictions = rf_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, rf_predictions))\n",
    "print(confusion_matrix(Y_validate, rf_predictions))\n",
    "print(classification_report(Y_validate, rf_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unbalanced Classifier [(0.0, 200), (1.0, 60)]\n",
      "0.8615384615384616\n",
      "[[44  4]\n",
      " [ 5 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.92      0.91        48\n",
      "         1.0       0.75      0.71      0.73        17\n",
      "\n",
      "    accuracy                           0.86        65\n",
      "   macro avg       0.82      0.81      0.82        65\n",
      "weighted avg       0.86      0.86      0.86        65\n",
      "\n",
      "\n",
      "Balanced Classifier [(1.0, 200), (0.0, 200)]\n",
      "0.8923076923076924\n",
      "[[44  4]\n",
      " [ 3 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.92      0.93        48\n",
      "         1.0       0.78      0.82      0.80        17\n",
      "\n",
      "    accuracy                           0.89        65\n",
      "   macro avg       0.86      0.87      0.86        65\n",
      "weighted avg       0.89      0.89      0.89        65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "#dt_demog = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "dt_demog = DecisionTreeClassifier()\n",
    "dt_demog.fit(X_train, Y_train)\n",
    "dt_predictions = dt_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Unbalanced Classifier {Counter(Y_train).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))\n",
    "\n",
    "######################################################\n",
    "# Balanced: Compare algorithms on validation test: Decision Tree\n",
    "######################################################\n",
    "#dt_demog = BaggingClassifier(DecisionTreeClassifier(), max_samples=0.5, max_features=0.5)\n",
    "\n",
    "dt_demog = DecisionTreeClassifier()\n",
    "dt_demog.fit(X_balanced, Y_balanced)\n",
    "dt_predictions = dt_demog.predict(X_validate)\n",
    "\n",
    "print()\n",
    "print(f'Balanced Classifier {Counter(Y_balanced).most_common()}')\n",
    "print(accuracy_score(Y_validate, dt_predictions))\n",
    "print(confusion_matrix(Y_validate, dt_predictions))\n",
    "print(classification_report(Y_validate, dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_demog_120220.joblib']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "joblib.dump(rf_demog, demog_model_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17f6b799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key savefig.frameon in file C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 421 ('savefig.frameon : True')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.level in file C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 472 ('verbose.level  : silent      # one of silent, helpful, debug, debug-annoying')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "\n",
      "Bad key verbose.fileo in file C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle, line 473 ('verbose.fileo  : sys.stdout  # a log filename, sys.stdout or sys.stderr')\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.3.1/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\theth\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import regex as re\n",
    "\n",
    "\n",
    "import os\n",
    "import random as rand\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from clean_text import stopwords_make, punctstr_make, unicode_make, apache_tokenize, clean_sentence_apache \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import sys, argparse\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adf8e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_test_data(path):\n",
    "    return open(path, 'rb')\n",
    "with open_test_data('/home/jovyan/work/tlu_storage/training_cultural_preprocessed_100321.pkl') as f:\n",
    "    cult = pickle.load(f)\n",
    "\n",
    "# with open_test_data('/home/jovyan/work/tlu_storage/training_demographic_preprocessed_100321.pkl') as f:\n",
    "#     demog = pickle.load(f)\n",
    "\n",
    "# with open_test_data('/home/jovyan/work/tlu_storage/training_orgs_preprocessed_100321.pkl') as f:\n",
    "#     orgs = pickle.load(f)\n",
    "\n",
    "# with open_test_data('/home/jovyan/work/tlu_storage/training_relational_preprocessed_100321.pkl') as f:\n",
    "#     rela = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e737d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "full_text = []\n",
    "\n",
    "for i in cult['text']:\n",
    "    joined = list(itertools.chain(*i))\n",
    "    full_text.append(\" \".join(joined))\n",
    "\n",
    "\n",
    "# ' '.join(cult['text'][0][0])\n",
    "\n",
    "# full_text_demog = []\n",
    "# for i in demog['text']:\n",
    "#     joined = list(itertools.chain(*i))\n",
    "#     full_text_demog.append(\" \".join(joined))\n",
    "\n",
    "\n",
    "# full_text_orgs = []\n",
    "# for j in orgs['text']:\n",
    "#     joined = list(itertools.chain(*j))\n",
    "#     full_text_orgs.append(\" \".join(joined))\n",
    "\n",
    "\n",
    "# full_text_rela = []\n",
    "# for j in rela['text']:\n",
    "#     joined = list(itertools.chain(*j))\n",
    "#     full_text_rela.append(\" \".join(joined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24127979",
   "metadata": {},
   "outputs": [],
   "source": [
    "cult['full_text'] = full_text\n",
    "# demog['full_text'] = full_text_demog\n",
    "# orgs['full_text'] = full_text_orgs\n",
    "# rela['full_text'] = full_text_rela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0aa649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(article):\n",
    "    article = re.sub('<plain_text> <page sequence=\"1\">', '', article)\n",
    "    article = re.sub(r'</page>(\\<.*?\\>)', ' \\n ', article)\n",
    "    # xml tags\n",
    "    article = re.sub(r'<.*?>', '', article)\n",
    "    article = re.sub(r'<body.*\\n\\s*.*\\s*.*>', '', article)\n",
    "    return article\n",
    "\n",
    "tags_removed = [remove_tags(art) for art in cult['full_text']]\n",
    "# tags_removed_demog = [remove_tags(art) for art in demog['full_text']]\n",
    "# tags_removed_org = [remove_tags(art) for art in orgs['full_text']]\n",
    "# tags_removed_rela = [remove_tags(art) for art in rela['full_text']]\n",
    "cult['text_no_tags'] = tags_removed\n",
    "# demog['text_no_tags'] = tags_removed_demog\n",
    "# orgs['text_no_tags'] = tags_removed_org\n",
    "# rela['text_no_tags'] = tags_removed_rela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c31e1927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cultural_score</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>article_name</th>\n",
       "      <th>full_text</th>\n",
       "      <th>text_no_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[Where, Do, Interorganizational, Networks, Co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_210179</td>\n",
       "      <td>Where Do Interorganizational Networks Come From?</td>\n",
       "      <td>Where Do Interorganizational Networks Come Fro...</td>\n",
       "      <td>Where Do Interorganizational Networks Come Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[Civil, Rights, Law, at, Work:, Sex, Discrimi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_210317</td>\n",
       "      <td>Civil Rights Law at Work: Sex Discrimination a...</td>\n",
       "      <td>Civil Rights Law at Work: Sex Discrimination a...</td>\n",
       "      <td>Civil Rights Law at Work: Sex Discrimination a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[Between, Markets, and, Politics:, Organizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_231084</td>\n",
       "      <td>Between Markets and Politics: Organizational R...</td>\n",
       "      <td>Between Markets and Politics: Organizational R...</td>\n",
       "      <td>Between Markets and Politics: Organizational R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[World, Society, and, the, Nation-State, John...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_231174</td>\n",
       "      <td>World Society and the Nation‐State</td>\n",
       "      <td>World Society and the Nation-State John Meyer ...</td>\n",
       "      <td>World Society and the Nation-State John Meyer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[&lt;body, xmlns:xlink=\"http://www..org//xlink\"]...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.1086_382347</td>\n",
       "      <td>Kinship Networks and Entrepreneurs in China’s ...</td>\n",
       "      <td>&lt;body xmlns:xlink=\"http://www..org//xlink\" xml...</td>\n",
       "      <td>Introduction Economists have long concurred t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>[[Institutionalized, Organizations:, Formal, S...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.2307_2778293</td>\n",
       "      <td>Institutionalized Organizations: Formal Struct...</td>\n",
       "      <td>Institutionalized Organizations: Formal Struct...</td>\n",
       "      <td>Institutionalized Organizations: Formal Struct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>[[The, Social, Construction, of, Organizationa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Management &amp; Organizational Behavior</td>\n",
       "      <td>10.2307_2667051</td>\n",
       "      <td>The Social Construction of Organizational Know...</td>\n",
       "      <td>The Social Construction of Organizational Know...</td>\n",
       "      <td>The Social Construction of Organizational Know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>[[Organizational, Structure, and, the, Institu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Management &amp; Organizational Behavior</td>\n",
       "      <td>10.2307_2392303</td>\n",
       "      <td>Organizational Structure and the Institutional...</td>\n",
       "      <td>Organizational Structure and the Institutional...</td>\n",
       "      <td>Organizational Structure and the Institutional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>[[Institutional, Sources, of, Change, in, the,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Management &amp; Organizational Behavior</td>\n",
       "      <td>10.2307_2392383</td>\n",
       "      <td>Institutional Sources of Change in the Formal ...</td>\n",
       "      <td>Institutional Sources of Change in the Formal ...</td>\n",
       "      <td>Institutional Sources of Change in the Formal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>[[THE, ROLE, OF, INSTITUTIONALIZATION, IN, CUL...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>10.2307_2094862</td>\n",
       "      <td>The Role of Institutionalization in Cultural P...</td>\n",
       "      <td>THE ROLE OF INSTITUTIONALIZATION IN CULTURAL P...</td>\n",
       "      <td>THE ROLE OF INSTITUTIONALIZATION IN CULTURAL P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>709 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  cultural_score  \\\n",
       "0    [[Where, Do, Interorganizational, Networks, Co...             0.0   \n",
       "1    [[Civil, Rights, Law, at, Work:, Sex, Discrimi...             1.0   \n",
       "2    [[Between, Markets, and, Politics:, Organizati...             0.0   \n",
       "3    [[World, Society, and, the, Nation-State, John...             1.0   \n",
       "4    [[<body, xmlns:xlink=\"http://www..org//xlink\"]...             1.0   \n",
       "..                                                 ...             ...   \n",
       "914  [[Institutionalized, Organizations:, Formal, S...             1.0   \n",
       "915  [[The, Social, Construction, of, Organizationa...             1.0   \n",
       "916  [[Organizational, Structure, and, the, Institu...             1.0   \n",
       "917  [[Institutional, Sources, of, Change, in, the,...             1.0   \n",
       "918  [[THE, ROLE, OF, INSTITUTIONALIZATION, IN, CUL...             1.0   \n",
       "\n",
       "                          primary_subject  edited_filename  \\\n",
       "0                               Sociology   10.1086_210179   \n",
       "1                               Sociology   10.1086_210317   \n",
       "2                               Sociology   10.1086_231084   \n",
       "3                               Sociology   10.1086_231174   \n",
       "4                               Sociology   10.1086_382347   \n",
       "..                                    ...              ...   \n",
       "914                             Sociology  10.2307_2778293   \n",
       "915  Management & Organizational Behavior  10.2307_2667051   \n",
       "916  Management & Organizational Behavior  10.2307_2392303   \n",
       "917  Management & Organizational Behavior  10.2307_2392383   \n",
       "918                             Sociology  10.2307_2094862   \n",
       "\n",
       "                                          article_name  \\\n",
       "0     Where Do Interorganizational Networks Come From?   \n",
       "1    Civil Rights Law at Work: Sex Discrimination a...   \n",
       "2    Between Markets and Politics: Organizational R...   \n",
       "3                   World Society and the Nation‐State   \n",
       "4    Kinship Networks and Entrepreneurs in China’s ...   \n",
       "..                                                 ...   \n",
       "914  Institutionalized Organizations: Formal Struct...   \n",
       "915  The Social Construction of Organizational Know...   \n",
       "916  Organizational Structure and the Institutional...   \n",
       "917  Institutional Sources of Change in the Formal ...   \n",
       "918  The Role of Institutionalization in Cultural P...   \n",
       "\n",
       "                                             full_text  \\\n",
       "0    Where Do Interorganizational Networks Come Fro...   \n",
       "1    Civil Rights Law at Work: Sex Discrimination a...   \n",
       "2    Between Markets and Politics: Organizational R...   \n",
       "3    World Society and the Nation-State John Meyer ...   \n",
       "4    <body xmlns:xlink=\"http://www..org//xlink\" xml...   \n",
       "..                                                 ...   \n",
       "914  Institutionalized Organizations: Formal Struct...   \n",
       "915  The Social Construction of Organizational Know...   \n",
       "916  Organizational Structure and the Institutional...   \n",
       "917  Institutional Sources of Change in the Formal ...   \n",
       "918  THE ROLE OF INSTITUTIONALIZATION IN CULTURAL P...   \n",
       "\n",
       "                                          text_no_tags  \n",
       "0    Where Do Interorganizational Networks Come Fro...  \n",
       "1    Civil Rights Law at Work: Sex Discrimination a...  \n",
       "2    Between Markets and Politics: Organizational R...  \n",
       "3    World Society and the Nation-State John Meyer ...  \n",
       "4     Introduction Economists have long concurred t...  \n",
       "..                                                 ...  \n",
       "914  Institutionalized Organizations: Formal Struct...  \n",
       "915  The Social Construction of Organizational Know...  \n",
       "916  Organizational Structure and the Institutional...  \n",
       "917  Institutional Sources of Change in the Formal ...  \n",
       "918  THE ROLE OF INSTITUTIONALIZATION IN CULTURAL P...  \n",
       "\n",
       "[709 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove cult_label that are 0.5\n",
    "\n",
    "cult = cult[cult['cultural_score']!=0.5]\n",
    "cult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4baaf791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1070 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41faef3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        if 'dropout' in params:\n",
    "            self.dropout = nn.Dropout(p=params['dropout'])\n",
    "        else:\n",
    "            self.dropout = None\n",
    "            \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False, do_basic_tokenize=False)\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        self.num_labels = params[\"label_length\"]\n",
    "\n",
    "        self.fc = nn.Linear(768, self.num_labels)\n",
    "\n",
    "    def get_batches(self, all_x, all_y, batch_size=10):\n",
    "\n",
    "        \"\"\" Get batches for input x, y data, with data tokenized according to the BERT tokenizer \n",
    "        (and limited to a maximum number of WordPiece tokens \"\"\"\n",
    "\n",
    "        batches_x=[]\n",
    "        batches_y=[]\n",
    "\n",
    "        for i in range(0, len(all_x), batch_size):\n",
    "\n",
    "            current_batch=[]\n",
    "\n",
    "            x=all_x[i:i+batch_size]\n",
    "\n",
    "            batch_x = self.tokenizer(x, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            batch_y=all_y[i:i+batch_size]\n",
    "\n",
    "            batches_x.append(batch_x.to(device))\n",
    "            batches_y.append(torch.LongTensor(batch_y).to(device))\n",
    "\n",
    "        return batches_x, batches_y\n",
    "\n",
    "\n",
    "    def forward(self, batch_x): \n",
    "\n",
    "        bert_output = self.bert(input_ids=batch_x[\"input_ids\"],\n",
    "                         attention_mask=batch_x[\"attention_mask\"],\n",
    "                         token_type_ids=batch_x[\"token_type_ids\"],\n",
    "                         output_hidden_states=True)\n",
    "\n",
    "        # We're going to represent an entire document just by its [CLS] embedding (at position 0)\n",
    "        # And use the *last* layer output (layer -1)\n",
    "        # as a result of this choice, this embedding will be optimized for this purpose during the training process.\n",
    "\n",
    "        bert_hidden_states = bert_output['hidden_states']\n",
    "\n",
    "        out = bert_hidden_states[-1][:,0,:]\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out.squeeze()\n",
    "\n",
    "    def evaluate(self, batch_x, batch_y):\n",
    "\n",
    "        self.eval()\n",
    "        corr = 0.\n",
    "        total = 0.\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in zip(batch_x, batch_y):\n",
    "                y_preds = self.forward(x)\n",
    "                for idx, y_pred in enumerate(y_preds):\n",
    "                    prediction=torch.argmax(y_pred)\n",
    "                    if prediction == y[idx]:\n",
    "                        corr += 1.\n",
    "                    total+=1                          \n",
    "        return corr/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aae5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_shuffle(X, y, random_state=52):\n",
    "    \"\"\"\n",
    "    Oversamples X and y for equal class proportions\n",
    "    \"\"\"\n",
    "    \n",
    "    fakeX = np.arange(len(X), dtype=int).reshape((-1, 1))\n",
    "    ros = RandomOverSampler(random_state=random_state, sampling_strategy=1.0)\n",
    "    fakeX, y = ros.fit_resample(fakeX, y)\n",
    "    p = np.random.permutation(len(fakeX))\n",
    "    fakeX, y = fakeX[p], y[p]\n",
    "    X = X.iloc[fakeX.reshape((-1, ))]\n",
    "    return X.to_numpy(), y.to_numpy()\n",
    "\n",
    "def create_datasets(texts, scores, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    A function to split the texts (X variables) and the scores (y variables), and oversample all at once\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "        texts, scores, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train1, y_train1 = oversample_shuffle(X_train1, y_train1, random_state=random_state)\n",
    "    X_test1, y_test1 = oversample_shuffle(X_test1, y_test1, random_state=random_state)\n",
    "    \n",
    "    return X_train1, y_train1, X_test1, y_test1\n",
    "    \n",
    "def train_bert(texts, scores, params, test_size=0.2, random_state=42, save=False):\n",
    "    \"\"\"\n",
    "    Trains an entire BERT model with the given parameters, then returns the best accuracy out of all epochs\n",
    "    \n",
    "    Parameters list: a dict with the following keys (all optional)\n",
    "    \n",
    "    {\n",
    "    batch_size: The batch size as far as the optimizer is concerned. default 32\n",
    "    max_memory_size: The largest batch to actually use. This is because large batches wont fit on many GPUs.\n",
    "                        Gradient accumulation will then let us run optimization on batch_size instead. default 4\n",
    "    \n",
    "    num_epochs: The total number of epochs to train. default 8\n",
    "    lr: The learning rate to use for the final linear layer. default 1e-3\n",
    "    pretrained_lr = A separate learning rate for only the pretrained model. default 1e-5\n",
    "    betas = The betas parameter in the Adam optimizer. default (0.9, 0.98)\n",
    "    eps = The eps parameter in the Adam optimizer. default 1e-9\n",
    "    weight_decay = The weight_decay parameter in the Adam optimizer. default 0.005\n",
    "    num_warmup_steps = The number of warmup steps (each step is a batch). default 0\n",
    "    num_training_steps = The number of training steps in the algorithm. default num_epochs * len(batch_x) - num_warmup_steps\n",
    "    \n",
    "    dropout: A float (i.e. 0.1) if dropout is desired. default None\n",
    "    }\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    torch.random.manual_seed(random_state)\n",
    "    \n",
    "    texts = texts.copy()\n",
    "    scores = scores.copy()\n",
    "    \n",
    "    bert_model = BERTClassifier(params={**params, \"label_length\": 2})\n",
    "    bert_model.to(device)\n",
    "    \n",
    "    \n",
    "    X_train1, y_train1, X_test1, y_test1 = create_datasets(\n",
    "        texts, scores, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    batch_size = params['batch_size'] if 'batch_size' in params else 32\n",
    "    max_memory_size = params['max_memory_size'] if 'max_memory_size' in params else 4\n",
    "    \n",
    "    batch_x, batch_y = bert_model.get_batches(list(X_train1), list(y_train1), batch_size=max_memory_size)\n",
    "    dev_batch_x, dev_batch_y = bert_model.get_batches(list(X_test1),list( y_test1), batch_size=max_memory_size)\n",
    "\n",
    "    \n",
    "    num_epochs = params['num_epochs'] if 'num_epochs' in params else 8\n",
    "    lr = params['lr'] if 'lr' in params else 1e-3\n",
    "    pretrained_lr = params['pretrained_lr'] if 'pretrained_lr' in params else 1e-5\n",
    "    betas = params['betas'] if 'betas' in params else (0.9, 0.98)\n",
    "    eps = params['eps'] if 'eps' in params else 1e-9\n",
    "    weight_decay = params['weight_decay'] if 'weight_decay' in params else 0.005\n",
    "    num_warmup_steps = params['num_warmup_steps'] if 'num_warmup_steps' in params else 0\n",
    "    num_training_steps = params['num_training_steps'] if 'num_training_steps' in params \\\n",
    "        else num_epochs * len(batch_x) - num_warmup_steps\n",
    "    \n",
    "    \n",
    "    trainable_parameters = list(\n",
    "        params for params in bert_model.parameters() if params.requires_grad)\n",
    "    pretrained_params = set(trainable_parameters) & set(bert_model.bert.parameters())\n",
    "    new_params = set(trainable_parameters) - pretrained_params\n",
    "    grouped_trainable_parameters = [\n",
    "        {\n",
    "            'params': list(pretrained_params),\n",
    "            'lr': pretrained_lr,\n",
    "        },\n",
    "        {\n",
    "            'params': list(new_params),\n",
    "            'lr': lr,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    #torch.optim.\n",
    "    optimizer = AdamW(\n",
    "        grouped_trainable_parameters, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = get_scheduler(\n",
    "        'linear',\n",
    "        optimizer=optimizer, \n",
    "        num_warmup_steps=num_warmup_steps, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_acc = 0.\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        bert_model.train()\n",
    "\n",
    "        # Train\n",
    "        optimizer.zero_grad()\n",
    "        for i, (x, y) in enumerate(zip(batch_x, batch_y)):\n",
    "            y_pred = bert_model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
    "            loss.backward()\n",
    "            if (i + 1) % (batch_size // max_memory_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate\n",
    "        dev_accuracy=bert_model.evaluate(dev_batch_x, dev_batch_y)\n",
    "        if epoch % 1 == 0:\n",
    "#             print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            if dev_accuracy > best_dev_acc:\n",
    "                if save:\n",
    "                    torch.save(bert_model.state_dict(), save)\n",
    "                best_dev_acc = dev_accuracy\n",
    "    if save:\n",
    "        bert_model.load_state_dict(torch.load('best-model-parameters.pt'))\n",
    "        \n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
    "    return best_dev_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dafdbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(texts, scores, params, folds=5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    states = np.random.randint(0, 100000, size=folds)\n",
    "    accs = []\n",
    "    for state in states:\n",
    "        print(state)\n",
    "        accs.append(train_bert(texts, scores, params, test_size=round(1/folds, 2), random_state=state, save=False))\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41930ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 0.799367986285495\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfbdd687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#0.8164062683262356\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], {'batch_size': 16, 'lr': 1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "017b23df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [15:12<00:00, 114.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.784\n",
      "860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [22:53<00:00, 171.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.870\n",
      "76820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [26:23<00:00, 197.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.799\n",
      "54886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:46<00:00, 215.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.822\n",
      "6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [29:00<00:00, 217.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.820989455049683"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.820989455049683\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], {'batch_size': 16, 'lr': 1e-4, 'dropout':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f149a2a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:33<00:00, 214.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.779\n",
      "860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:30<00:00, 213.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.865\n",
      "76820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:44<00:00, 215.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.825\n",
      "54886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [29:07<00:00, 218.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.839\n",
      "6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [29:13<00:00, 219.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8206066141611448"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.8206066141611448\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], {'batch_size': 16, 'lr': 1e-4, 'num_warmup_steps':40})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a963f0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:46<00:00, 215.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.800\n",
      "860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [28:45<00:00, 215.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.875\n",
      "76820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [29:34<00:00, 221.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.835\n",
      "54886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [30:51<00:00, 231.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.839\n",
      "6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [32:06<00:00, 240.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8322880870561283"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.8322880870561283\n",
    "\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], {'batch_size': 16, 'lr': 1e-4, 'num_warmup_steps':40, 'dropout':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b6c1c88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [31:54<00:00, 239.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.774\n",
      "860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [31:26<00:00, 235.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.840\n",
      "76820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [34:39<00:00, 259.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.784\n",
      "54886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [33:21<00:00, 250.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.817\n",
      "6265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 8/8 [32:10<00:00, 241.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Performing Model achieves dev accuracy of : 0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8120893881846133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.8120893881846133\n",
    "\n",
    "# cross_validate(cult['text_no_tags'], cult['cultural_score'], {'batch_size': 32, 'lr': 1e-4, 'dropout':0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02db6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

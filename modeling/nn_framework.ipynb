{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using balanced samples from hand-labeled set of articles\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: December 2020\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/81/8db4d87b03b998fda7c6f835d807c9ae4e3b141f978597b8d7f31600be15/imbalanced_learn-0.7.0-py3-none-any.whl (167kB)\n",
      "\u001b[K     |████████████████████████████████| 174kB 6.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.2.1)\n",
      "Collecting joblib>=0.11 (from imbalanced-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn>=0.23 (from imbalanced-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/ed/ab51a8da34d2b3f4524b21093081e7f9e2ddf1c9eac9f795dcf68ad0a57d/scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
      "\u001b[K     |████████████████████████████████| 22.3MB 7.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.15.4)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.23->imbalanced-learn)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Installing collected packages: joblib, threadpoolctl, scikit-learn, imbalanced-learn\n",
      "  Found existing installation: scikit-learn 0.20.3\n",
      "    Uninstalling scikit-learn-0.20.3:\n",
      "      Successfully uninstalled scikit-learn-0.20.3\n",
      "Successfully installed imbalanced-learn-0.7.0 joblib-1.0.0 scikit-learn-0.24.0 threadpoolctl-2.1.0\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 6.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.0)\n",
      "Collecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/28/ff0d0936a31f15a0879caf6dac1f1cbaab1fc7b9e8baf8a1d5a70380fb22/regex-2020.11.13-cp37-cp37m-manylinux2010_x86_64.whl (667kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 21.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 31.5MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: regex, tqdm, nltk\n",
      "Successfully installed nltk-3.5 regex-2020.11.13 tqdm-4.56.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/f5/d79b5b40735086ff1100c680703e0f3efc830fa455e268e9e96f3c857e93/mxnet-1.6.0-py2.py3-none-any.whl (68.7MB)\n",
      "\u001b[K     |████████████████████████████████| 68.7MB 61.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
      "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.21.0)\n",
      "Collecting numpy<2.0.0,>1.16.0 (from mxnet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8MB)\n",
      "\u001b[K     |████████████████████████████████| 14.8MB 56.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2019.3.9)\n",
      "Installing collected packages: graphviz, numpy, mxnet\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "Successfully installed graphviz-0.8.4 mxnet-1.6.0 numpy-1.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, KFold\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "data_ctx = ctx\n",
    "model_ctx = ctx\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format\n",
    "from text_to_file import write_textlist, read_text # custom scripts for reading and writing text lists to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/modeling', '')\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + 'classification/data/'\n",
    "model_fp = root + 'classification/models/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_121620.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_121620.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_121620.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + 'vectorizer_cult_121620.joblib'\n",
    "relt_vec_fp = model_fp + 'vectorizer_relt_121620.joblib'\n",
    "demog_vec_fp = model_fp + 'vectorizer_demog_121620.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cultural_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[journal, of, managerial, issues, vol, xxiii,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[organization, ht, icna, vol, no, may, june, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[from, fiefs, to, clans, and, network, capita...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[the, collective, strategy, framework, an, ap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[manag, int, rev, doi, sl, research, article,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[int, studies, ofmgt, amp, org, vol, no, pp, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[linking, organizational, values, to, relatio...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[journal, of, organizational, behavior, organ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[®, academy, oí, management, learning, amp, e...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[strategie, management, journal, strat, mgmt,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cultural_score\n",
       "0  [[journal, of, managerial, issues, vol, xxiii,...             1.0\n",
       "1  [[organization, ht, icna, vol, no, may, june, ...             1.0\n",
       "2  [[from, fiefs, to, clans, and, network, capita...             1.0\n",
       "3  [[the, collective, strategy, framework, an, ap...             1.0\n",
       "4  [[manag, int, rev, doi, sl, research, article,...             1.0\n",
       "5  [[int, studies, ofmgt, amp, org, vol, no, pp, ...             1.0\n",
       "6  [[linking, organizational, values, to, relatio...             1.0\n",
       "7  [[journal, of, organizational, behavior, organ...             1.0\n",
       "8  [[®, academy, oí, management, learning, amp, e...             1.0\n",
       "9  [[strategie, management, journal, strat, mgmt,...             1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    209\n",
      "0.5     12\n",
      "1.0    133\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    229\n",
      "0.5      7\n",
      "1.0    114\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    248\n",
      "0.5      5\n",
      "1.0    101\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsure cases: where X_score = 0.5\n",
    "drop_unsure = True\n",
    "\n",
    "if drop_unsure:\n",
    "    cult_df_yes = cult_df[cult_df['cultural_score'] == 1.0]\n",
    "    cult_df_no = cult_df[cult_df['cultural_score'] == 0.0]\n",
    "    cult_df = pd.concat([cult_df_yes, cult_df_no])\n",
    "    \n",
    "    relt_df_yes = relt_df[relt_df['relational_score'] == 1.0]\n",
    "    relt_df_no = relt_df[relt_df['relational_score'] == 0.0]\n",
    "    relt_df = pd.concat([relt_df_yes, relt_df_no])\n",
    "    \n",
    "    demog_df_yes = demog_df[demog_df['demographic_score'] == 1.0]\n",
    "    demog_df_no = demog_df[demog_df['demographic_score'] == 0.0]\n",
    "    demog_df = pd.concat([demog_df_yes, demog_df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_article_tokens(article, return_string=False):\n",
    "    '''\n",
    "    Collects words from already-tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "        return_string: whether to return single, long string representing article\n",
    "    Returns:\n",
    "        tokens: string if return_string, else list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = [] # initialize\n",
    "    \n",
    "    if return_string:\n",
    "        for sent in article:\n",
    "            sent = ' '.join(sent) # make sentence into a string\n",
    "            tokens.append(sent) # add sentence to list of sentences\n",
    "        tokens = ' '.join(tokens) # join sentences into string\n",
    "        return tokens # return string\n",
    "    \n",
    "    else:\n",
    "        for sent in article:\n",
    "            tokens += [word for word in sent] # add each word to list of tokens\n",
    "        return tokens # return list of tokens\n",
    "\n",
    "# For capturing word frequencies, add all words from each article to single, shared list (can't use this to create models)\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 80217\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 596822),\n",
       " ('of', 428762),\n",
       " ('and', 330459),\n",
       " ('in', 232360),\n",
       " ('to', 218627),\n",
       " ('that', 117768),\n",
       " ('is', 103616),\n",
       " ('for', 94102),\n",
       " ('as', 76669),\n",
       " ('on', 63680),\n",
       " ('are', 63279),\n",
       " ('with', 59141),\n",
       " ('by', 57364),\n",
       " ('this', 53800),\n",
       " ('be', 47394),\n",
       " ('oasis', 46263),\n",
       " ('or', 42620),\n",
       " ('it', 40088),\n",
       " ('entry', 39972),\n",
       " ('from', 39891)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = (cult_tokens + relt_tokens) + demog_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 133028\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('colsep', 12325),\n",
       " ('rowsep', 12319),\n",
       " ('oasis entry colname', 11914),\n",
       " ('char', 7881),\n",
       " ('align char', 7802),\n",
       " ('valign bottom oasis entry', 7243),\n",
       " ('oasis row', 6022),\n",
       " ('align center', 2775),\n",
       " ('valign bottom', 2595),\n",
       " ('oasis entry colname colsep rowsep align char char oasis entry', 1589),\n",
       " ('align left', 1573),\n",
       " ('oasis entry colname colsep rowsep align char char', 1250),\n",
       " ('oasis entry colsep rowsep valign bottom oasis entry', 1045),\n",
       " ('label label', 767),\n",
       " ('fn', 744),\n",
       " ('sec', 606),\n",
       " ('oasis colspec colnum colname colwidth pi', 573),\n",
       " ('sec id sc', 560),\n",
       " ('oasis entry colsep rowsep oasis entry', 531),\n",
       " ('fn id fn', 509)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(\n",
    "    lambda article: cult_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(\n",
    "    lambda article: relt_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(\n",
    "    lambda article: demog_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = (cult_sents + relt_sents) + demog_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and apply text vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect articles: Add each article as single str to list of str:\n",
    "cult_docs = [] # empty list\n",
    "cult_df['text'].apply(\n",
    "    lambda article: cult_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "relt_docs = [] # empty list\n",
    "relt_df['text'].apply(\n",
    "    lambda article: relt_docs.append(\n",
    "       collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "demog_docs = [] # empty list\n",
    "demog_df['text'].apply(\n",
    "    lambda article: demog_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "print() # skip weird output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 66098\n",
      "['aa', 'affects', 'analogize', 'argumentatively', 'aveni', 'beifuss', 'blustein', 'buchholtz', 'carron', 'childlessness', 'coinciding', 'confused', 'councillors', 'czabansku', 'demirguc', 'dijeffren', 'dogs', 'economets', 'endler', 'ethnidzed', 'fad', 'fiskarbankadministrasjon', 'frenzen', 'ger', 'greenwood', 'haskell', 'hobson', 'identified', 'indexes', 'interceded', 'itbs', 'kaleidoscope', 'kraiger', 'legalized', 'lockett', 'makrosoziologie', 'mcenough', 'millenarian', 'mot', 'ncjrs', 'noninstrumental', 'offaculty', 'oshpd', 'parsimonious', 'phase', 'poovanalingum', 'primatively', 'punctuate', 'readership', 'relabeled', 'review', 'ruddy', 'schneider', 'severed', 'site', 'sparrowe', 'stochas', 'superlative', 'taurus', 'thus', 'transitioning', 'ularco', 'unsafe', 'vessies', 'weekends', 'wrinkle', 'ïï']\n",
      "\n",
      "Number of features in relational vectorizer: 64919\n",
      "['aa', 'affordance', 'ancestors', 'arose', 'aydin', 'benefitted', 'bonacich', 'burkart', 'ccc', 'circumventing', 'commodcomposition', 'contempowith', 'criticised', 'decadence', 'destroys', 'dispassionate', 'dube', 'eloy', 'ereignet', 'explosivefrom', 'fight', 'fracture', 'generalised', 'grants', 'harmoniic', 'hoard', 'identit', 'indices', 'interdis', 'iunmar', 'kangaroo', 'krise', 'lementair', 'longitudinal', 'malmo', 'meandeviated', 'minutes', 'mrvar', 'neighbor', 'nonshacho', 'oijt', 'opportunists', 'paedophiles', 'perfonnance', 'plundering', 'prejudiced', 'prowse', 'raphy', 'regular', 'retaining', 'rounded', 'schaechter', 'series', 'singleproject', 'sozialen', 'stipends', 'superconducting', 'taxing', 'tiefigrom', 'transzendentale', 'unanticipate', 'unwind', 'violently', 'wheatsheaf', 'xtxwit']\n",
      "\n",
      "Number of features in demographic vectorizer: 63542\n",
      "['aa', 'aforementioned', 'ancilliary', 'aronsson', 'aydin', 'benevolence', 'bond', 'burkhalter', 'ccnrwtcidi', 'citationsa', 'communalities', 'contextual', 'cronache', 'decisionistic', 'deterministic', 'disquieting', 'dumb', 'embarrassed', 'erupts', 'extra', 'finesse', 'frederica', 'geographies', 'greyser', 'hays', 'homelike', 'ija', 'inequities', 'interrupted', 'jason', 'kekre', 'lackey', 'libertarians', 'lrus', 'mario', 'mentd', 'mockery', 'mutualism', 'nificance', 'nummarr', 'operandi', 'oç', 'perchoices', 'plunged', 'premed', 'prussia', 'rated', 'reinhart', 'revenues', 'ruckgr', 'schuette', 'shaking', 'skogan', 'spiritedness', 'strik', 'swamp', 'tenor', 'tjto', 'truisms', 'undesirable', 'vague', 'wahrman', 'withdrawn', 'zat']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Uses TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "cult_vectorizer = joblib.load(cult_vec_fp, \"r+\")\n",
    "X_cult = cult_vectorizer.transform(cult_docs)\n",
    "print('Number of features in cultural vectorizer:', len(cult_vectorizer.get_feature_names()))\n",
    "print(cult_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "relt_vectorizer = joblib.load(relt_vec_fp, \"r+\")\n",
    "X_relt = relt_vectorizer.transform(relt_docs)\n",
    "print('Number of features in relational vectorizer:', len(relt_vectorizer.get_feature_names()))\n",
    "print(relt_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "demog_vectorizer = joblib.load(demog_vec_fp, \"r+\")\n",
    "X_demog = demog_vectorizer.transform(demog_docs)\n",
    "print('Number of features in demographic vectorizer:', len(demog_vectorizer.get_feature_names()))\n",
    "print(demog_vectorizer.get_feature_names()[::1000]) # get every 1000th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in preprocessed text for training cultural classifier (after applying cultural vectorizer): 66098\n",
      "['aa', 'affects', 'analogize', 'argumentatively', 'aveni', 'beifuss', 'blustein', 'buchholtz', 'carron', 'childlessness', 'coinciding', 'confused', 'councillors', 'czabansku', 'demirguc', 'dijeffren', 'dogs', 'economets', 'endler', 'ethnidzed', 'fad', 'fiskarbankadministrasjon', 'frenzen', 'ger', 'greenwood', 'haskell', 'hobson', 'identified', 'indexes', 'interceded', 'itbs', 'kaleidoscope', 'kraiger', 'legalized', 'lockett', 'makrosoziologie', 'mcenough', 'millenarian', 'mot', 'ncjrs', 'noninstrumental', 'offaculty', 'oshpd', 'parsimonious', 'phase', 'poovanalingum', 'primatively', 'punctuate', 'readership', 'relabeled', 'review', 'ruddy', 'schneider', 'severed', 'site', 'sparrowe', 'stochas', 'superlative', 'taurus', 'thus', 'transitioning', 'ularco', 'unsafe', 'vessies', 'weekends', 'wrinkle', 'ïï']\n",
      "\n",
      "Number of features in preprocessed text for training relational classifier (after applying relational vectorizer): 64919\n",
      "['aa', 'affordance', 'ancestors', 'arose', 'aydin', 'benefitted', 'bonacich', 'burkart', 'ccc', 'circumventing', 'commodcomposition', 'contempowith', 'criticised', 'decadence', 'destroys', 'dispassionate', 'dube', 'eloy', 'ereignet', 'explosivefrom', 'fight', 'fracture', 'generalised', 'grants', 'harmoniic', 'hoard', 'identit', 'indices', 'interdis', 'iunmar', 'kangaroo', 'krise', 'lementair', 'longitudinal', 'malmo', 'meandeviated', 'minutes', 'mrvar', 'neighbor', 'nonshacho', 'oijt', 'opportunists', 'paedophiles', 'perfonnance', 'plundering', 'prejudiced', 'prowse', 'raphy', 'regular', 'retaining', 'rounded', 'schaechter', 'series', 'singleproject', 'sozialen', 'stipends', 'superconducting', 'taxing', 'tiefigrom', 'transzendentale', 'unanticipate', 'unwind', 'violently', 'wheatsheaf', 'xtxwit']\n",
      "\n",
      "Number of features in preprocessed text for training demographic classifier (after applying demographic vectorizer): 63542\n",
      "['aa', 'aforementioned', 'ancilliary', 'aronsson', 'aydin', 'benevolence', 'bond', 'burkhalter', 'ccnrwtcidi', 'citationsa', 'communalities', 'contextual', 'cronache', 'decisionistic', 'deterministic', 'disquieting', 'dumb', 'embarrassed', 'erupts', 'extra', 'finesse', 'frederica', 'geographies', 'greyser', 'hays', 'homelike', 'ija', 'inequities', 'interrupted', 'jason', 'kekre', 'lackey', 'libertarians', 'lrus', 'mario', 'mentd', 'mockery', 'mutualism', 'nificance', 'nummarr', 'operandi', 'oç', 'perchoices', 'plunged', 'premed', 'prussia', 'rated', 'reinhart', 'revenues', 'ruckgr', 'schuette', 'shaking', 'skogan', 'spiritedness', 'strik', 'swamp', 'tenor', 'tjto', 'truisms', 'undesirable', 'vague', 'wahrman', 'withdrawn', 'zat']\n"
     ]
    }
   ],
   "source": [
    "# check out column order for data once vectorizer has been applied (should be exactly the same as list from previous cell)\n",
    "test = pd.DataFrame(X_cult.toarray(), columns=cult_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training cultural classifier (after applying cultural vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])\n",
    "print()\n",
    "test = pd.DataFrame(X_relt.toarray(), columns=relt_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training relational classifier (after applying relational vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])\n",
    "print()\n",
    "test = pd.DataFrame(X_demog.toarray(), columns=demog_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training demographic classifier (after applying demographic vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Balance x_train, y_train\n",
    "######################################################\n",
    "\n",
    "def resample_data(X_train, Y_train, undersample = False, sampling_ratio = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train: X training data\n",
    "        Y_train: Y training data\n",
    "        undersample: boolean for over or undersampling\n",
    "        sampling_ratio: ratio of minority to majority class\n",
    "        \n",
    "        archived/not used:\n",
    "        sampling_strategy: strategy for resampled distribution\n",
    "            if oversample: 'majority' makes minority = to majority\n",
    "            if undersample: 'minority' makes majority = to minority\n",
    "            \n",
    "    Returns:\n",
    "        X_balanced: predictors at balanced ratio\n",
    "        Y_balanced: outcomes at balanced ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = undersample.fit_resample(X_train, Y_train)\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = oversample.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    print(f'Y_train: {Counter(Y_train)}\\nY_resample: {Counter(Y_balanced)}')\n",
    "    \n",
    "    return X_balanced, Y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# k-fold cross validation for model evaluation\n",
    "######################################################\n",
    "\n",
    "# Define test options for k-fold CV\n",
    "num_folds = 10 \n",
    "seed = 3\n",
    "scoring='f1_weighted' # set scoring metric (not used here)\n",
    "\n",
    "def show_kfold_output(models, \n",
    "                      X, \n",
    "                      Y, \n",
    "                      num_folds = num_folds, \n",
    "                      random_state = seed, \n",
    "                      shuffle = True):\n",
    "    '''\n",
    "    Estimates the accuracy of different model algorithms, adds results to a results array and returns.\n",
    "    Prints the accuracy results: averages and std.\n",
    "    Uses cross_val_predict, which unlike cross_val_score cannot define scoring option/evaluation metric.\n",
    "    \n",
    "    Args:\n",
    "        models: list of (name, model) tuples\n",
    "        X: predictors\n",
    "        Y: outcomes\n",
    "        num_folds: Split data randomly into num_folds parts: (num_folds-1) for training, 1 for scoring\n",
    "        random_state: seed\n",
    "        shuffle: \n",
    "    \n",
    "    Returns:\n",
    "        results: list of model results\n",
    "        names: list of model names (matches results)\n",
    "        \n",
    "    Source: \n",
    "        https://stackoverflow.com/questions/40057049/using-confusion-matrix-as-scoring-metric-in-cross-validation-in-scikit-learn\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Setup model options\n",
    "        kfold = KFold(\n",
    "            n_splits=num_folds, \n",
    "            random_state=seed, \n",
    "            shuffle=True)\n",
    "        \n",
    "        # Get kfold results\n",
    "        cv_results = cross_val_predict(\n",
    "            model, \n",
    "            X, \n",
    "            Y, \n",
    "            cv=kfold, \n",
    "            #scoring=scoring, \n",
    "            n_jobs=-1) # use all cores = faster\n",
    "        \n",
    "        # Add results and name of each algorithm to the model array\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'{name}:')\n",
    "        print()\n",
    "        print(f'Mean (std):\\t {round(cv_results.mean(),4)} ({round(cv_results.std(),4)})')\n",
    "        print(f'Accuracy:\\t', {round(accuracy_score(Y_balanced, cv_results)), 4})\n",
    "        print()\n",
    "        print('Confusion matrix:\\n', confusion_matrix(Y_balanced, cv_results))\n",
    "        print()\n",
    "        print('Report:\\n', classification_report(Y_balanced, cv_results))\n",
    "        print()\n",
    "        \n",
    "    # Return arrays\n",
    "    return results, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 342\n",
      "Number of codes (should match): 342\n",
      "Y_train Distribution: [(0.0, 165), (1.0, 108)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "print(\"Number of cases:\", str(X_cult.shape[0]))\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    test_size=test_size, \n",
    "    random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 209, 1.0: 133})\n",
      "Y_resample: Counter({1.0: 209, 0.0: 209})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "# Use these settings here and below\n",
    "sampling_ratio = 1.0 # ratio of minority to majority cases\n",
    "undersample = False # whether to undersample or oversample\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "num_outputs = 2\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for batch in data_iterator:\n",
    "        data = batch.data[0].as_in_context(model_ctx).reshape((-1, 66098))\n",
    "        label = batch.label[0].as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = X_train.todense()\n",
    "X_validate_dense = X_validate.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(X_train.todense().reshape(-1,66098), Y_train, 1, True)\n",
    "test_iter = mx.io.NDArrayIter(X_validate.todense().reshape(-1,66098), Y_validate, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.io.io.NDArrayIter at 0x7f7f547227f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.6790636607578823, Train_acc nan, Test_acc 0.6376811594202898\n",
      "Epoch 1. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 2. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 3. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 4. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 5. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 6. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 7. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 8. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 9. Loss: 0.0, Train_acc nan, Test_acc nan\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for batch in train_iter:\n",
    "        data = batch.data[0].as_in_context(model_ctx).reshape((-1, 66098))\n",
    "        label = batch.label[0].as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/len(X_train.todense()), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.842 (+/-0.102) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.095) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.639 (+/-0.388) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.701 (+/-0.431) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.842 (+/-0.102) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.849 (+/-0.095) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.847 (+/-0.106) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.852 (+/-0.090) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.627 (+/-0.365) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.809 (+/-0.441) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.519 (+/-0.059) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.847 (+/-0.097) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.086) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.852 (+/-0.099) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.852 (+/-0.102) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.805 (+/-0.436) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.845 (+/-0.099) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.854 (+/-0.075) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.849 (+/-0.101) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.854 (+/-0.105) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.560 (+/-0.168) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.770 (+/-0.416) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.091) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.510 (+/-0.032) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.857 (+/-0.091) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.084) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.859 (+/-0.090) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.675 (+/-0.351) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.081) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.514 (+/-0.046) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.857 (+/-0.102) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.071) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.082) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.703 (+/-0.303) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.672 (+/-0.241) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.505 (+/-0.009) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.859 (+/-0.088) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.859 (+/-0.078) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.869 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.010) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.861 (+/-0.084) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.620 (+/-0.344) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.641 (+/-0.395) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.861 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.009) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.082) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.876 (+/-0.075) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.522 (+/-0.056) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.079) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.639 (+/-0.388) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.618 (+/-0.327) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.897 (+/-0.190) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.864 (+/-0.082) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.505 (+/-0.024) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.079) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.869 (+/-0.082) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.869 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.507 (+/-0.010) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.806 (+/-0.131) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.517 (+/-0.053) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.610 (+/-0.310) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.869 (+/-0.077) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.071) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.876 (+/-0.079) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.596 (+/-0.276) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.782 (+/-0.253) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.510 (+/-0.032) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.871 (+/-0.076) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.505 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.071) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99577857e-01, 4.22142538e-04]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential() # should this be \"net\"?\\nmodel.add(Dense(12, input_dim=8, activation=\\'relu\\'))\\nmodel.add(Dense(8, activation=\\'relu\\'))\\nmodel.add(Dense(1, activation=\\'sigmoid\\'))\\n# compile the keras model\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\n# fit the keras model on the dataset\\nmodel.fit(X_balanced, Y_balanced, epochs=150, batch_size=10)\\n# evaluate the keras model\\n_, accuracy = model.evaluate(X, y)\\nprint(\\'Accuracy: %.2f\\' % (accuracy*100))\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#y = dataset[:,8]\n",
    "# define the keras model\n",
    "'''\n",
    "model = Sequential() # should this be \"net\"?\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_balanced, Y_balanced, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6292 (0.483)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[140  69]\n",
      " [ 15 194]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.67      0.77       209\n",
      "         1.0       0.74      0.93      0.82       209\n",
      "\n",
      "    accuracy                           0.80       418\n",
      "   macro avg       0.82      0.80      0.80       418\n",
      "weighted avg       0.82      0.80      0.80       418\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.4761 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[196  13]\n",
      " [ 23 186]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.92       209\n",
      "         1.0       0.93      0.89      0.91       209\n",
      "\n",
      "    accuracy                           0.91       418\n",
      "   macro avg       0.91      0.91      0.91       418\n",
      "weighted avg       0.91      0.91      0.91       418\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5718 (0.4948)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[156  53]\n",
      " [ 23 186]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.75      0.80       209\n",
      "         1.0       0.78      0.89      0.83       209\n",
      "\n",
      "    accuracy                           0.82       418\n",
      "   macro avg       0.82      0.82      0.82       418\n",
      "weighted avg       0.82      0.82      0.82       418\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.8373 (0.3691)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 68 141]\n",
      " [  0 209]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.33      0.49       209\n",
      "         1.0       0.60      1.00      0.75       209\n",
      "\n",
      "    accuracy                           0.66       418\n",
      "   macro avg       0.80      0.66      0.62       418\n",
      "weighted avg       0.80      0.66      0.62       418\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.4665 (0.4989)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[191  18]\n",
      " [ 32 177]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.88       209\n",
      "         1.0       0.91      0.85      0.88       209\n",
      "\n",
      "    accuracy                           0.88       418\n",
      "   macro avg       0.88      0.88      0.88       418\n",
      "weighted avg       0.88      0.88      0.88       418\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.5191 (0.4996)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 91 118]\n",
      " [110  99]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.44      0.44       209\n",
      "         1.0       0.46      0.47      0.46       209\n",
      "\n",
      "    accuracy                           0.45       418\n",
      "   macro avg       0.45      0.45      0.45       418\n",
      "weighted avg       0.45      0.45      0.45       418\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.5144 (0.4998)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[185  24]\n",
      " [ 18 191]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90       209\n",
      "         1.0       0.89      0.91      0.90       209\n",
      "\n",
      "    accuracy                           0.90       418\n",
      "   macro avg       0.90      0.90      0.90       418\n",
      "weighted avg       0.90      0.90      0.90       418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_cult_012021.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_cult = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "#joblib.dump(rf_cult, cult_model_fp)\n",
    "\n",
    "joblib.dump(clf, cult_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 343\n",
      "Number of codes (should match): 343\n",
      "Y_train Distribution: [(0.0, 183), (1.0, 91)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "relt_df = relt_df[['text', 'relational_score']]\n",
    "print(\"Number of cases:\", str(X_relt.shape[0]))\n",
    "\n",
    "valueArray = relt_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_relt, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 229, 1.0: 114})\n",
      "Y_resample: Counter({1.0: 229, 0.0: 229})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_relt, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.928 (+/-0.092) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.928 (+/-0.092) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.526 (+/-0.065) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.653 (+/-0.436) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.930 (+/-0.077) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.612 (+/-0.307) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.081) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.081) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.081) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.609 (+/-0.303) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.725 (+/-0.397) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.081) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.513 (+/-0.019) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.928 (+/-0.092) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.520 (+/-0.056) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.935 (+/-0.065) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.535 (+/-0.099) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.071) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.876 (+/-0.221) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.599 (+/-0.288) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.648 (+/-0.424) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.089) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.624 (+/-0.285) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.926 (+/-0.089) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.928 (+/-0.093) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.081) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.736 (+/-0.384) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.782 (+/-0.406) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.515 (+/-0.044) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.928 (+/-0.083) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.089) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.507 (+/-0.014) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.089) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.080) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.749 (+/-0.385) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.633 (+/-0.381) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.924 (+/-0.086) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.507 (+/-0.028) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.921 (+/-0.083) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.935 (+/-0.067) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.504 (+/-0.008) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.930 (+/-0.079) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.948 (+/-0.098) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.806 (+/-0.428) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.524 (+/-0.068) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.928 (+/-0.074) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.928 (+/-0.074) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.522 (+/-0.035) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.943 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.733 (+/-0.382) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.533 (+/-0.084) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.653 (+/-0.436) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.939 (+/-0.089) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.544 (+/-0.133) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.935 (+/-0.085) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.945 (+/-0.071) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.948 (+/-0.075) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.520 (+/-0.051) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.575 (+/-0.211) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.632 (+/-0.381) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.753 (+/-0.359) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.017) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.941 (+/-0.083) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.941 (+/-0.083) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.945 (+/-0.071) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.945 (+/-0.081) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.061) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.611 (+/-0.310) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.664 (+/-0.474) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.531 (+/-0.041) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.943 (+/-0.087) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.943 (+/-0.087) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.077) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.954 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.570 (+/-0.193) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.718 (+/-0.324) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.509 (+/-0.020) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.737 (+/-0.382) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.531 (+/-0.087) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.948 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.535 (+/-0.103) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.950 (+/-0.089) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6223 (0.4848)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[163  66]\n",
      " [ 10 219]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.71      0.81       229\n",
      "         1.0       0.77      0.96      0.85       229\n",
      "\n",
      "    accuracy                           0.83       458\n",
      "   macro avg       0.86      0.83      0.83       458\n",
      "weighted avg       0.86      0.83      0.83       458\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.5022 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[220   9]\n",
      " [  8 221]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       229\n",
      "         1.0       0.96      0.97      0.96       229\n",
      "\n",
      "    accuracy                           0.96       458\n",
      "   macro avg       0.96      0.96      0.96       458\n",
      "weighted avg       0.96      0.96      0.96       458\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5546 (0.497)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[195  34]\n",
      " [  9 220]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90       229\n",
      "         1.0       0.87      0.96      0.91       229\n",
      "\n",
      "    accuracy                           0.91       458\n",
      "   macro avg       0.91      0.91      0.91       458\n",
      "weighted avg       0.91      0.91      0.91       458\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.69 (0.4625)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[138  91]\n",
      " [  4 225]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.60      0.74       229\n",
      "         1.0       0.71      0.98      0.83       229\n",
      "\n",
      "    accuracy                           0.79       458\n",
      "   macro avg       0.84      0.79      0.78       458\n",
      "weighted avg       0.84      0.79      0.78       458\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.476 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[218  11]\n",
      " [ 22 207]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       229\n",
      "         1.0       0.95      0.90      0.93       229\n",
      "\n",
      "    accuracy                           0.93       458\n",
      "   macro avg       0.93      0.93      0.93       458\n",
      "weighted avg       0.93      0.93      0.93       458\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.6376 (0.4807)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 80 149]\n",
      " [ 86 143]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.35      0.41       229\n",
      "         1.0       0.49      0.62      0.55       229\n",
      "\n",
      "    accuracy                           0.49       458\n",
      "   macro avg       0.49      0.49      0.48       458\n",
      "weighted avg       0.49      0.49      0.48       458\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.524 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[214  15]\n",
      " [  4 225]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.93      0.96       229\n",
      "         1.0       0.94      0.98      0.96       229\n",
      "\n",
      "    accuracy                           0.96       458\n",
      "   macro avg       0.96      0.96      0.96       458\n",
      "weighted avg       0.96      0.96      0.96       458\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_relt_012021.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_relt = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(clf, relt_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 349\n",
      "Number of codes (should match): 349\n",
      "Y_train Distribution: [(0.0, 195), (1.0, 84)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First reltove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "demog_df = demog_df[['text', 'demographic_score']]\n",
    "print(\"Number of cases:\", str(X_demog.shape[0]))\n",
    "\n",
    "valueArray = demog_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_demog, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 248, 1.0: 101})\n",
      "Y_resample: Counter({1.0: 248, 0.0: 248})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_demog, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.641 (+/-0.398) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.647 (+/-0.406) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.544 (+/-0.072) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.748 (+/-0.408) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.583 (+/-0.239) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.799 (+/-0.423) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.587 (+/-0.241) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.631 (+/-0.369) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.522 (+/-0.071) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.510 (+/-0.033) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.627 (+/-0.364) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.873 (+/-0.277) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.647 (+/-0.423) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.548 (+/-0.141) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.546 (+/-0.139) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.964 (+/-0.052) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.812 (+/-0.438) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.014) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.964 (+/-0.052) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.964 (+/-0.052) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.915 (+/-0.183) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.652 (+/-0.424) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.859 (+/-0.200) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.637 (+/-0.376) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.974 (+/-0.037) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.615 (+/-0.237) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.970 (+/-0.043) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.816 (+/-0.445) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.908 (+/-0.204) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.658 (+/-0.441) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.968 (+/-0.046) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.966 (+/-0.049) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.980 (+/-0.030) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.978 (+/-0.032) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.666 (+/-0.473) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.542 (+/-0.116) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.655 (+/-0.429) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.008) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.974 (+/-0.037) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.647 (+/-0.421) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.974 (+/-0.037) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.976 (+/-0.036) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.978 (+/-0.032) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.666 (+/-0.473) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.664 (+/-0.467) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.768 (+/-0.391) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.972 (+/-0.040) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.616 (+/-0.321) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.972 (+/-0.040) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.506 (+/-0.009) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.978 (+/-0.032) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.982 (+/-0.030) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.808 (+/-0.444) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.660 (+/-0.456) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.794 (+/-0.425) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.976 (+/-0.036) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.974 (+/-0.040) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6371 (0.4808)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[179  69]\n",
      " [  1 247]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.72      0.84       248\n",
      "         1.0       0.78      1.00      0.88       248\n",
      "\n",
      "    accuracy                           0.86       496\n",
      "   macro avg       0.89      0.86      0.86       496\n",
      "weighted avg       0.89      0.86      0.86       496\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.496 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[244   4]\n",
      " [  6 242]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       248\n",
      "         1.0       0.98      0.98      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5484 (0.4977)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[217  31]\n",
      " [  7 241]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.88      0.92       248\n",
      "         1.0       0.89      0.97      0.93       248\n",
      "\n",
      "    accuracy                           0.92       496\n",
      "   macro avg       0.93      0.92      0.92       496\n",
      "weighted avg       0.93      0.92      0.92       496\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.6472 (0.4778)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[175  73]\n",
      " [  0 248]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.71      0.83       248\n",
      "         1.0       0.77      1.00      0.87       248\n",
      "\n",
      "    accuracy                           0.85       496\n",
      "   macro avg       0.89      0.85      0.85       496\n",
      "weighted avg       0.89      0.85      0.85       496\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.5 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[243   5]\n",
      " [  5 243]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       248\n",
      "         1.0       0.98      0.98      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.6996 (0.4584)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 62 186]\n",
      " [ 87 161]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.25      0.31       248\n",
      "         1.0       0.46      0.65      0.54       248\n",
      "\n",
      "    accuracy                           0.45       496\n",
      "   macro avg       0.44      0.45      0.43       496\n",
      "weighted avg       0.44      0.45      0.43       496\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.504 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[243   5]\n",
      " [  3 245]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       248\n",
      "         1.0       0.98      0.99      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_demog_012021.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_demog = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(clf, demog_model_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare classification methods for identifying org. science perspectives in JSTOR articles\n",
    "## Using balanced samples from hand-labeled set of articles\n",
    "\n",
    "@author: Jaren Haber, PhD<br>\n",
    "@coauthors: Prof. Heather Haveman, UC Berkeley; Yoon Sung Hong, Wayfair<br>\n",
    "@contact: Jaren.Haber@georgetown.edu<br>\n",
    "@project: Computational Literature Review of Organizational Scholarship<br>\n",
    "@date: December 2020\n",
    "\n",
    "'''\n",
    "Trains classifiers to predict whether an article is about a given perspective in org. science. To train the classifiers, uses preliminary labeled articles, broken down as follows: \n",
    "Cultural: 105 yes, 209 no\n",
    "Relational: 92 yes, 230 no\n",
    "Demographic: 77 yes, 249 no\n",
    "Compares f1_weighted scores of four model structures using 10-Fold Cross Validation: Logistic regression, SVM, Naive Bayes, and Decision Tree. Oversamples training data to .7 (7:10 minority:majority class).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.23 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (0.24.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23->imbalanced-learn) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk) (4.56.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U imbalanced-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mxnet in /opt/conda/lib/python3.7/site-packages (1.6.0)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (2.21.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /opt/conda/lib/python3.7/site-packages (from mxnet) (1.19.5)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from mxnet) (0.8.4)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet) (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Import libraries\n",
    "######################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split, KFold\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "data_ctx = ctx\n",
    "model_ctx = ctx\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sys; sys.path.insert(0, \"../preprocess/\") # For loading functions from files in other directory\n",
    "from quickpickle import quickpickle_dump, quickpickle_load # custom scripts for quick saving & loading to pickle format\n",
    "from text_to_file import write_textlist, read_text # custom scripts for reading and writing text lists to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Define filepaths\n",
    "######################################################\n",
    "\n",
    "thisday = date.today().strftime(\"%m%d%y\")\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root = str.replace(cwd, 'classification/modeling', '')\n",
    "\n",
    "# Directory for prepared data and trained models: save files here\n",
    "data_fp = root + 'classification/data/'\n",
    "model_fp = root + 'classification/models/'\n",
    "\n",
    "# Current article lists\n",
    "article_list_fp = data_fp + 'filtered_length_index.csv' # Filtered index of research articles\n",
    "article_paths_fp = data_fp + 'filtered_length_article_paths.csv' # List of article file paths\n",
    "\n",
    "# Preprocessed training data\n",
    "cult_labeled_fp = data_fp + 'training_cultural_preprocessed_121620.pkl'\n",
    "relt_labeled_fp = data_fp + 'training_relational_preprocessed_121620.pkl'\n",
    "demog_labeled_fp = data_fp + 'training_demographic_preprocessed_121620.pkl'\n",
    "\n",
    "# Model filepaths\n",
    "cult_model_fp = model_fp + f'classifier_cult_{str(thisday)}.joblib'\n",
    "relt_model_fp = model_fp + f'classifier_relt_{str(thisday)}.joblib'\n",
    "demog_model_fp = model_fp + f'classifier_demog_{str(thisday)}.joblib'\n",
    "\n",
    "# Vectorizers trained on hand-coded data (use to limit vocab of input texts)\n",
    "cult_vec_fp = model_fp + 'vectorizer_cult_121620.joblib'\n",
    "relt_vec_fp = model_fp + 'vectorizer_relt_121620.joblib'\n",
    "demog_vec_fp = model_fp + 'vectorizer_demog_121620.joblib'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cultural_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[journal, of, managerial, issues, vol, xxiii,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[organization, ht, icna, vol, no, may, june, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[from, fiefs, to, clans, and, network, capita...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[the, collective, strategy, framework, an, ap...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[manag, int, rev, doi, sl, research, article,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[int, studies, ofmgt, amp, org, vol, no, pp, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[linking, organizational, values, to, relatio...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[journal, of, organizational, behavior, organ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[®, academy, oí, management, learning, amp, e...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[strategie, management, journal, strat, mgmt,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  cultural_score\n",
       "0  [[journal, of, managerial, issues, vol, xxiii,...             1.0\n",
       "1  [[organization, ht, icna, vol, no, may, june, ...             1.0\n",
       "2  [[from, fiefs, to, clans, and, network, capita...             1.0\n",
       "3  [[the, collective, strategy, framework, an, ap...             1.0\n",
       "4  [[manag, int, rev, doi, sl, research, article,...             1.0\n",
       "5  [[int, studies, ofmgt, amp, org, vol, no, pp, ...             1.0\n",
       "6  [[linking, organizational, values, to, relatio...             1.0\n",
       "7  [[journal, of, organizational, behavior, organ...             1.0\n",
       "8  [[®, academy, oí, management, learning, amp, e...             1.0\n",
       "9  [[strategie, management, journal, strat, mgmt,...             1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cult_df = quickpickle_load(cult_labeled_fp)\n",
    "relt_df = quickpickle_load(relt_labeled_fp)\n",
    "demog_df = quickpickle_load(demog_labeled_fp)\n",
    "\n",
    "cult_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural_score\n",
      "0.0    209\n",
      "0.5     12\n",
      "1.0    133\n",
      "dtype: int64\n",
      "\n",
      "relational_score\n",
      "0.0    229\n",
      "0.5      7\n",
      "1.0    114\n",
      "dtype: int64\n",
      "\n",
      "demographic_score\n",
      "0.0    248\n",
      "0.5      5\n",
      "1.0    101\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check score distribution across classes\n",
    "print(cult_df.groupby('cultural_score').size())\n",
    "print()\n",
    "print(relt_df.groupby('relational_score').size())\n",
    "print()\n",
    "print(demog_df.groupby('demographic_score').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unsure cases: where X_score = 0.5\n",
    "drop_unsure = True\n",
    "\n",
    "if drop_unsure:\n",
    "    cult_df_yes = cult_df[cult_df['cultural_score'] == 1.0]\n",
    "    cult_df_no = cult_df[cult_df['cultural_score'] == 0.0]\n",
    "    cult_df = pd.concat([cult_df_yes, cult_df_no])\n",
    "    \n",
    "    relt_df_yes = relt_df[relt_df['relational_score'] == 1.0]\n",
    "    relt_df_no = relt_df[relt_df['relational_score'] == 0.0]\n",
    "    relt_df = pd.concat([relt_df_yes, relt_df_no])\n",
    "    \n",
    "    demog_df_yes = demog_df[demog_df['demographic_score'] == 1.0]\n",
    "    demog_df_no = demog_df[demog_df['demographic_score'] == 0.0]\n",
    "    demog_df = pd.concat([demog_df_yes, demog_df_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocab size and frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def collect_article_tokens(article, return_string=False):\n",
    "    '''\n",
    "    Collects words from already-tokenized sentences representing each article.\n",
    "    \n",
    "    Args:\n",
    "        article: list of lists of words (each list is a sentence)\n",
    "        return_string: whether to return single, long string representing article\n",
    "    Returns:\n",
    "        tokens: string if return_string, else list of tokens\n",
    "    '''\n",
    "    \n",
    "    tokens = [] # initialize\n",
    "    \n",
    "    if return_string:\n",
    "        for sent in article:\n",
    "            sent = ' '.join(sent) # make sentence into a string\n",
    "            tokens.append(sent) # add sentence to list of sentences\n",
    "        tokens = ' '.join(tokens) # join sentences into string\n",
    "        return tokens # return string\n",
    "    \n",
    "    else:\n",
    "        for sent in article:\n",
    "            tokens += [word for word in sent] # add each word to list of tokens\n",
    "        return tokens # return list of tokens\n",
    "\n",
    "# For capturing word frequencies, add all words from each article to single, shared list (can't use this to create models)\n",
    "cult_tokens = []; cult_df['text'].apply(lambda article: cult_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "relt_tokens = []; relt_df['text'].apply(lambda article: relt_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "demog_tokens = []; demog_df['text'].apply(lambda article: demog_tokens.extend([word for word in collect_article_tokens(article)]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 80217\n",
      "\n",
      "20 most frequent words in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 596822),\n",
       " ('of', 428762),\n",
       " ('and', 330459),\n",
       " ('in', 232360),\n",
       " ('to', 218627),\n",
       " ('that', 117768),\n",
       " ('is', 103616),\n",
       " ('for', 94102),\n",
       " ('as', 76669),\n",
       " ('on', 63680),\n",
       " ('are', 63279),\n",
       " ('with', 59141),\n",
       " ('by', 57364),\n",
       " ('this', 53800),\n",
       " ('be', 47394),\n",
       " ('oasis', 46263),\n",
       " ('or', 42620),\n",
       " ('it', 40088),\n",
       " ('entry', 39972),\n",
       " ('from', 39891)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at size of vocabulary and most frequent words\n",
    "tokens = (cult_tokens + relt_tokens) + demog_tokens\n",
    "print('Vocab size:', len(set(tokens)))\n",
    "print()\n",
    "\n",
    "# Check out most frequent words in labeled texts\n",
    "freq = Counter(tokens)\n",
    "print('20 most frequent words in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check frequent sentences (to improve cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 133028\n",
      "\n",
      "20 most frequent sentences in labeled articles:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('colsep', 12325),\n",
       " ('rowsep', 12319),\n",
       " ('oasis entry colname', 11914),\n",
       " ('char', 7881),\n",
       " ('align char', 7802),\n",
       " ('valign bottom oasis entry', 7243),\n",
       " ('oasis row', 6022),\n",
       " ('align center', 2775),\n",
       " ('valign bottom', 2595),\n",
       " ('oasis entry colname colsep rowsep align char char oasis entry', 1589),\n",
       " ('align left', 1573),\n",
       " ('oasis entry colname colsep rowsep align char char', 1250),\n",
       " ('oasis entry colsep rowsep valign bottom oasis entry', 1045),\n",
       " ('label label', 767),\n",
       " ('fn', 744),\n",
       " ('sec', 606),\n",
       " ('oasis colspec colnum colname colwidth pi', 573),\n",
       " ('sec id sc', 560),\n",
       " ('oasis entry colsep rowsep oasis entry', 531),\n",
       " ('fn id fn', 509)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentences from each article to empty list:\n",
    "cult_sents = []; cult_df['text'].apply(\n",
    "    lambda article: cult_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "relt_sents = []; relt_df['text'].apply(\n",
    "    lambda article: relt_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "demog_sents = []; demog_df['text'].apply(\n",
    "    lambda article: demog_sents.extend(\n",
    "        [' '.join([word for word in sent]) for sent in article]))\n",
    "\n",
    "sents = (cult_sents + relt_sents) + demog_sents\n",
    "print('Number of sentences:', len(sents))\n",
    "print()\n",
    "\n",
    "# Check out most frequent sentences in labeled texts\n",
    "freq = Counter(sents)\n",
    "print('20 most frequent sentences in labeled articles:')\n",
    "freq.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and apply text vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect articles: Add each article as single str to list of str:\n",
    "cult_docs = [] # empty list\n",
    "cult_df['text'].apply(\n",
    "    lambda article: cult_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "relt_docs = [] # empty list\n",
    "relt_df['text'].apply(\n",
    "    lambda article: relt_docs.append(\n",
    "       collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "demog_docs = [] # empty list\n",
    "demog_df['text'].apply(\n",
    "    lambda article: demog_docs.append(\n",
    "        collect_article_tokens(\n",
    "            article, \n",
    "            return_string=True)))\n",
    "\n",
    "print() # skip weird output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in cultural vectorizer: 66098\n",
      "['aa', 'affects', 'analogize', 'argumentatively', 'aveni', 'beifuss', 'blustein', 'buchholtz', 'carron', 'childlessness', 'coinciding', 'confused', 'councillors', 'czabansku', 'demirguc', 'dijeffren', 'dogs', 'economets', 'endler', 'ethnidzed', 'fad', 'fiskarbankadministrasjon', 'frenzen', 'ger', 'greenwood', 'haskell', 'hobson', 'identified', 'indexes', 'interceded', 'itbs', 'kaleidoscope', 'kraiger', 'legalized', 'lockett', 'makrosoziologie', 'mcenough', 'millenarian', 'mot', 'ncjrs', 'noninstrumental', 'offaculty', 'oshpd', 'parsimonious', 'phase', 'poovanalingum', 'primatively', 'punctuate', 'readership', 'relabeled', 'review', 'ruddy', 'schneider', 'severed', 'site', 'sparrowe', 'stochas', 'superlative', 'taurus', 'thus', 'transitioning', 'ularco', 'unsafe', 'vessies', 'weekends', 'wrinkle', 'ïï']\n",
      "\n",
      "Number of features in relational vectorizer: 64919\n",
      "['aa', 'affordance', 'ancestors', 'arose', 'aydin', 'benefitted', 'bonacich', 'burkart', 'ccc', 'circumventing', 'commodcomposition', 'contempowith', 'criticised', 'decadence', 'destroys', 'dispassionate', 'dube', 'eloy', 'ereignet', 'explosivefrom', 'fight', 'fracture', 'generalised', 'grants', 'harmoniic', 'hoard', 'identit', 'indices', 'interdis', 'iunmar', 'kangaroo', 'krise', 'lementair', 'longitudinal', 'malmo', 'meandeviated', 'minutes', 'mrvar', 'neighbor', 'nonshacho', 'oijt', 'opportunists', 'paedophiles', 'perfonnance', 'plundering', 'prejudiced', 'prowse', 'raphy', 'regular', 'retaining', 'rounded', 'schaechter', 'series', 'singleproject', 'sozialen', 'stipends', 'superconducting', 'taxing', 'tiefigrom', 'transzendentale', 'unanticipate', 'unwind', 'violently', 'wheatsheaf', 'xtxwit']\n",
      "\n",
      "Number of features in demographic vectorizer: 63542\n",
      "['aa', 'aforementioned', 'ancilliary', 'aronsson', 'aydin', 'benevolence', 'bond', 'burkhalter', 'ccnrwtcidi', 'citationsa', 'communalities', 'contextual', 'cronache', 'decisionistic', 'deterministic', 'disquieting', 'dumb', 'embarrassed', 'erupts', 'extra', 'finesse', 'frederica', 'geographies', 'greyser', 'hays', 'homelike', 'ija', 'inequities', 'interrupted', 'jason', 'kekre', 'lackey', 'libertarians', 'lrus', 'mario', 'mentd', 'mockery', 'mutualism', 'nificance', 'nummarr', 'operandi', 'oç', 'perchoices', 'plunged', 'premed', 'prussia', 'rated', 'reinhart', 'revenues', 'ruckgr', 'schuette', 'shaking', 'skogan', 'spiritedness', 'strik', 'swamp', 'tenor', 'tjto', 'truisms', 'undesirable', 'vague', 'wahrman', 'withdrawn', 'zat']\n"
     ]
    }
   ],
   "source": [
    "# Define stopwords used by JSTOR\n",
    "jstor_stopwords = set([\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"])\n",
    "\n",
    "# Uses TFIDF weighted DTM because results in better classifier accuracy than unweighted\n",
    "cult_vectorizer = joblib.load(cult_vec_fp, \"r+\")\n",
    "X_cult = cult_vectorizer.transform(cult_docs)\n",
    "print('Number of features in cultural vectorizer:', len(cult_vectorizer.get_feature_names()))\n",
    "print(cult_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "relt_vectorizer = joblib.load(relt_vec_fp, \"r+\")\n",
    "X_relt = relt_vectorizer.transform(relt_docs)\n",
    "print('Number of features in relational vectorizer:', len(relt_vectorizer.get_feature_names()))\n",
    "print(relt_vectorizer.get_feature_names()[::1000]) # get every 1000th word\n",
    "print()\n",
    "\n",
    "demog_vectorizer = joblib.load(demog_vec_fp, \"r+\")\n",
    "X_demog = demog_vectorizer.transform(demog_docs)\n",
    "print('Number of features in demographic vectorizer:', len(demog_vectorizer.get_feature_names()))\n",
    "print(demog_vectorizer.get_feature_names()[::1000]) # get every 1000th word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in preprocessed text for training cultural classifier (after applying cultural vectorizer): 66098\n",
      "['aa', 'affects', 'analogize', 'argumentatively', 'aveni', 'beifuss', 'blustein', 'buchholtz', 'carron', 'childlessness', 'coinciding', 'confused', 'councillors', 'czabansku', 'demirguc', 'dijeffren', 'dogs', 'economets', 'endler', 'ethnidzed', 'fad', 'fiskarbankadministrasjon', 'frenzen', 'ger', 'greenwood', 'haskell', 'hobson', 'identified', 'indexes', 'interceded', 'itbs', 'kaleidoscope', 'kraiger', 'legalized', 'lockett', 'makrosoziologie', 'mcenough', 'millenarian', 'mot', 'ncjrs', 'noninstrumental', 'offaculty', 'oshpd', 'parsimonious', 'phase', 'poovanalingum', 'primatively', 'punctuate', 'readership', 'relabeled', 'review', 'ruddy', 'schneider', 'severed', 'site', 'sparrowe', 'stochas', 'superlative', 'taurus', 'thus', 'transitioning', 'ularco', 'unsafe', 'vessies', 'weekends', 'wrinkle', 'ïï']\n",
      "\n",
      "Number of features in preprocessed text for training relational classifier (after applying relational vectorizer): 64919\n",
      "['aa', 'affordance', 'ancestors', 'arose', 'aydin', 'benefitted', 'bonacich', 'burkart', 'ccc', 'circumventing', 'commodcomposition', 'contempowith', 'criticised', 'decadence', 'destroys', 'dispassionate', 'dube', 'eloy', 'ereignet', 'explosivefrom', 'fight', 'fracture', 'generalised', 'grants', 'harmoniic', 'hoard', 'identit', 'indices', 'interdis', 'iunmar', 'kangaroo', 'krise', 'lementair', 'longitudinal', 'malmo', 'meandeviated', 'minutes', 'mrvar', 'neighbor', 'nonshacho', 'oijt', 'opportunists', 'paedophiles', 'perfonnance', 'plundering', 'prejudiced', 'prowse', 'raphy', 'regular', 'retaining', 'rounded', 'schaechter', 'series', 'singleproject', 'sozialen', 'stipends', 'superconducting', 'taxing', 'tiefigrom', 'transzendentale', 'unanticipate', 'unwind', 'violently', 'wheatsheaf', 'xtxwit']\n",
      "\n",
      "Number of features in preprocessed text for training demographic classifier (after applying demographic vectorizer): 63542\n",
      "['aa', 'aforementioned', 'ancilliary', 'aronsson', 'aydin', 'benevolence', 'bond', 'burkhalter', 'ccnrwtcidi', 'citationsa', 'communalities', 'contextual', 'cronache', 'decisionistic', 'deterministic', 'disquieting', 'dumb', 'embarrassed', 'erupts', 'extra', 'finesse', 'frederica', 'geographies', 'greyser', 'hays', 'homelike', 'ija', 'inequities', 'interrupted', 'jason', 'kekre', 'lackey', 'libertarians', 'lrus', 'mario', 'mentd', 'mockery', 'mutualism', 'nificance', 'nummarr', 'operandi', 'oç', 'perchoices', 'plunged', 'premed', 'prussia', 'rated', 'reinhart', 'revenues', 'ruckgr', 'schuette', 'shaking', 'skogan', 'spiritedness', 'strik', 'swamp', 'tenor', 'tjto', 'truisms', 'undesirable', 'vague', 'wahrman', 'withdrawn', 'zat']\n"
     ]
    }
   ],
   "source": [
    "# check out column order for data once vectorizer has been applied (should be exactly the same as list from previous cell)\n",
    "test = pd.DataFrame(X_cult.toarray(), columns=cult_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training cultural classifier (after applying cultural vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])\n",
    "print()\n",
    "test = pd.DataFrame(X_relt.toarray(), columns=relt_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training relational classifier (after applying relational vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])\n",
    "print()\n",
    "test = pd.DataFrame(X_demog.toarray(), columns=demog_vectorizer.get_feature_names())\n",
    "print('Number of features in preprocessed text for training demographic classifier (after applying demographic vectorizer):', len(list(test)))\n",
    "print(list(test)[::1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Balance x_train, y_train\n",
    "######################################################\n",
    "\n",
    "def resample_data(X_train, Y_train, undersample = False, sampling_ratio = 1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X_train: X training data\n",
    "        Y_train: Y training data\n",
    "        undersample: boolean for over or undersampling\n",
    "        sampling_ratio: ratio of minority to majority class\n",
    "        \n",
    "        archived/not used:\n",
    "        sampling_strategy: strategy for resampled distribution\n",
    "            if oversample: 'majority' makes minority = to majority\n",
    "            if undersample: 'minority' makes majority = to minority\n",
    "            \n",
    "    Returns:\n",
    "        X_balanced: predictors at balanced ratio\n",
    "        Y_balanced: outcomes at balanced ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    if undersample == True:\n",
    "        undersample = RandomUnderSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = undersample.fit_resample(X_train, Y_train)\n",
    "    else:\n",
    "        oversample = RandomOverSampler(sampling_strategy=sampling_ratio)\n",
    "        X_balanced, Y_balanced = oversample.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    print(f'Y_train: {Counter(Y_train)}\\nY_resample: {Counter(Y_balanced)}')\n",
    "    \n",
    "    return X_balanced, Y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# k-fold cross validation for model evaluation\n",
    "######################################################\n",
    "\n",
    "# Define test options for k-fold CV\n",
    "num_folds = 10 \n",
    "seed = 3\n",
    "scoring='f1_weighted' # set scoring metric (not used here)\n",
    "\n",
    "def show_kfold_output(models, \n",
    "                      X, \n",
    "                      Y, \n",
    "                      num_folds = num_folds, \n",
    "                      random_state = seed, \n",
    "                      shuffle = True):\n",
    "    '''\n",
    "    Estimates the accuracy of different model algorithms, adds results to a results array and returns.\n",
    "    Prints the accuracy results: averages and std.\n",
    "    Uses cross_val_predict, which unlike cross_val_score cannot define scoring option/evaluation metric.\n",
    "    \n",
    "    Args:\n",
    "        models: list of (name, model) tuples\n",
    "        X: predictors\n",
    "        Y: outcomes\n",
    "        num_folds: Split data randomly into num_folds parts: (num_folds-1) for training, 1 for scoring\n",
    "        random_state: seed\n",
    "        shuffle: \n",
    "    \n",
    "    Returns:\n",
    "        results: list of model results\n",
    "        names: list of model names (matches results)\n",
    "        \n",
    "    Source: \n",
    "        https://stackoverflow.com/questions/40057049/using-confusion-matrix-as-scoring-metric-in-cross-validation-in-scikit-learn\n",
    "    '''\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    \n",
    "    for name, model in models:\n",
    "        # Setup model options\n",
    "        kfold = KFold(\n",
    "            n_splits=num_folds, \n",
    "            random_state=seed, \n",
    "            shuffle=True)\n",
    "        \n",
    "        # Get kfold results\n",
    "        cv_results = cross_val_predict(\n",
    "            model, \n",
    "            X, \n",
    "            Y, \n",
    "            cv=kfold, \n",
    "            #scoring=scoring, \n",
    "            n_jobs=-1) # use all cores = faster\n",
    "        \n",
    "        # Add results and name of each algorithm to the model array\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'{name}:')\n",
    "        print()\n",
    "        print(f'Mean (std):\\t {round(cv_results.mean(),4)} ({round(cv_results.std(),4)})')\n",
    "        print(f'Accuracy:\\t', {round(accuracy_score(Y_balanced, cv_results)), 4})\n",
    "        print()\n",
    "        print('Confusion matrix:\\n', confusion_matrix(Y_balanced, cv_results))\n",
    "        print()\n",
    "        print('Report:\\n', classification_report(Y_balanced, cv_results))\n",
    "        print()\n",
    "        \n",
    "    # Return arrays\n",
    "    return results, names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Cultural perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 342\n",
      "Number of codes (should match): 342\n",
      "Y_train Distribution: [(0.0, 165), (1.0, 108)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "cult_df = cult_df[['text', 'cultural_score']]\n",
    "print(\"Number of cases:\", str(X_cult.shape[0]))\n",
    "\n",
    "valueArray = cult_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    test_size=test_size, \n",
    "    random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 209, 1.0: 133})\n",
      "Y_resample: Counter({1.0: 209, 0.0: 209})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "# Use these settings here and below\n",
    "sampling_ratio = 1.0 # ratio of minority to majority cases\n",
    "undersample = False # whether to undersample or oversample\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_cult, \n",
    "    Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural net models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_balanced.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "num_outputs = 2\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for batch in data_iterator:\n",
    "        data = batch.data[0].as_in_context(model_ctx).reshape((-1, 66098))\n",
    "        label = batch.label[0].as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced_dense = X_balanced.todense()\n",
    "X_validate_dense = X_validate.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = mx.io.NDArrayIter(X_balanced.todense().reshape(-1,66098), Y_balanced, 1, True)\n",
    "test_iter = mx.io.NDArrayIter(X_validate.todense().reshape(-1,66098), Y_validate, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.io.io.NDArrayIter at 0x7fabd0afb240>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.061260027763171, Train_acc nan, Test_acc 0.42028985507246375\n",
      "Epoch 1. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 2. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 3. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 4. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 5. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 6. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 7. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 8. Loss: 0.0, Train_acc nan, Test_acc nan\n",
      "Epoch 9. Loss: 0.0, Train_acc nan, Test_acc nan\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    cumulative_loss = 0\n",
    "    for batch in train_iter:\n",
    "        data = batch.data[0].as_in_context(model_ctx).reshape((-1, 66098))\n",
    "        label = batch.label[0].as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        cumulative_loss += nd.sum(loss).asscalar()\n",
    "\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_iter, net)\n",
    "    train_accuracy = evaluate_accuracy(train_iter, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, cumulative_loss/len(X_train.todense()), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with Scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.847 (+/-0.084) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.084) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.595 (+/-0.269) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.847 (+/-0.084) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.084) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.849 (+/-0.092) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.084) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.624 (+/-0.266) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.668 (+/-0.470) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.854 (+/-0.095) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.009) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.849 (+/-0.091) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.864 (+/-0.081) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.857 (+/-0.091) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.699 (+/-0.434) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.531 (+/-0.083) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.102) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.091) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.854 (+/-0.095) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.854 (+/-0.095) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.711 (+/-0.299) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.720 (+/-0.317) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.859 (+/-0.088) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.859 (+/-0.100) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.864 (+/-0.071) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.849 (+/-0.102) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.607 (+/-0.303) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.009) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.586 (+/-0.249) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.859 (+/-0.088) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.857 (+/-0.081) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.854 (+/-0.100) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.847 (+/-0.097) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.538 (+/-0.103) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.605 (+/-0.296) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.699 (+/-0.284) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.529 (+/-0.066) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.857 (+/-0.084) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.548 (+/-0.141) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.081) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.859 (+/-0.082) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.081) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.689 (+/-0.267) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.565 (+/-0.178) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.866 (+/-0.079) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.859 (+/-0.099) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.510 (+/-0.032) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.866 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.861 (+/-0.066) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.773 (+/-0.201) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.746 (+/-0.349) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.859 (+/-0.087) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.529 (+/-0.081) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.861 (+/-0.077) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.864 (+/-0.087) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.857 (+/-0.073) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.672 (+/-0.300) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.531 (+/-0.083) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.570 (+/-0.202) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.866 (+/-0.082) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.864 (+/-0.095) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.873 (+/-0.066) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.006) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.871 (+/-0.058) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.550 (+/-0.148) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.574 (+/-0.215) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.550 (+/-0.137) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.612 (+/-0.317) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.861 (+/-0.081) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.586 (+/-0.232) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.866 (+/-0.081) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential() # should this be \"net\"?\\nmodel.add(Dense(12, input_dim=8, activation=\\'relu\\'))\\nmodel.add(Dense(8, activation=\\'relu\\'))\\nmodel.add(Dense(1, activation=\\'sigmoid\\'))\\n# compile the keras model\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=\\'adam\\', metrics=[\\'accuracy\\'])\\n# fit the keras model on the dataset\\nmodel.fit(X_balanced, Y_balanced, epochs=150, batch_size=10)\\n# evaluate the keras model\\n_, accuracy = model.evaluate(X, y)\\nprint(\\'Accuracy: %.2f\\' % (accuracy*100))\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "#dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "# split into input (X) and output (y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#y = dataset[:,8]\n",
    "# define the keras model\n",
    "'''\n",
    "model = Sequential() # should this be \"net\"?\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_balanced, Y_balanced, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model accuracy using k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6292 (0.483)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[140  69]\n",
      " [ 15 194]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.67      0.77       209\n",
      "         1.0       0.74      0.93      0.82       209\n",
      "\n",
      "    accuracy                           0.80       418\n",
      "   macro avg       0.82      0.80      0.80       418\n",
      "weighted avg       0.82      0.80      0.80       418\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.4761 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[196  13]\n",
      " [ 23 186]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.94      0.92       209\n",
      "         1.0       0.93      0.89      0.91       209\n",
      "\n",
      "    accuracy                           0.91       418\n",
      "   macro avg       0.91      0.91      0.91       418\n",
      "weighted avg       0.91      0.91      0.91       418\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5718 (0.4948)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[156  53]\n",
      " [ 23 186]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.75      0.80       209\n",
      "         1.0       0.78      0.89      0.83       209\n",
      "\n",
      "    accuracy                           0.82       418\n",
      "   macro avg       0.82      0.82      0.82       418\n",
      "weighted avg       0.82      0.82      0.82       418\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.8373 (0.3691)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 68 141]\n",
      " [  0 209]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.33      0.49       209\n",
      "         1.0       0.60      1.00      0.75       209\n",
      "\n",
      "    accuracy                           0.66       418\n",
      "   macro avg       0.80      0.66      0.62       418\n",
      "weighted avg       0.80      0.66      0.62       418\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.4665 (0.4989)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[191  18]\n",
      " [ 32 177]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.88       209\n",
      "         1.0       0.91      0.85      0.88       209\n",
      "\n",
      "    accuracy                           0.88       418\n",
      "   macro avg       0.88      0.88      0.88       418\n",
      "weighted avg       0.88      0.88      0.88       418\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.5191 (0.4996)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 91 118]\n",
      " [110  99]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.45      0.44      0.44       209\n",
      "         1.0       0.46      0.47      0.46       209\n",
      "\n",
      "    accuracy                           0.45       418\n",
      "   macro avg       0.45      0.45      0.45       418\n",
      "weighted avg       0.45      0.45      0.45       418\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.5144 (0.4998)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[185  24]\n",
      " [ 18 191]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90       209\n",
      "         1.0       0.89      0.91      0.90       209\n",
      "\n",
      "    accuracy                           0.90       418\n",
      "   macro avg       0.90      0.90      0.90       418\n",
      "weighted avg       0.90      0.90      0.90       418\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_cult_012021.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_cult = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "#joblib.dump(rf_cult, cult_model_fp)\n",
    "\n",
    "joblib.dump(clf, cult_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation step: Use selected model to predict class probabilities in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "#cult_model_fp = model_fp + 'classifier_cult_012021.joblib' # model filepath\n",
    "#clf = joblib.load(cult_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(text, vectorizer_model, class_model):\n",
    "    '''\n",
    "    Predicts the label for an input article using a given model trained to classify organizational perspectives in articles. \n",
    "    Uses vectorizer_model to restrict the vocab of the input article so it's consistent with vocab in class_model (avoids errors).\n",
    "    \n",
    "    Args:\n",
    "        text: preprocessed article text in format of list of sentences, each a str or list of tokens\n",
    "        vectorizer_model: fitted text vectorizer\n",
    "        class_model: trained classification model\n",
    "    Returns:\n",
    "        label: label for text predicted by model, false for tie\n",
    "        prob: probability for label\n",
    "    '''\n",
    "    \n",
    "    X = vectorizer_model.transform(text) # create TF-IDF-weighted DTM from text\n",
    "    probabilities = class_model.predict_proba(X)\n",
    "    \n",
    "    label = 'no'\n",
    "    prob_no = probabilities[0][0]\n",
    "    prob_yes = probabilities[0][1]\n",
    "    \n",
    "    # predicted label is one with greater probability\n",
    "    if probabilities[0][0] < probabilities[0][1]:\n",
    "        label = 'yes'\n",
    "        \n",
    "    return label, prob_yes, prob_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: cultural persp.: 100%|██████████| 342/342 [00:03<00:00, 86.76it/s] \n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tqdm.pandas(desc = \"Predicting: cultural persp.\")\n",
    "cult_df[['prediction_cult','prediction_cult_prob_yes','prediction_cult_prob_no']] = cult_df['text'].progress_apply(lambda sentlist: pd.Series(compute_predictions([' '.join(sent) for sent in sentlist], cult_vectorizer, clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     208\n",
       "yes    134\n",
       "Name: prediction_cult, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out predictions: labels should be balanced\n",
    "pred_col = 'prediction_cult'\n",
    "cult_df[pred_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labeled cultural data: [(0.0, 209), (1.0, 133)]\n"
     ]
    }
   ],
   "source": [
    "# Compare to distribution of labeled data\n",
    "print(f'Distribution of labeled cultural data: {Counter(cult_df.values[:,1]).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fabcbdb7828>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out predicted probabilities: Distribution should be balanced\n",
    "prob_col = 'prediction_cult_prob_yes'\n",
    "cult_df[prob_col].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Relational perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 343\n",
      "Number of codes (should match): 343\n",
      "Y_train Distribution: [(0.0, 183), (1.0, 91)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First remove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "relt_df = relt_df[['text', 'relational_score']]\n",
    "print(\"Number of cases:\", str(X_relt.shape[0]))\n",
    "\n",
    "valueArray = relt_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_relt, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 229, 1.0: 114})\n",
      "Y_resample: Counter({1.0: 229, 0.0: 229})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_relt, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=40,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=40, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.613 (+/-0.325) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.935 (+/-0.065) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.504 (+/-0.008) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.928 (+/-0.074) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.667 (+/-0.417) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.598 (+/-0.282) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.667 (+/-0.471) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.059) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.511 (+/-0.031) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.061) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.071) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.935 (+/-0.065) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.622 (+/-0.340) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.696 (+/-0.381) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.834 (+/-0.394) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.596 (+/-0.267) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.071) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.550 (+/-0.143) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.937 (+/-0.068) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.068) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.937 (+/-0.050) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.631 (+/-0.365) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.546 (+/-0.121) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.647 (+/-0.416) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.930 (+/-0.077) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.935 (+/-0.065) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.022) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.062) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.930 (+/-0.077) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.557 (+/-0.152) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.828 (+/-0.459) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.862 (+/-0.180) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.526 (+/-0.069) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.935 (+/-0.065) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.062) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.059) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.939 (+/-0.062) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.605 (+/-0.300) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.784 (+/-0.402) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.541 (+/-0.122) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.511 (+/-0.026) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.059) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.937 (+/-0.068) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.059) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.932 (+/-0.062) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.932 (+/-0.073) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.657 (+/-0.439) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.515 (+/-0.044) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.059) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.939 (+/-0.062) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.945 (+/-0.062) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.945 (+/-0.062) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.511 (+/-0.026) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.878 (+/-0.291) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.624 (+/-0.356) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.937 (+/-0.068) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.531 (+/-0.082) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.939 (+/-0.071) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.959 (+/-0.034) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.954 (+/-0.028) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.636 (+/-0.375) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.941 (+/-0.093) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.615 (+/-0.322) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.721 (+/-0.319) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.948 (+/-0.056) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.948 (+/-0.065) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.522 (+/-0.057) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.050) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.956 (+/-0.050) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.744 (+/-0.346) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.511 (+/-0.026) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.948 (+/-0.105) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.941 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.513 (+/-0.042) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.941 (+/-0.075) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.963 (+/-0.043) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.963 (+/-0.043) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.620 (+/-0.344) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.727 (+/-0.356) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.553 (+/-0.149) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.828 (+/-0.459) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.522 (+/-0.062) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.061) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.956 (+/-0.061) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6223 (0.4848)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[163  66]\n",
      " [ 10 219]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.71      0.81       229\n",
      "         1.0       0.77      0.96      0.85       229\n",
      "\n",
      "    accuracy                           0.83       458\n",
      "   macro avg       0.86      0.83      0.83       458\n",
      "weighted avg       0.86      0.83      0.83       458\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.5022 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[220   9]\n",
      " [  8 221]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.96      0.96       229\n",
      "         1.0       0.96      0.97      0.96       229\n",
      "\n",
      "    accuracy                           0.96       458\n",
      "   macro avg       0.96      0.96      0.96       458\n",
      "weighted avg       0.96      0.96      0.96       458\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5546 (0.497)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[195  34]\n",
      " [  9 220]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90       229\n",
      "         1.0       0.87      0.96      0.91       229\n",
      "\n",
      "    accuracy                           0.91       458\n",
      "   macro avg       0.91      0.91      0.91       458\n",
      "weighted avg       0.91      0.91      0.91       458\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.69 (0.4625)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[138  91]\n",
      " [  4 225]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.60      0.74       229\n",
      "         1.0       0.71      0.98      0.83       229\n",
      "\n",
      "    accuracy                           0.79       458\n",
      "   macro avg       0.84      0.79      0.78       458\n",
      "weighted avg       0.84      0.79      0.78       458\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.476 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[218  11]\n",
      " [ 22 207]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.95      0.93       229\n",
      "         1.0       0.95      0.90      0.93       229\n",
      "\n",
      "    accuracy                           0.93       458\n",
      "   macro avg       0.93      0.93      0.93       458\n",
      "weighted avg       0.93      0.93      0.93       458\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.6376 (0.4807)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 80 149]\n",
      " [ 86 143]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.48      0.35      0.41       229\n",
      "         1.0       0.49      0.62      0.55       229\n",
      "\n",
      "    accuracy                           0.49       458\n",
      "   macro avg       0.49      0.49      0.48       458\n",
      "weighted avg       0.49      0.49      0.48       458\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.524 (0.4994)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[214  15]\n",
      " [  4 225]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.93      0.96       229\n",
      "         1.0       0.94      0.98      0.96       229\n",
      "\n",
      "    accuracy                           0.96       458\n",
      "   macro avg       0.96      0.96      0.96       458\n",
      "weighted avg       0.96      0.96      0.96       458\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_relt_012021.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_relt = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(clf, relt_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation step: Use selected model to predict class probabilities in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "#relt_model_fp = model_fp + 'classifier_relt_012021.joblib' # model filepath\n",
    "#clf = joblib.load(relt_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: relational persp.: 100%|██████████| 343/343 [00:04<00:00, 82.38it/s] \n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tqdm.pandas(desc = \"Predicting: relational persp.\")\n",
    "relt_df[['prediction_relt','prediction_relt_prob_yes','prediction_relt_prob_no']] = relt_df['text'].progress_apply(lambda sentlist: pd.Series(compute_predictions([' '.join(sent) for sent in sentlist], relt_vectorizer, clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     238\n",
       "yes    105\n",
       "Name: prediction_relt, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out predictions: labels should be balanced\n",
    "pred_col = 'prediction_relt'\n",
    "relt_df[pred_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labeled relational data: [(0.0, 229), (1.0, 114)]\n"
     ]
    }
   ],
   "source": [
    "# Compare to distribution of labeled data\n",
    "print(f'Distribution of labeled relational data: {Counter(relt_df.values[:,1]).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1945a39940>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out predicted probabilities: Distribution should be balanced\n",
    "prob_col = 'prediction_relt_prob_yes'\n",
    "relt_df[prob_col].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate algorithms: Demographic perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases: 349\n",
      "Number of codes (should match): 349\n",
      "Y_train Distribution: [(0.0, 195), (1.0, 84)]\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Prepare training and validation data\n",
    "######################################################\n",
    "\n",
    "# Separate training and final validation data set. First reltove class\n",
    "# label from data (X). Setup target class (Y)\n",
    "# Then make the validation set 10% of the entire\n",
    "# set of labeled data (X_validate, Y_validate)\n",
    "\n",
    "demog_df = demog_df[['text', 'demographic_score']]\n",
    "print(\"Number of cases:\", str(X_demog.shape[0]))\n",
    "\n",
    "valueArray = demog_df.values\n",
    "Y = valueArray[:,1]\n",
    "Y = Y.astype('float')\n",
    "print(\"Number of codes (should match):\", str(len(Y)))\n",
    "\n",
    "test_size = 0.2\n",
    "seed = 3\n",
    "X_train, X_validate, Y_train, Y_validate = train_test_split(X_demog, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(f'Y_train Distribution: {Counter(Y_train).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train: Counter({0.0: 248, 1.0: 101})\n",
      "Y_resample: Counter({1.0: 248, 0.0: 248})\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Oversample to desirable ratio\n",
    "######################################################\n",
    "\n",
    "X_balanced, Y_balanced = resample_data(\n",
    "    X_demog, Y, \n",
    "    undersample=undersample, \n",
    "    sampling_ratio=sampling_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=100, activation='relu')\n",
    "\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50), (50,50,2), (50,), (100,100), (100,100,2), (100,)],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(max_iter=100), n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1],\n",
       "                         'hidden_layer_sizes': [(50, 50), (50, 50, 2), (50,),\n",
       "                                                (100, 100), (100, 100, 2),\n",
       "                                                (100,)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'solver': ['sgd', 'adam']})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(X_balanced, Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.629 (+/-0.355) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.516 (+/-0.046) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.653 (+/-0.440) for {'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.0001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.633 (+/-0.366) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.811 (+/-0.443) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.668 (+/-0.470) for {'alpha': 0.0001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.012) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.601 (+/-0.281) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.956 (+/-0.063) for {'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.508 (+/-0.019) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.617 (+/-0.327) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.724 (+/-0.359) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.544 (+/-0.130) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.954 (+/-0.066) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.593 (+/-0.267) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.629 (+/-0.361) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.895 (+/-0.152) for {'alpha': 0.001, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.786 (+/-0.409) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.964 (+/-0.051) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.964 (+/-0.052) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.773 (+/-0.401) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.737 (+/-0.376) for {'alpha': 0.01, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.506 (+/-0.013) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.958 (+/-0.060) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.057) for {'alpha': 0.01, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.966 (+/-0.049) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.962 (+/-0.054) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.879 (+/-0.177) for {'alpha': 0.01, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.012) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.960 (+/-0.058) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.714 (+/-0.330) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.960 (+/-0.058) for {'alpha': 0.01, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.972 (+/-0.041) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.976 (+/-0.036) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.668 (+/-0.470) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.774 (+/-0.411) for {'alpha': 0.05, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.643 (+/-0.275) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.968 (+/-0.046) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.837 (+/-0.095) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.964 (+/-0.052) for {'alpha': 0.05, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.980 (+/-0.030) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.534 (+/-0.056) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.980 (+/-0.030) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.578 (+/-0.213) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.764 (+/-0.396) for {'alpha': 0.05, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.585 (+/-0.115) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.970 (+/-0.045) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.643 (+/-0.239) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.972 (+/-0.041) for {'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.528 (+/-0.088) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.978 (+/-0.035) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.978 (+/-0.035) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.818 (+/-0.447) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.510 (+/-0.037) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.822 (+/-0.452) for {'alpha': 0.1, 'hidden_layer_sizes': (50, 50, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.968 (+/-0.047) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.504 (+/-0.008) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.968 (+/-0.047) for {'alpha': 0.1, 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.498 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.984 (+/-0.025) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.982 (+/-0.030) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.787 (+/-0.416) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.500 (+/-0.005) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.738 (+/-0.351) for {'alpha': 0.1, 'hidden_layer_sizes': (100, 100, 2), 'learning_rate': 'adaptive', 'solver': 'adam'}\n",
      "0.504 (+/-0.020) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'sgd'}\n",
      "0.974 (+/-0.040) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "0.502 (+/-0.003) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'sgd'}\n",
      "0.976 (+/-0.036) for {'alpha': 0.1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#plotting model with most optimized parameters found above\n",
    "clf = MLPClassifier(random_state=1, max_iter=200, activation='relu', \n",
    "                   alpha=0.0001, hidden_layer_sizes=(50, 50), \n",
    "                    learning_rate='adaptive', solver='adam').fit(X_balanced, Y_balanced)\n",
    "#clf.predict_proba(X_validate[:1])\n",
    "\n",
    "print(clf.score(X_balanced, Y_balanced))\n",
    "print(clf.score(X_validate, Y_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "\n",
      "Mean (std):\t 0.6371 (0.4808)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[179  69]\n",
      " [  1 247]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.72      0.84       248\n",
      "         1.0       0.78      1.00      0.88       248\n",
      "\n",
      "    accuracy                           0.86       496\n",
      "   macro avg       0.89      0.86      0.86       496\n",
      "weighted avg       0.89      0.86      0.86       496\n",
      "\n",
      "\n",
      "Random Forest (RF):\n",
      "\n",
      "Mean (std):\t 0.496 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[244   4]\n",
      " [  6 242]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       248\n",
      "         1.0       0.98      0.98      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n",
      "Decision Tree (DT):\n",
      "\n",
      "Mean (std):\t 0.5484 (0.4977)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[217  31]\n",
      " [  7 241]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.88      0.92       248\n",
      "         1.0       0.89      0.97      0.93       248\n",
      "\n",
      "    accuracy                           0.92       496\n",
      "   macro avg       0.93      0.92      0.92       496\n",
      "weighted avg       0.93      0.92      0.92       496\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes (MNB):\n",
      "\n",
      "Mean (std):\t 0.6472 (0.4778)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[175  73]\n",
      " [  0 248]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.71      0.83       248\n",
      "         1.0       0.77      1.00      0.87       248\n",
      "\n",
      "    accuracy                           0.85       496\n",
      "   macro avg       0.89      0.85      0.85       496\n",
      "weighted avg       0.89      0.85      0.85       496\n",
      "\n",
      "\n",
      "Logistic Regression (LR):\n",
      "\n",
      "Mean (std):\t 0.5 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[243   5]\n",
      " [  5 243]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       248\n",
      "         1.0       0.98      0.98      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n",
      "Support Vector Machine (SVM):\n",
      "\n",
      "Mean (std):\t 0.6996 (0.4584)\n",
      "Accuracy:\t {0, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 62 186]\n",
      " [ 87 161]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.25      0.31       248\n",
      "         1.0       0.46      0.65      0.54       248\n",
      "\n",
      "    accuracy                           0.45       496\n",
      "   macro avg       0.44      0.45      0.43       496\n",
      "weighted avg       0.44      0.45      0.43       496\n",
      "\n",
      "\n",
      "Multi-Layer Perceptron (MLP) (Optimized):\n",
      "\n",
      "Mean (std):\t 0.504 (0.5)\n",
      "Accuracy:\t {1, 4}\n",
      "\n",
      "Confusion matrix:\n",
      " [[243   5]\n",
      " [  3 245]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98       248\n",
      "         1.0       0.98      0.99      0.98       248\n",
      "\n",
      "    accuracy                           0.98       496\n",
      "   macro avg       0.98      0.98      0.98       496\n",
      "weighted avg       0.98      0.98      0.98       496\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######################################################\n",
    "# Use different algorithms to build models\n",
    "######################################################\n",
    "\n",
    "models = []\n",
    "models.append(('K-Nearest Neighbors (KNN)', KNeighborsClassifier()))\n",
    "models.append(('Random Forest (RF)', RandomForestClassifier(random_state=seed)))\n",
    "models.append(('Decision Tree (DT)', DecisionTreeClassifier(random_state=seed)))\n",
    "models.append(('Multinomial Naive Bayes (MNB)', MultinomialNB()))\n",
    "models.append(('Logistic Regression (LR)', LogisticRegression(random_state=seed)))\n",
    "models.append(('Support Vector Machine (SVM)', SVC(gamma='auto')))\n",
    "models.append(('Multi-Layer Perceptron (MLP) (Optimized)', clf))\n",
    "\n",
    "# Evaluate algorithms using 10-fold cross validation\n",
    "results, names = show_kfold_output(models=models, \n",
    "                                   X=X_balanced, \n",
    "                                   Y=Y_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/work/classification/models/classifier_demog_012021.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# Save best model\n",
    "######################################################\n",
    "\n",
    "#rf_demog = RandomForestClassifier(random_state=seed).fit(X_balanced, Y_balanced)\n",
    "joblib.dump(clf, demog_model_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation step: Use selected model to predict class probabilities in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "#demog_model_fp = model_fp + 'classifier_demog_012021.joblib' # model filepath\n",
    "#clf = joblib.load(demog_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: demographic persp.: 100%|██████████| 349/349 [00:04<00:00, 81.98it/s] \n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "tqdm.pandas(desc = \"Predicting: demographic persp.\")\n",
    "demog_df[['prediction_demog','prediction_demog_prob_yes','prediction_demog_prob_no']] = demog_df['text'].progress_apply(lambda sentlist: pd.Series(compute_predictions([' '.join(sent) for sent in sentlist], demog_vectorizer, clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     258\n",
       "yes     91\n",
       "Name: prediction_demog, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out predictions: labels should be balanced\n",
    "pred_col = 'prediction_demog'\n",
    "demog_df[pred_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labeled cultural data: [(0.0, 248), (1.0, 101)]\n"
     ]
    }
   ],
   "source": [
    "# Compare to distribution of labeled data\n",
    "print(f'Distribution of labeled cultural data: {Counter(demog_df.values[:,1]).most_common()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f19482e8e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAE/lJREFUeJzt3X+MZfV53/H3p2CjhIkWHPBou+AOjrAbAzHNTqnVNNZMaMIGVyWO4hSKCLZpxlax5ar+w9itYqsWEm2D3RbHsjYBQZQtY2SwlxKcBtGMSRQTe7fFLJjgLHiDF9BuYd3FgxHV4qd/zN3qZndm5+79MT++835Jo73ne773nOfZWX3m7Jlzz0lVIUlq199a7QIkSaNl0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIad+pqFwBw1lln1cTERN/vf/nllzn99NOHV9Aat9H6BXveKOz55OzevfuFqjp7uXlrIugnJibYtWtX3++fm5tjampqeAWtcRutX7DnjcKeT06Sv+5lnqduJKlxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq3LJBn+TcJH+S5Ikkjyf5SGf8DUkeSPJXnT/P7HrPx5PsTfJkkstG2YAk6cR6OaI/Any0qn4aeAdwfZK3ATcAD1bV+cCDnWU6664ELgC2AZ9PcsooipckLW/ZT8ZW1fPA853XP0jyBLAFuAKY6ky7A5gDPtYZn62qV4HvJtkLXAJ8fdjFH7Xn2cO894Y/PG58303vGtUuJWndOKlz9EkmgL8H/AUw3vkhcPSHwRs707YA3+t62/7OmCRpFfR8r5skY8DdwL+qqpeSLDl1kbFaZHszwAzA+Pg4c3NzvZZynPEfg49edOS48UG2uZbNz88329tS7HljsOfR6Cnok7yOhZDfUVX3dIYPJNlcVc8n2Qwc7IzvB87tevs5wHPHbrOqtgPbASYnJ2uQGxndsmMnN+85vpV9V/e/zbXMGz9tDPa8MaxEz71cdRPgVuCJqvpM16p7gWs7r68FdnaNX5nktCTnAecD3xheyZKkk9HLEf3PAdcAe5I80hn7BHATcFeS64BngPcAVNXjSe4Cvs3CFTvXV9VrQ69cktSTXq66+TMWP+8OcOkS77kRuHGAuiRJQ+InYyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxvTwz9rYkB5M81jX2xSSPdL72HX3EYJKJJK90rfvCKIuXJC2vl2fG3g58Dvj9owNV9c+Ovk5yM3C4a/5TVXXxsAqUJA2ml2fGPpRkYrF1SQL8OvALwy1LkjQsqarlJy0E/X1VdeEx4+8EPlNVk13zHge+A7wE/Nuq+tMltjkDzACMj49vnZ2d7bcHDh46zIFXjh+/aMumvre5ls3PzzM2NrbaZawoe94Y7PnkTE9P7z6avyfSy6mbE7kKuLNr+XngTVX1YpKtwFeSXFBVLx37xqraDmwHmJycrKmpqb6LuGXHTm7ec3wr+67uf5tr2dzcHIP8fa1H9rwx2PNo9H3VTZJTgV8Fvnh0rKperaoXO693A08Bbxm0SElS/wa5vPIfA39ZVfuPDiQ5O8kpnddvBs4Hnh6sREnSIHq5vPJO4OvAW5PsT3JdZ9WV/M3TNgDvBB5N8i3gS8AHq+rQMAuWJJ2cXq66uWqJ8fcuMnY3cPfgZUmShsVPxkpS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGtfLE6ZuS3IwyWNdY59K8mySRzpfl3et+3iSvUmeTHLZqAqXJPWmlyP624Fti4x/tqou7nzdD5DkbSw8YvCCzns+f/QZspKk1bFs0FfVQ0Cvz329Apitqler6rvAXuCSAeqTJA1okHP0H0ryaOfUzpmdsS3A97rm7O+MSZJWSapq+UnJBHBfVV3YWR4HXgAK+DSwuaren+R3gK9X1R905t0K3N95aPix25wBZgDGx8e3zs7O9t3EwUOHOfDK8eMXbdnU9zbXsvn5ecbGxla7jBVlzxuDPZ+c6enp3VU1udy8U/vZeFUdOPo6ye8C93UW9wPndk09B3huiW1sB7YDTE5O1tTUVD+lAHDLjp3cvOf4VvZd3f8217K5uTkG+ftaj+x5Y7Dn0ejr1E2SzV2L7waOXpFzL3BlktOSnAecD3xjsBIlSYNY9og+yZ3AFHBWkv3AJ4GpJBezcOpmH/ABgKp6PMldwLeBI8D1VfXaaEqXJPVi2aCvqqsWGb71BPNvBG4cpChJ0vD4yVhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklq3LJBn+S2JAeTPNY19h+T/GWSR5N8OckZnfGJJK8keaTz9YVRFi9JWl4vR/S3A9uOGXsAuLCqfgb4DvDxrnVPVdXFna8PDqdMSVK/lg36qnoIOHTM2B9X1ZHO4sPAOSOoTZI0BMM4R/9+4Ktdy+cl+V9Jvpbk54ewfUnSAFJVy09KJoD7qurCY8b/DTAJ/GpVVZLTgLGqejHJVuArwAVV9dIi25wBZgDGx8e3zs7O9t3EwUOHOfDK8eMXbdnU9zbXsvn5ecbGxla7jBVlzxuDPZ+c6enp3VU1udy8U/vaOpDkWuCfAJdW56dFVb0KvNp5vTvJU8BbgF3Hvr+qtgPbASYnJ2tqaqrfUrhlx05u3nN8K/uu7n+ba9nc3ByD/H2tR/a8MdjzaPR16ibJNuBjwD+tqh92jZ+d5JTO6zcD5wNPD6NQSVJ/lj2iT3InMAWclWQ/8EkWrrI5DXggCcDDnSts3gn8uyRHgNeAD1bVoUU3LElaEcsGfVVdtcjwrUvMvRu4e9CiJEnD4ydjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXHLBn2S25IcTPJY19gbkjyQ5K86f57Zte7jSfYmeTLJZaMqXJLUm16O6G8Hth0zdgPwYFWdDzzYWSbJ24ArgQs67/n80YeFS5JWx7JBX1UPAcc+4PsK4I7O6zuAX+kan62qV6vqu8Be4JIh1SpJ6kOqavlJyQRwX1Vd2Fn+P1V1Rtf671fVmUk+BzxcVX/QGb8V+GpVfWmRbc4AMwDj4+NbZ2dn+27i4KHDHHjl+PGLtmzqe5tr2fz8PGNjY6tdxoqy543Bnk/O9PT07qqaXG7eqX1tfWlZZGzRnyRVtR3YDjA5OVlTU1N97/SWHTu5ec/xrey7uv9trmVzc3MM8ve1HtnzxmDPo9HvVTcHkmwG6Px5sDO+Hzi3a945wHP9lydJGlS/QX8vcG3n9bXAzq7xK5OcluQ84HzgG4OVKEkaxLKnbpLcCUwBZyXZD3wSuAm4K8l1wDPAewCq6vEkdwHfBo4A11fVayOqXZLUg2WDvqquWmLVpUvMvxG4cZCiJEnD4ydjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN6/vh4EneCnyxa+jNwG8BZwC/Cfzvzvgnqur+viuUJA2k76CvqieBiwGSnAI8C3wZeB/w2ar67aFUKEkayLBO3VwKPFVVfz2k7UmShmRYQX8lcGfX8oeSPJrktiRnDmkfkqQ+pKoG20DyeuA54IKqOpBkHHgBKODTwOaqev8i75sBZgDGx8e3zs7O9l3DwUOHOfDK8eMXbdnU9zbXsvn5ecbGxla7jBVlzxuDPZ+c6enp3VU1udy8YQT9FcD1VfVLi6ybAO6rqgtPtI3JycnatWtX3zXcsmMnN+85/tcN+256V9/bXMvm5uaYmppa7TJWlD1vDPZ8cpL0FPTDOHVzFV2nbZJs7lr3buCxIexDktSnvq+6AUjy48AvAh/oGv4PSS5m4dTNvmPWSZJW2EBBX1U/BH7ymLFrBqpIkjRUfjJWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW6gyyslSb2ZuOEPFx2/fdvpI9+3R/SS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGjfoowT3AT8AXgOOVNVkkjcAXwQmWHiU4K9X1fcHK1OS1K9hHNFPV9XFXU8ivwF4sKrOBx7sLEuSVskoTt1cAdzReX0H8Csj2IckqUeDBn0Bf5xkd5KZzth4VT0P0PnzjQPuQ5I0gFRV/29O/nZVPZfkjcADwIeBe6vqjK4536+qMxd57wwwAzA+Pr51dna27zoOHjrMgVeOH79oy6a+t7mWzc/PMzY2ttplrCh73hha7nnPs4cXHT9v0yl99zw9Pb2767T5kgYK+r+xoeRTwDzwm8BUVT2fZDMwV1VvPdF7Jycna9euXX3v+5YdO7l5z/G/V95307v63uZaNjc3x9TU1GqXsaLseWNouecT3Y++356T9BT0fZ+6SXJ6kp84+hr4JeAx4F7g2s60a4Gd/e5DkjS4QS6vHAe+nOTodv5rVf1Rkm8CdyW5DngGeM/gZUqS+tV30FfV08DbFxl/Ebh0kKIkScPjJ2MlqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LiBHg6+1i11/+dW71MvSYvxiF6SGmfQS1LjDHpJapxBL0mNG+SZsecm+ZMkTyR5PMlHOuOfSvJskkc6X5cPr1xJ0ska5KqbI8BHq+p/dh4SvjvJA511n62q3x68PEnSoAZ5ZuzzwPOd1z9I8gSwZViFSZKGI1U1+EaSCeAh4ELgXwPvBV4CdrFw1P/9Rd4zA8wAjI+Pb52dne17/wcPHebAK73Pv2jLpr73tRbMz88zNja22mWsKHveGFruec+zhxcdP2/TKX33PD09vbuqJpebN3DQJxkDvgbcWFX3JBkHXgAK+DSwuaref6JtTE5O1q5du/qu4ZYdO7l5T+//OVnvH5iam5tjampqtctYUfa8MbTc81If4Lx92+l995ykp6Af6KqbJK8D7gZ2VNU9AFV1oKpeq6ofAb8LXDLIPiRJgxnkqpsAtwJPVNVnusY3d017N/BY/+VJkgY1yFU3PwdcA+xJ8khn7BPAVUkuZuHUzT7gAwNVKEkayCBX3fwZkEVW3d9/OZKkYfOTsZLUuKZvUyxJK22pq2tWk0f0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zuvota4tdc3yer9DqTRMHtFLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxo3s8sok24D/DJwC/F5V3TSqfUnSSluLtyNeykiCPskpwO8AvwjsB76Z5N6q+vYo9neyvPZa0kYyqiP6S4C9VfU0QJJZ4ApgTQT9Uk72B4A/MCStB6MK+i3A97qW9wP/YET7GrnV+i/aUvu9fdvpI9/HsH5YracfhqtV60p8nzea9XRaZSWkqoa/0eQ9wGVV9S86y9cAl1TVh7vmzAAzncW3Ak8OsMuzgBcGeP96s9H6BXveKOz55Pydqjp7uUmjOqLfD5zbtXwO8Fz3hKraDmwfxs6S7KqqyWFsaz3YaP2CPW8U9jwao7q88pvA+UnOS/J64Erg3hHtS5J0AiM5oq+qI0k+BPx3Fi6vvK2qHh/FviRJJzay6+ir6n7g/lFt/xhDOQW0jmy0fsGeNwp7HoGR/DJWkrR2eAsESWrcugn6JNuSPJlkb5IbFlmfJP+ls/7RJD+7GnUOUw89X93p9dEkf57k7atR5zAt13PXvL+f5LUkv7aS9Y1CLz0nmUrySJLHk3xtpWscth7+bW9K8t+SfKvT8/tWo85hSXJbkoNJHlti/Wjzq6rW/BcLv9B9Cngz8HrgW8DbjplzOfBVIMA7gL9Y7bpXoOd/CJzZef3LG6Hnrnn/g4XfAf3aate9At/nM1j4VPmbOstvXO26V6DnTwD/vvP6bOAQ8PrVrn2Ant8J/Czw2BLrR5pf6+WI/v/fUqGq/i9w9JYK3a4Afr8WPAyckWTzShc6RMv2XFV/XlXf7yw+zMLnFdazXr7PAB8G7gYOrmRxI9JLz/8cuKeqngGoqvXedy89F/ATSQKMsRD0R1a2zOGpqodY6GEpI82v9RL0i91SYUsfc9aTk+3nOhaOCNazZXtOsgV4N/CFFaxrlHr5Pr8FODPJXJLdSX5jxaobjV56/hzw0yx80HIP8JGq+tHKlLcqRppfI7u8csiyyNixlwv1Mmc96bmfJNMsBP0/GmlFo9dLz/8J+FhVvbZwsLfu9dLzqcBW4FLgx4CvJ3m4qr4z6uJGpJeeLwMeAX4B+CnggSR/WlUvjbq4VTLS/FovQb/sLRV6nLOe9NRPkp8Bfg/45ap6cYVqG5Veep4EZjshfxZweZIjVfWVlSlx6Hr9t/1CVb0MvJzkIeDtwHoN+l56fh9wUy2cwN6b5LvA3wW+sTIlrriR5td6OXXTyy0V7gV+o/Pb63cAh6vq+ZUudIiW7TnJm4B7gGvW8dFdt2V7rqrzqmqiqiaALwH/ch2HPPT2b3sn8PNJTk3y4yzcCfaJFa5zmHrp+RkW/gdDknEWbnz49IpWubJGml/r4oi+lrilQpIPdtZ/gYUrMC4H9gI/ZOGIYN3qseffAn4S+HznCPdIreMbQvXYc1N66bmqnkjyR8CjwI9YeGLbopfprQc9fp8/DdyeZA8LpzU+VlXr9q6WSe4EpoCzkuwHPgm8DlYmv/xkrCQ1br2cupEk9cmgl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcf8Pj3iSqLxrPIMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out predicted probabilities: Distribution should be balanced\n",
    "prob_col = 'prediction_demog_prob_yes'\n",
    "demog_df[prob_col].hist(bins=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
